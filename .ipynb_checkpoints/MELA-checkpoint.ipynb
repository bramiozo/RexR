{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ Firing up RexR! ++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from RexR import *\n",
    "import _helpers\n",
    "Rocket = RexR(datalocation = None, #'_data/genomic_data/data.pkl', \n",
    "              seed = 3123, \n",
    "              debug = False, \n",
    "              write_out=True,\n",
    "              set_name = 'MELA') # data to read in ALL_10, or MELA\n",
    "Rocket.load_probeset_data();\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Prepping data, this may take a while..\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Grouping probesets\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: ET accuracy:  0.89156626506 +/-: 0.00417841932341\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: CART accuracy:  0.831325301205 +/-: 0.0229210012742\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: RF accuracy:  0.903614457831 +/-: 0.00497276135015\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: GBM accuracy:  0.89156626506 +/-: 0.00482807677675\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: ADA accuracy:  0.927710843373 +/-: 0.00226010348979\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: LR accuracy:  0.927710843373 +/-: 0.00376612758618\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: SVM accuracy:  0.927710843373 +/-: 0.00376612758618\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: GNB accuracy:  0.903614457831 +/-: 0.00263005720022\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: MLNN accuracy:  0.89156626506 +/-: 0.00543343940373\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Initial alpha = [[ 0.03009873]]\n",
      "   1 - L=-420.0683769 - Gamma= 1.9999523 (M=   2) - s=0.0100\n",
      "   2 - L=-193.6203481 - Gamma= 2.9999108 (M=   3) - s=0.0100\n",
      "   3 - L=-137.1142347 - Gamma= 3.9997586 (M=   4) - s=0.0100\n",
      "   4 - L=-103.8331887 - Gamma= 4.9995167 (M=   5) - s=0.0100\n",
      "   5 - L=-79.7935304 - Gamma= 5.9991809 (M=   6) - s=0.0100\n",
      "   6 - L=-59.8328716 - Gamma= 6.9987759 (M=   7) - s=0.0100\n",
      "   7 - L=-44.9253857 - Gamma= 7.9981972 (M=   8) - s=0.0100\n",
      "   8 - L=-33.0212706 - Gamma= 8.9974920 (M=   9) - s=0.0100\n",
      "   9 - L=-25.0613862 - Gamma= 9.9964409 (M=  10) - s=0.0100\n",
      "  10 - L=-18.2993788 - Gamma=10.9952021 (M=  11) - s=0.0100\n",
      "  11 - L=-13.2368736 - Gamma=11.9932378 (M=  12) - s=0.0100\n",
      "  12 - L=-8.7682821 - Gamma=12.9913969 (M=  13) - s=0.0100\n",
      "  13 - L=-6.0393662 - Gamma=13.9883511 (M=  14) - s=0.0100\n",
      "  14 - L=-3.8603755 - Gamma=14.9847401 (M=  15) - s=0.0100\n",
      "  15 - L=-2.4565273 - Gamma=15.9790326 (M=  16) - s=0.0100\n",
      "  16 - L=-1.0072276 - Gamma=16.9737150 (M=  17) - s=0.0100\n",
      "  17 - L= 0.0206647 - Gamma=17.9659509 (M=  18) - s=0.0100\n",
      "  18 - L= 0.7207357 - Gamma=18.9550671 (M=  19) - s=0.0100\n",
      "  19 - L= 1.2535130 - Gamma=19.9410107 (M=  20) - s=0.0100\n",
      "  20 - L= 1.7936651 - Gamma=20.9259010 (M=  21) - s=0.0100\n",
      "  21 - L= 2.1470023 - Gamma=21.9051665 (M=  22) - s=0.0100\n",
      "  22 - L= 2.4308239 - Gamma=22.8786174 (M=  23) - s=0.0100\n",
      "  23 - L= 2.6323513 - Gamma=23.8442670 (M=  24) - s=0.0100\n",
      "  24 - L= 2.7599646 - Gamma=24.7922264 (M=  25) - s=0.0100\n",
      "  25 - L= 2.8450159 - Gamma=25.7200505 (M=  26) - s=0.0100\n",
      "  26 - L= 2.8934535 - Gamma=26.6119724 (M=  27) - s=0.0100\n",
      "  27 - L= 2.9345898 - Gamma=27.4892658 (M=  28) - s=0.0100\n",
      "  28 - L= 2.9479201 - Gamma=28.2423735 (M=  29) - s=0.0100\n",
      "  29 - L= 2.9603092 - Gamma=28.9847183 (M=  30) - s=0.0100\n",
      "  30 - L= 2.9649189 - Gamma=28.9859372 (M=  30) - s=0.0100\n",
      "  31 - L= 2.9689500 - Gamma=28.9852766 (M=  30) - s=0.0100\n",
      "  32 - L= 2.9722480 - Gamma=28.9860466 (M=  30) - s=0.0100\n",
      "  33 - L= 2.9752465 - Gamma=29.5068433 (M=  31) - s=0.0100\n",
      "  34 - L= 2.9783065 - Gamma=29.5076715 (M=  31) - s=0.0100\n",
      "  35 - L= 2.9806871 - Gamma=29.5079305 (M=  31) - s=0.0100\n",
      "  36 - L= 2.9830639 - Gamma=29.9955789 (M=  32) - s=0.0100\n",
      "  37 - L= 2.9850511 - Gamma=29.9829262 (M=  32) - s=0.0100\n",
      "  38 - L= 2.9866397 - Gamma=29.9809325 (M=  32) - s=0.0100\n",
      "  39 - L= 2.9881361 - Gamma=29.9821255 (M=  32) - s=0.0100\n",
      "  40 - L= 2.9893850 - Gamma=30.3768674 (M=  33) - s=0.0100\n",
      "  41 - L= 2.9905683 - Gamma=30.3830562 (M=  33) - s=0.0100\n",
      "  42 - L= 2.9914317 - Gamma=30.3832609 (M=  33) - s=0.0100\n",
      "  43 - L= 2.9921430 - Gamma=30.3834355 (M=  33) - s=0.0100\n",
      "  44 - L= 2.9925961 - Gamma=30.3730711 (M=  33) - s=0.0100\n",
      "  45 - L= 2.9928970 - Gamma=30.3641592 (M=  33) - s=0.0100\n",
      "  46 - L= 2.9931567 - Gamma=30.3299889 (M=  33) - s=0.0100\n",
      "  47 - L= 2.9933633 - Gamma=30.5181234 (M=  34) - s=0.0100\n",
      "  48 - L= 2.9935729 - Gamma=30.4524024 (M=  34) - s=0.0100\n",
      "  49 - L= 2.9937532 - Gamma=30.4549405 (M=  34) - s=0.0100\n",
      "  50 - L= 2.9939115 - Gamma=30.4549635 (M=  34) - s=0.0100\n",
      "  51 - L= 2.9940412 - Gamma=30.4403131 (M=  34) - s=0.0100\n",
      "  52 - L= 2.9941227 - Gamma=30.4409229 (M=  34) - s=0.0100\n",
      "  53 - L= 2.9941848 - Gamma=30.4409634 (M=  34) - s=0.0100\n",
      "  54 - L= 2.9942397 - Gamma=30.4087115 (M=  34) - s=0.0100\n",
      "  55 - L= 2.9942750 - Gamma=30.4033983 (M=  34) - s=0.0100\n",
      "  56 - L= 2.9943038 - Gamma=30.4037021 (M=  34) - s=0.0100\n",
      "  57 - L= 2.9943300 - Gamma=30.3616047 (M=  34) - s=0.0100\n",
      "  58 - L= 2.9943430 - Gamma=30.3271349 (M=  34) - s=0.0100\n",
      "  59 - L= 2.9943601 - Gamma=30.3750109 (M=  34) - s=0.0100\n",
      "  60 - L= 2.9943737 - Gamma=30.4250541 (M=  35) - s=0.0100\n",
      "  61 - L= 2.9943838 - Gamma=30.4246384 (M=  35) - s=0.0100\n",
      "  62 - L= 2.9943916 - Gamma=30.4298348 (M=  35) - s=0.0100\n",
      "  63 - L= 2.9943995 - Gamma=30.4014542 (M=  35) - s=0.0100\n",
      "  64 - L= 2.9944067 - Gamma=30.4014508 (M=  35) - s=0.0100\n",
      "  65 - L= 2.9944104 - Gamma=30.4014306 (M=  35) - s=0.0100\n",
      "  66 - L= 2.9944136 - Gamma=30.4023777 (M=  35) - s=0.0100\n",
      "  67 - L= 2.9944165 - Gamma=30.4021915 (M=  35) - s=0.0100\n",
      "  68 - L= 2.9944190 - Gamma=30.4200029 (M=  35) - s=0.0100\n",
      "  69 - L= 2.9944211 - Gamma=30.4208017 (M=  35) - s=0.0100\n",
      "  70 - L= 2.9944230 - Gamma=30.4207141 (M=  35) - s=0.0100\n",
      "  71 - L= 2.9944247 - Gamma=30.4218840 (M=  35) - s=0.0100\n",
      "  72 - L= 2.9944263 - Gamma=30.4126164 (M=  35) - s=0.0100\n",
      "  73 - L= 2.9944280 - Gamma=30.4296507 (M=  35) - s=0.0100\n",
      "  74 - L= 2.9944291 - Gamma=30.4296922 (M=  35) - s=0.0100\n",
      "  75 - L= 2.9944301 - Gamma=30.4273885 (M=  35) - s=0.0100\n",
      "  76 - L= 2.9944310 - Gamma=30.4174372 (M=  35) - s=0.0100\n",
      "  77 - L= 2.9944323 - Gamma=30.4333703 (M=  36) - s=0.0100\n",
      "  78 - L= 2.9944336 - Gamma=30.4248394 (M=  36) - s=0.0100\n",
      "  79 - L= 2.9944344 - Gamma=30.4245185 (M=  36) - s=0.0100\n",
      "  80 - L= 2.9944349 - Gamma=30.4245293 (M=  36) - s=0.0100\n",
      "  81 - L= 2.9944354 - Gamma=30.4334561 (M=  36) - s=0.0100\n",
      "  82 - L= 2.9944360 - Gamma=30.4323990 (M=  36) - s=0.0100\n",
      "  83 - L= 2.9944365 - Gamma=30.4325267 (M=  36) - s=0.0100\n",
      "  84 - L= 2.9944369 - Gamma=30.4293912 (M=  36) - s=0.0100\n",
      "  85 - L= 2.9944372 - Gamma=30.4293559 (M=  36) - s=0.0100\n",
      "  86 - L= 2.9944375 - Gamma=30.4367851 (M=  36) - s=0.0100\n",
      "  87 - L= 2.9944379 - Gamma=30.4306266 (M=  36) - s=0.0100\n",
      "  88 - L= 2.9944382 - Gamma=30.4316198 (M=  36) - s=0.0100\n",
      "  89 - L= 2.9944384 - Gamma=30.4277133 (M=  36) - s=0.0100\n",
      "  90 - L= 2.9944386 - Gamma=30.4335299 (M=  36) - s=0.0100\n",
      "  91 - L= 2.9944388 - Gamma=30.4355536 (M=  36) - s=0.0100\n",
      "  92 - L= 2.9944390 - Gamma=30.4324927 (M=  36) - s=0.0100\n",
      "  93 - L= 2.9944392 - Gamma=30.4328844 (M=  36) - s=0.0100\n",
      "  94 - L= 2.9944394 - Gamma=30.4326383 (M=  36) - s=0.0100\n",
      "  95 - L= 2.9944396 - Gamma=30.4324232 (M=  36) - s=0.0100\n",
      "  96 - L= 2.9944397 - Gamma=30.4326111 (M=  36) - s=0.0100\n",
      "  97 - L= 2.9944398 - Gamma=30.4370794 (M=  36) - s=0.0100\n",
      "  98 - L= 2.9944399 - Gamma=30.4344494 (M=  36) - s=0.0100\n",
      "  99 - L= 2.9944401 - Gamma=30.4392570 (M=  36) - s=0.0100\n",
      " 100 - L= 2.9944402 - Gamma=30.4355607 (M=  36) - s=0.0100\n",
      " 101 - L= 2.9944402 - Gamma=30.4350264 (M=  36) - s=0.0100\n",
      " 102 - L= 2.9944403 - Gamma=30.4377282 (M=  36) - s=0.0100\n",
      " 103 - L= 2.9944403 - Gamma=30.4359989 (M=  36) - s=0.0100\n",
      " 104 - L= 2.9944404 - Gamma=30.4390349 (M=  36) - s=0.0100\n",
      " 105 - L= 2.9944404 - Gamma=30.4390594 (M=  36) - s=0.0100\n",
      " 106 - L= 2.9944404 - Gamma=30.4390634 (M=  36) - s=0.0100\n",
      " 107 - L= 2.9944404 - Gamma=30.4390550 (M=  36) - s=0.0100\n",
      " 108 - L= 2.9944405 - Gamma=30.4383152 (M=  36) - s=0.0100\n",
      " 109 - L= 2.9944405 - Gamma=30.4383160 (M=  36) - s=0.0100\n",
      " 110 - L= 2.9944405 - Gamma=30.4383106 (M=  36) - s=0.0100\n",
      " 111 - L= 2.9944405 - Gamma=30.4372025 (M=  36) - s=0.0100\n",
      " 112 - L= 2.9944405 - Gamma=30.4372018 (M=  36) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 113 - L= 2.9944406 - Gamma=30.4372320 (M=  36) - s=0.0100\n",
      " 114 - L= 2.9944406 - Gamma=30.4372451 (M=  36) - s=0.0100\n",
      " 115 - L= 2.9944406 - Gamma=30.4373428 (M=  36) - s=0.0100\n",
      " 116 - L= 2.9944406 - Gamma=30.4364944 (M=  36) - s=0.0100\n",
      " 117 - L= 2.9944406 - Gamma=30.4386393 (M=  36) - s=0.0100\n",
      " 118 - L= 2.9944406 - Gamma=30.4385744 (M=  36) - s=0.0100\n",
      " 119 - L= 2.9944407 - Gamma=30.4376387 (M=  36) - s=0.0100\n",
      " 120 - L= 2.9944407 - Gamma=30.4375697 (M=  36) - s=0.0100\n",
      " 121 - L= 2.9944407 - Gamma=30.4391528 (M=  36) - s=0.0100\n",
      " 122 - L= 2.9944407 - Gamma=30.4379110 (M=  36) - s=0.0100\n",
      " 123 - L= 2.9944407 - Gamma=30.4390231 (M=  36) - s=0.0100\n",
      " 124 - L= 2.9944407 - Gamma=30.4392099 (M=  36) - s=0.0100\n",
      " 125 - L= 2.9944407 - Gamma=30.4392034 (M=  36) - s=0.0100\n",
      " 126 - L= 2.9944407 - Gamma=30.4392092 (M=  36) - s=0.0100\n",
      " 127 - L= 2.9944407 - Gamma=30.4392083 (M=  36) - s=0.0100\n",
      " 128 - L= 2.9944407 - Gamma=30.4392080 (M=  36) - s=0.0100\n",
      " 129 - L= 2.9944408 - Gamma=30.4400948 (M=  36) - s=0.0100\n",
      " 130 - L= 2.9944408 - Gamma=30.4394329 (M=  36) - s=0.0100\n",
      " 131 - L= 2.9944408 - Gamma=30.4394451 (M=  36) - s=0.0100\n",
      " 132 - L= 2.9944408 - Gamma=30.4394455 (M=  36) - s=0.0100\n",
      " 133 - L= 2.9944408 - Gamma=30.4397474 (M=  36) - s=0.0100\n",
      " 134 - L= 2.9944408 - Gamma=30.4397474 (M=  36) - s=0.0100\n",
      "Stopping at iteration 134 - max_delta_ml=2.2008180747089398e-07\n",
      "L=2.994440774131381 - Gamma=30.439747437708156 (M=36) - s=0.01\n",
      "Initial alpha = [[ 0.03008213]]\n",
      "   1 - L=-411.4472469 - Gamma= 1.9999523 (M=   2) - s=0.0100\n",
      "   2 - L=-224.7324921 - Gamma= 2.9999074 (M=   3) - s=0.0100\n",
      "   3 - L=-151.8240260 - Gamma= 3.9997923 (M=   4) - s=0.0100\n",
      "   4 - L=-115.7257533 - Gamma= 4.9995766 (M=   5) - s=0.0100\n",
      "   5 - L=-83.9418410 - Gamma= 5.9993252 (M=   6) - s=0.0100\n",
      "   6 - L=-63.5142109 - Gamma= 6.9989461 (M=   7) - s=0.0100\n",
      "   7 - L=-46.5603407 - Gamma= 7.9984744 (M=   8) - s=0.0100\n",
      "   8 - L=-35.4490112 - Gamma= 8.9977734 (M=   9) - s=0.0100\n",
      "   9 - L=-26.4449508 - Gamma= 9.9969062 (M=  10) - s=0.0100\n",
      "  10 - L=-18.6558083 - Gamma=10.9958922 (M=  11) - s=0.0100\n",
      "  11 - L=-13.2278406 - Gamma=11.9944271 (M=  12) - s=0.0100\n",
      "  12 - L=-8.7266277 - Gamma=12.9926648 (M=  13) - s=0.0100\n",
      "  13 - L=-5.5650458 - Gamma=13.9897942 (M=  14) - s=0.0100\n",
      "  14 - L=-3.4406988 - Gamma=14.9856788 (M=  15) - s=0.0100\n",
      "  15 - L=-1.9373476 - Gamma=15.9804888 (M=  16) - s=0.0100\n",
      "  16 - L=-0.6905066 - Gamma=16.9737622 (M=  17) - s=0.0100\n",
      "  17 - L= 0.4058577 - Gamma=17.9666905 (M=  18) - s=0.0100\n",
      "  18 - L= 0.9781259 - Gamma=18.9530861 (M=  19) - s=0.0100\n",
      "  19 - L= 1.5116140 - Gamma=19.9386794 (M=  20) - s=0.0100\n",
      "  20 - L= 1.9673611 - Gamma=20.9223734 (M=  21) - s=0.0100\n",
      "  21 - L= 2.2977713 - Gamma=21.8974040 (M=  22) - s=0.0100\n",
      "  22 - L= 2.4893248 - Gamma=22.8612532 (M=  23) - s=0.0100\n",
      "  23 - L= 2.6352700 - Gamma=23.8158365 (M=  24) - s=0.0100\n",
      "  24 - L= 2.7631360 - Gamma=24.7664439 (M=  25) - s=0.0100\n",
      "  25 - L= 2.8366101 - Gamma=25.6889766 (M=  26) - s=0.0100\n",
      "  26 - L= 2.8934060 - Gamma=26.5927757 (M=  27) - s=0.0100\n",
      "  27 - L= 2.9278958 - Gamma=27.4574616 (M=  28) - s=0.0100\n",
      "  28 - L= 2.9540031 - Gamma=28.2887195 (M=  29) - s=0.0100\n",
      "  29 - L= 2.9623939 - Gamma=28.9736368 (M=  30) - s=0.0100\n",
      "  30 - L= 2.9700524 - Gamma=28.9733346 (M=  30) - s=0.0100\n",
      "  31 - L= 2.9770189 - Gamma=29.6379573 (M=  31) - s=0.0100\n",
      "  32 - L= 2.9815827 - Gamma=29.6368884 (M=  31) - s=0.0100\n",
      "  33 - L= 2.9856358 - Gamma=29.6388843 (M=  31) - s=0.0100\n",
      "  34 - L= 2.9896407 - Gamma=29.6479875 (M=  31) - s=0.0100\n",
      "  35 - L= 2.9926539 - Gamma=29.6497377 (M=  31) - s=0.0100\n",
      "  36 - L= 2.9956624 - Gamma=29.6586523 (M=  31) - s=0.0100\n",
      "  37 - L= 2.9979450 - Gamma=29.6595719 (M=  31) - s=0.0100\n",
      "  38 - L= 3.0000332 - Gamma=29.6484806 (M=  31) - s=0.0100\n",
      "  39 - L= 3.0019473 - Gamma=29.6496163 (M=  31) - s=0.0100\n",
      "  40 - L= 3.0037567 - Gamma=30.1051882 (M=  32) - s=0.0100\n",
      "  41 - L= 3.0050607 - Gamma=30.1056749 (M=  32) - s=0.0100\n",
      "  42 - L= 3.0061940 - Gamma=30.1124829 (M=  32) - s=0.0100\n",
      "  43 - L= 3.0072617 - Gamma=30.1127904 (M=  32) - s=0.0100\n",
      "  44 - L= 3.0078957 - Gamma=30.1129006 (M=  32) - s=0.0100\n",
      "  45 - L= 3.0084826 - Gamma=30.1149661 (M=  32) - s=0.0100\n",
      "  46 - L= 3.0089592 - Gamma=30.1125825 (M=  32) - s=0.0100\n",
      "  47 - L= 3.0094084 - Gamma=30.0526809 (M=  32) - s=0.0100\n",
      "  48 - L= 3.0097690 - Gamma=30.0523996 (M=  32) - s=0.0100\n",
      "  49 - L= 3.0100483 - Gamma=30.0614429 (M=  32) - s=0.0100\n",
      "  50 - L= 3.0102839 - Gamma=30.0611572 (M=  32) - s=0.0100\n",
      "  51 - L= 3.0105191 - Gamma=30.0657749 (M=  32) - s=0.0100\n",
      "  52 - L= 3.0106245 - Gamma=30.2133121 (M=  33) - s=0.0100\n",
      "  53 - L= 3.0107115 - Gamma=30.3428382 (M=  34) - s=0.0100\n",
      "  54 - L= 3.0107776 - Gamma=30.2668482 (M=  34) - s=0.0100\n",
      "  55 - L= 3.0108515 - Gamma=30.3779492 (M=  35) - s=0.0100\n",
      "  56 - L= 3.0109124 - Gamma=30.3891602 (M=  35) - s=0.0100\n",
      "  57 - L= 3.0109635 - Gamma=30.3479745 (M=  35) - s=0.0100\n",
      "  58 - L= 3.0110069 - Gamma=30.3479652 (M=  35) - s=0.0100\n",
      "  59 - L= 3.0110381 - Gamma=30.3436895 (M=  35) - s=0.0100\n",
      "  60 - L= 3.0110677 - Gamma=30.4204703 (M=  36) - s=0.0100\n",
      "  61 - L= 3.0111240 - Gamma=30.3421589 (M=  36) - s=0.0100\n",
      "  62 - L= 3.0111818 - Gamma=30.4287712 (M=  36) - s=0.0100\n",
      "  63 - L= 3.0112236 - Gamma=30.4066866 (M=  36) - s=0.0100\n",
      "  64 - L= 3.0112502 - Gamma=30.4735682 (M=  36) - s=0.0100\n",
      "  65 - L= 3.0112999 - Gamma=30.3917795 (M=  36) - s=0.0100\n",
      "  66 - L= 3.0113571 - Gamma=30.4681758 (M=  36) - s=0.0100\n",
      "  67 - L= 3.0114084 - Gamma=30.3638822 (M=  36) - s=0.0100\n",
      "  68 - L= 3.0114613 - Gamma=30.4496173 (M=  36) - s=0.0100\n",
      "  69 - L= 3.0115271 - Gamma=30.3439875 (M=  36) - s=0.0100\n",
      "  70 - L= 3.0115910 - Gamma=30.4416526 (M=  37) - s=0.0100\n",
      "  71 - L= 3.0116233 - Gamma=30.3848579 (M=  36) - s=0.0100\n",
      "  72 - L= 3.0116498 - Gamma=30.3469594 (M=  35) - s=0.0100\n",
      "  73 - L= 3.0116900 - Gamma=30.3063415 (M=  35) - s=0.0100\n",
      "  74 - L= 3.0117382 - Gamma=30.3807735 (M=  35) - s=0.0100\n",
      "  75 - L= 3.0117948 - Gamma=30.3597732 (M=  35) - s=0.0100\n",
      "  76 - L= 3.0118598 - Gamma=30.4307395 (M=  35) - s=0.0100\n",
      "  77 - L= 3.0119230 - Gamma=30.4657668 (M=  35) - s=0.0100\n",
      "  78 - L= 3.0119794 - Gamma=30.4631193 (M=  35) - s=0.0100\n",
      "  79 - L= 3.0120246 - Gamma=30.4368703 (M=  35) - s=0.0100\n",
      "  80 - L= 3.0120606 - Gamma=30.4439117 (M=  35) - s=0.0100\n",
      "  81 - L= 3.0120869 - Gamma=30.4394594 (M=  35) - s=0.0100\n",
      "  82 - L= 3.0121074 - Gamma=30.3757398 (M=  35) - s=0.0100\n",
      "  83 - L= 3.0121356 - Gamma=30.4276194 (M=  35) - s=0.0100\n",
      "  84 - L= 3.0121689 - Gamma=30.4730571 (M=  35) - s=0.0100\n",
      "  85 - L= 3.0122105 - Gamma=30.3920994 (M=  35) - s=0.0100\n",
      "  86 - L= 3.0122462 - Gamma=30.4702824 (M=  36) - s=0.0100\n",
      "  87 - L= 3.0122541 - Gamma=30.4574445 (M=  35) - s=0.0100\n",
      "  88 - L= 3.0123157 - Gamma=30.4322781 (M=  35) - s=0.0100\n",
      "  89 - L= 3.0123554 - Gamma=30.3874096 (M=  35) - s=0.0100\n",
      "  90 - L= 3.0123819 - Gamma=30.3161839 (M=  34) - s=0.0100\n",
      "  91 - L= 3.0124342 - Gamma=30.4018392 (M=  34) - s=0.0100\n",
      "  92 - L= 3.0124899 - Gamma=30.4673034 (M=  34) - s=0.0100\n",
      "  93 - L= 3.0125167 - Gamma=30.4880302 (M=  34) - s=0.0100\n",
      "  94 - L= 3.0125407 - Gamma=30.4886771 (M=  34) - s=0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bram van Es\\DEV\\RexR\\rvm.py:289: RuntimeWarning: invalid value encountered in log\n",
      "  change = np.abs(np.log(newAlpha) - np.log(self.Alpha[j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  95 - L= 3.0125599 - Gamma=30.4548073 (M=  34) - s=0.0100\n",
      "  96 - L= 3.0125798 - Gamma=30.4390262 (M=  34) - s=0.0100\n",
      "  97 - L= 3.0126044 - Gamma=30.5075810 (M=  35) - s=0.0100\n",
      "  98 - L= 3.0126163 - Gamma=30.5057769 (M=  35) - s=0.0100\n",
      "  99 - L= 3.0126273 - Gamma=30.5045169 (M=  35) - s=0.0100\n",
      " 100 - L= 3.0126397 - Gamma=30.5304397 (M=  35) - s=0.0100\n",
      " 101 - L= 3.0126540 - Gamma=30.4993036 (M=  35) - s=0.0100\n",
      " 102 - L= 3.0126632 - Gamma=30.4864499 (M=  35) - s=0.0100\n",
      " 103 - L= 3.0126705 - Gamma=30.4839657 (M=  35) - s=0.0100\n",
      " 104 - L= 3.0126777 - Gamma=30.5190191 (M=  35) - s=0.0100\n",
      " 105 - L= 3.0126886 - Gamma=30.5066104 (M=  35) - s=0.0100\n",
      " 106 - L= 3.0126949 - Gamma=30.5242400 (M=  35) - s=0.0100\n",
      " 107 - L= 3.0127019 - Gamma=30.5245671 (M=  35) - s=0.0100\n",
      " 108 - L= 3.0127074 - Gamma=30.5045558 (M=  35) - s=0.0100\n",
      " 109 - L= 3.0127119 - Gamma=30.5315556 (M=  35) - s=0.0100\n",
      " 110 - L= 3.0127161 - Gamma=30.5315110 (M=  35) - s=0.0100\n",
      " 111 - L= 3.0127199 - Gamma=30.5315038 (M=  35) - s=0.0100\n",
      " 112 - L= 3.0127235 - Gamma=30.5315085 (M=  35) - s=0.0100\n",
      " 113 - L= 3.0127260 - Gamma=30.5308798 (M=  35) - s=0.0100\n",
      " 114 - L= 3.0127283 - Gamma=30.5309155 (M=  35) - s=0.0100\n",
      " 115 - L= 3.0127302 - Gamma=30.5256191 (M=  35) - s=0.0100\n",
      " 116 - L= 3.0127324 - Gamma=30.5380838 (M=  35) - s=0.0100\n",
      " 117 - L= 3.0127351 - Gamma=30.5562742 (M=  35) - s=0.0100\n",
      " 118 - L= 3.0127395 - Gamma=30.5376871 (M=  35) - s=0.0100\n",
      " 119 - L= 3.0127414 - Gamma=30.5375838 (M=  35) - s=0.0100\n",
      " 120 - L= 3.0127431 - Gamma=30.5317308 (M=  35) - s=0.0100\n",
      " 121 - L= 3.0127443 - Gamma=30.5306979 (M=  35) - s=0.0100\n",
      " 122 - L= 3.0127455 - Gamma=30.5439692 (M=  35) - s=0.0100\n",
      " 123 - L= 3.0127472 - Gamma=30.5387191 (M=  35) - s=0.0100\n",
      " 124 - L= 3.0127489 - Gamma=30.5474833 (M=  35) - s=0.0100\n",
      " 125 - L= 3.0127500 - Gamma=30.5379805 (M=  35) - s=0.0100\n",
      " 126 - L= 3.0127511 - Gamma=30.5367698 (M=  35) - s=0.0100\n",
      " 127 - L= 3.0127522 - Gamma=30.5367583 (M=  35) - s=0.0100\n",
      " 128 - L= 3.0127532 - Gamma=30.5367730 (M=  35) - s=0.0100\n",
      " 129 - L= 3.0127541 - Gamma=30.5486076 (M=  35) - s=0.0100\n",
      " 130 - L= 3.0127550 - Gamma=30.5487666 (M=  35) - s=0.0100\n",
      " 131 - L= 3.0127560 - Gamma=30.5482515 (M=  35) - s=0.0100\n",
      " 132 - L= 3.0127567 - Gamma=30.5470471 (M=  35) - s=0.0100\n",
      " 133 - L= 3.0127574 - Gamma=30.5467094 (M=  35) - s=0.0100\n",
      " 134 - L= 3.0127580 - Gamma=30.5467122 (M=  35) - s=0.0100\n",
      " 135 - L= 3.0127585 - Gamma=30.5467284 (M=  35) - s=0.0100\n",
      " 136 - L= 3.0127590 - Gamma=30.5466393 (M=  35) - s=0.0100\n",
      " 137 - L= 3.0127594 - Gamma=30.5440007 (M=  35) - s=0.0100\n",
      " 138 - L= 3.0127599 - Gamma=30.5465540 (M=  35) - s=0.0100\n",
      " 139 - L= 3.0127604 - Gamma=30.5432788 (M=  35) - s=0.0100\n",
      " 140 - L= 3.0127609 - Gamma=30.5481097 (M=  35) - s=0.0100\n",
      " 141 - L= 3.0127616 - Gamma=30.5406974 (M=  35) - s=0.0100\n",
      " 142 - L= 3.0127621 - Gamma=30.5496855 (M=  35) - s=0.0100\n",
      " 143 - L= 3.0127626 - Gamma=30.5489992 (M=  35) - s=0.0100\n",
      " 144 - L= 3.0127631 - Gamma=30.5490125 (M=  35) - s=0.0100\n",
      " 145 - L= 3.0127635 - Gamma=30.5490116 (M=  35) - s=0.0100\n",
      " 146 - L= 3.0127638 - Gamma=30.5488475 (M=  35) - s=0.0100\n",
      " 147 - L= 3.0127641 - Gamma=30.5528249 (M=  35) - s=0.0100\n",
      " 148 - L= 3.0127644 - Gamma=30.5505859 (M=  35) - s=0.0100\n",
      " 149 - L= 3.0127647 - Gamma=30.5568754 (M=  35) - s=0.0100\n",
      " 150 - L= 3.0127652 - Gamma=30.5503453 (M=  35) - s=0.0100\n",
      " 151 - L= 3.0127654 - Gamma=30.5556316 (M=  35) - s=0.0100\n",
      " 152 - L= 3.0127656 - Gamma=30.5554622 (M=  35) - s=0.0100\n",
      " 153 - L= 3.0127657 - Gamma=30.5538380 (M=  35) - s=0.0100\n",
      " 154 - L= 3.0127659 - Gamma=30.5521027 (M=  35) - s=0.0100\n",
      " 155 - L= 3.0127660 - Gamma=30.5544399 (M=  35) - s=0.0100\n",
      " 156 - L= 3.0127662 - Gamma=30.5507685 (M=  35) - s=0.0100\n",
      " 157 - L= 3.0127663 - Gamma=30.5507767 (M=  35) - s=0.0100\n",
      " 158 - L= 3.0127664 - Gamma=30.5547632 (M=  35) - s=0.0100\n",
      " 159 - L= 3.0127665 - Gamma=30.5550111 (M=  35) - s=0.0100\n",
      " 160 - L= 3.0127666 - Gamma=30.5549782 (M=  35) - s=0.0100\n",
      " 161 - L= 3.0127666 - Gamma=30.5546780 (M=  35) - s=0.0100\n",
      " 162 - L= 3.0127667 - Gamma=30.5537485 (M=  35) - s=0.0100\n",
      " 163 - L= 3.0127667 - Gamma=30.5565662 (M=  35) - s=0.0100\n",
      " 164 - L= 3.0127668 - Gamma=30.5585758 (M=  35) - s=0.0100\n",
      " 165 - L= 3.0127669 - Gamma=30.5559211 (M=  35) - s=0.0100\n",
      " 166 - L= 3.0127669 - Gamma=30.5559161 (M=  35) - s=0.0100\n",
      " 167 - L= 3.0127670 - Gamma=30.5559414 (M=  35) - s=0.0100\n",
      " 168 - L= 3.0127670 - Gamma=30.5559266 (M=  35) - s=0.0100\n",
      " 169 - L= 3.0127670 - Gamma=30.5559311 (M=  35) - s=0.0100\n",
      " 170 - L= 3.0127671 - Gamma=30.5558548 (M=  35) - s=0.0100\n",
      " 171 - L= 3.0127671 - Gamma=30.5551182 (M=  35) - s=0.0100\n",
      " 172 - L= 3.0127672 - Gamma=30.5583941 (M=  36) - s=0.0100\n",
      " 173 - L= 3.0127672 - Gamma=30.5607605 (M=  36) - s=0.0100\n",
      " 174 - L= 3.0127673 - Gamma=30.5588684 (M=  36) - s=0.0100\n",
      " 175 - L= 3.0127673 - Gamma=30.5618895 (M=  36) - s=0.0100\n",
      " 176 - L= 3.0127674 - Gamma=30.5609101 (M=  36) - s=0.0100\n",
      " 177 - L= 3.0127674 - Gamma=30.5606824 (M=  36) - s=0.0100\n",
      " 178 - L= 3.0127675 - Gamma=30.5638683 (M=  36) - s=0.0100\n",
      " 179 - L= 3.0127675 - Gamma=30.5615927 (M=  36) - s=0.0100\n",
      " 180 - L= 3.0127676 - Gamma=30.5640040 (M=  36) - s=0.0100\n",
      " 181 - L= 3.0127676 - Gamma=30.5631176 (M=  36) - s=0.0100\n",
      " 182 - L= 3.0127676 - Gamma=30.5631164 (M=  36) - s=0.0100\n",
      " 183 - L= 3.0127677 - Gamma=30.5624349 (M=  36) - s=0.0100\n",
      " 184 - L= 3.0127677 - Gamma=30.5647570 (M=  36) - s=0.0100\n",
      " 185 - L= 3.0127677 - Gamma=30.5628013 (M=  36) - s=0.0100\n",
      " 186 - L= 3.0127678 - Gamma=30.5664350 (M=  36) - s=0.0100\n",
      " 187 - L= 3.0127679 - Gamma=30.5682520 (M=  36) - s=0.0100\n",
      " 188 - L= 3.0127679 - Gamma=30.5673682 (M=  36) - s=0.0100\n",
      " 189 - L= 3.0127680 - Gamma=30.5671005 (M=  36) - s=0.0100\n",
      " 190 - L= 3.0127680 - Gamma=30.5670245 (M=  36) - s=0.0100\n",
      " 191 - L= 3.0127680 - Gamma=30.5692435 (M=  36) - s=0.0100\n",
      " 192 - L= 3.0127681 - Gamma=30.5667019 (M=  36) - s=0.0100\n",
      " 193 - L= 3.0127682 - Gamma=30.5704422 (M=  36) - s=0.0100\n",
      " 194 - L= 3.0127682 - Gamma=30.5695734 (M=  36) - s=0.0100\n",
      " 195 - L= 3.0127683 - Gamma=30.5720804 (M=  36) - s=0.0100\n",
      " 196 - L= 3.0127683 - Gamma=30.5701742 (M=  36) - s=0.0100\n",
      " 197 - L= 3.0127684 - Gamma=30.5729858 (M=  36) - s=0.0100\n",
      " 198 - L= 3.0127684 - Gamma=30.5729280 (M=  36) - s=0.0100\n",
      " 199 - L= 3.0127684 - Gamma=30.5754309 (M=  36) - s=0.0100\n",
      " 200 - L= 3.0127685 - Gamma=30.5745202 (M=  36) - s=0.0100\n",
      "Initial alpha = [[ 0.02815097]]\n",
      "   1 - L=-330.7718267 - Gamma= 1.9999431 (M=   2) - s=0.0100\n",
      "   2 - L=-228.4009324 - Gamma= 2.9998695 (M=   3) - s=0.0100\n",
      "   3 - L=-142.4705855 - Gamma= 3.9997176 (M=   4) - s=0.0100\n",
      "   4 - L=-104.0016970 - Gamma= 4.9994743 (M=   5) - s=0.0100\n",
      "   5 - L=-79.2292113 - Gamma= 5.9991474 (M=   6) - s=0.0100\n",
      "   6 - L=-53.2558638 - Gamma= 6.9986631 (M=   7) - s=0.0100\n",
      "   7 - L=-42.6992673 - Gamma= 7.9978851 (M=   8) - s=0.0100\n",
      "   8 - L=-33.1563311 - Gamma= 8.9968667 (M=   9) - s=0.0100\n",
      "   9 - L=-24.8081892 - Gamma= 9.9958457 (M=  10) - s=0.0100\n",
      "  10 - L=-19.4298540 - Gamma=10.9943698 (M=  11) - s=0.0100\n",
      "  11 - L=-14.4373974 - Gamma=11.9927749 (M=  12) - s=0.0100\n",
      "  12 - L=-10.3430680 - Gamma=12.9907905 (M=  13) - s=0.0100\n",
      "  13 - L=-6.7732224 - Gamma=13.9886616 (M=  14) - s=0.0100\n",
      "  14 - L=-4.4936277 - Gamma=14.9852992 (M=  15) - s=0.0100\n",
      "  15 - L=-2.7890540 - Gamma=15.9807340 (M=  16) - s=0.0100\n",
      "  16 - L=-1.1824798 - Gamma=16.9752748 (M=  17) - s=0.0100\n",
      "  17 - L=-0.0685476 - Gamma=17.9681008 (M=  18) - s=0.0100\n",
      "  18 - L= 0.7180297 - Gamma=18.9582354 (M=  19) - s=0.0100\n",
      "  19 - L= 1.4159590 - Gamma=19.9442672 (M=  20) - s=0.0100\n",
      "  20 - L= 1.8534955 - Gamma=20.9278585 (M=  21) - s=0.0100\n",
      "  21 - L= 2.1648413 - Gamma=21.9048023 (M=  22) - s=0.0100\n",
      "  22 - L= 2.4709737 - Gamma=22.8814492 (M=  23) - s=0.0100\n",
      "  23 - L= 2.6660156 - Gamma=23.8439659 (M=  24) - s=0.0100\n",
      "  24 - L= 2.7676970 - Gamma=24.7818972 (M=  25) - s=0.0100\n",
      "  25 - L= 2.8233221 - Gamma=25.6803832 (M=  26) - s=0.0100\n",
      "  26 - L= 2.8698325 - Gamma=26.5706335 (M=  27) - s=0.0100\n",
      "  27 - L= 2.9041915 - Gamma=26.5711630 (M=  27) - s=0.0100\n",
      "  28 - L= 2.9384696 - Gamma=27.4318806 (M=  28) - s=0.0100\n",
      "  29 - L= 2.9578527 - Gamma=28.2278360 (M=  29) - s=0.0100\n",
      "  30 - L= 2.9713423 - Gamma=28.9323854 (M=  30) - s=0.0100\n",
      "  31 - L= 2.9826581 - Gamma=28.9333796 (M=  30) - s=0.0100\n",
      "  32 - L= 2.9920953 - Gamma=29.6350735 (M=  31) - s=0.0100\n",
      "  33 - L= 2.9987285 - Gamma=29.6348934 (M=  31) - s=0.0100\n",
      "  34 - L= 3.0033702 - Gamma=29.6352348 (M=  31) - s=0.0100\n",
      "  35 - L= 3.0069098 - Gamma=29.6495955 (M=  31) - s=0.0100\n",
      "  36 - L= 3.0098490 - Gamma=30.1707742 (M=  32) - s=0.0100\n",
      "  37 - L= 3.0116488 - Gamma=30.1886742 (M=  32) - s=0.0100\n",
      "  38 - L= 3.0130235 - Gamma=30.1892350 (M=  32) - s=0.0100\n",
      "  39 - L= 3.0142043 - Gamma=30.1870930 (M=  32) - s=0.0100\n",
      "  40 - L= 3.0152772 - Gamma=30.0962851 (M=  32) - s=0.0100\n",
      "  41 - L= 3.0165362 - Gamma=30.1804158 (M=  32) - s=0.0100\n",
      "  42 - L= 3.0176012 - Gamma=30.0639094 (M=  32) - s=0.0100\n",
      "  43 - L= 3.0185396 - Gamma=30.0633421 (M=  32) - s=0.0100\n",
      "  44 - L= 3.0193557 - Gamma=30.0647132 (M=  32) - s=0.0100\n",
      "  45 - L= 3.0201408 - Gamma=30.3955284 (M=  33) - s=0.0100\n",
      "  46 - L= 3.0207422 - Gamma=30.3941152 (M=  33) - s=0.0100\n",
      "  47 - L= 3.0210608 - Gamma=30.4193095 (M=  33) - s=0.0100\n",
      "  48 - L= 3.0213745 - Gamma=30.4197891 (M=  33) - s=0.0100\n",
      "  49 - L= 3.0215766 - Gamma=30.4126046 (M=  33) - s=0.0100\n",
      "  50 - L= 3.0217567 - Gamma=30.4122744 (M=  33) - s=0.0100\n",
      "  51 - L= 3.0219123 - Gamma=30.4128543 (M=  33) - s=0.0100\n",
      "  52 - L= 3.0220570 - Gamma=30.4146181 (M=  33) - s=0.0100\n",
      "  53 - L= 3.0221862 - Gamma=30.4154889 (M=  33) - s=0.0100\n",
      "  54 - L= 3.0222685 - Gamma=30.4444857 (M=  33) - s=0.0100\n",
      "  55 - L= 3.0223483 - Gamma=30.4111662 (M=  33) - s=0.0100\n",
      "  56 - L= 3.0224228 - Gamma=30.4133270 (M=  33) - s=0.0100\n",
      "  57 - L= 3.0224920 - Gamma=30.4143626 (M=  33) - s=0.0100\n",
      "  58 - L= 3.0225477 - Gamma=30.4143265 (M=  33) - s=0.0100\n",
      "  59 - L= 3.0225974 - Gamma=30.3589666 (M=  33) - s=0.0100\n",
      "  60 - L= 3.0226612 - Gamma=30.3211856 (M=  33) - s=0.0100\n",
      "  61 - L= 3.0227375 - Gamma=30.4438175 (M=  34) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62 - L= 3.0227958 - Gamma=30.5522041 (M=  35) - s=0.0100\n",
      "  63 - L= 3.0228564 - Gamma=30.5503711 (M=  35) - s=0.0100\n",
      "  64 - L= 3.0229070 - Gamma=30.5503510 (M=  35) - s=0.0100\n",
      "  65 - L= 3.0229515 - Gamma=30.4922876 (M=  35) - s=0.0100\n",
      "  66 - L= 3.0229936 - Gamma=30.4211825 (M=  35) - s=0.0100\n",
      "  67 - L= 3.0230329 - Gamma=30.4212376 (M=  35) - s=0.0100\n",
      "  68 - L= 3.0230617 - Gamma=30.4904592 (M=  35) - s=0.0100\n",
      "  69 - L= 3.0230845 - Gamma=30.5040355 (M=  35) - s=0.0100\n",
      "  70 - L= 3.0231140 - Gamma=30.4755965 (M=  35) - s=0.0100\n",
      "  71 - L= 3.0231382 - Gamma=30.4849023 (M=  35) - s=0.0100\n",
      "  72 - L= 3.0231564 - Gamma=30.4447151 (M=  35) - s=0.0100\n",
      "  73 - L= 3.0231678 - Gamma=30.4488475 (M=  35) - s=0.0100\n",
      "  74 - L= 3.0231790 - Gamma=30.4094487 (M=  35) - s=0.0100\n",
      "  75 - L= 3.0231895 - Gamma=30.4487407 (M=  35) - s=0.0100\n",
      "  76 - L= 3.0231973 - Gamma=30.4562194 (M=  35) - s=0.0100\n",
      "  77 - L= 3.0232020 - Gamma=30.4461944 (M=  35) - s=0.0100\n",
      "  78 - L= 3.0232062 - Gamma=30.4209689 (M=  35) - s=0.0100\n",
      "  79 - L= 3.0232097 - Gamma=30.4026852 (M=  35) - s=0.0100\n",
      "  80 - L= 3.0232138 - Gamma=30.3914264 (M=  35) - s=0.0100\n",
      "  81 - L= 3.0232180 - Gamma=30.4150551 (M=  35) - s=0.0100\n",
      "  82 - L= 3.0232205 - Gamma=30.4155831 (M=  35) - s=0.0100\n",
      "  83 - L= 3.0232230 - Gamma=30.4151901 (M=  35) - s=0.0100\n",
      "  84 - L= 3.0232256 - Gamma=30.3987145 (M=  35) - s=0.0100\n",
      "  85 - L= 3.0232283 - Gamma=30.3981998 (M=  35) - s=0.0100\n",
      "  86 - L= 3.0232310 - Gamma=30.4220259 (M=  36) - s=0.0100\n",
      "  87 - L= 3.0232338 - Gamma=30.4153715 (M=  36) - s=0.0100\n",
      "  88 - L= 3.0232363 - Gamma=30.4156585 (M=  36) - s=0.0100\n",
      "  89 - L= 3.0232385 - Gamma=30.4155930 (M=  36) - s=0.0100\n",
      "  90 - L= 3.0232402 - Gamma=30.4190024 (M=  36) - s=0.0100\n",
      "  91 - L= 3.0232425 - Gamma=30.4104658 (M=  36) - s=0.0100\n",
      "  92 - L= 3.0232445 - Gamma=30.4090161 (M=  36) - s=0.0100\n",
      "  93 - L= 3.0232466 - Gamma=30.4280013 (M=  36) - s=0.0100\n",
      "  94 - L= 3.0232484 - Gamma=30.4276792 (M=  36) - s=0.0100\n",
      "  95 - L= 3.0232499 - Gamma=30.4299189 (M=  36) - s=0.0100\n",
      "  96 - L= 3.0232514 - Gamma=30.4300040 (M=  36) - s=0.0100\n",
      "  97 - L= 3.0232529 - Gamma=30.4481699 (M=  37) - s=0.0100\n",
      "  98 - L= 3.0232552 - Gamma=30.4324419 (M=  37) - s=0.0100\n",
      "  99 - L= 3.0232574 - Gamma=30.4341892 (M=  37) - s=0.0100\n",
      " 100 - L= 3.0232591 - Gamma=30.4265597 (M=  37) - s=0.0100\n",
      " 101 - L= 3.0232614 - Gamma=30.4478242 (M=  37) - s=0.0100\n",
      " 102 - L= 3.0232652 - Gamma=30.4398339 (M=  37) - s=0.0100\n",
      " 103 - L= 3.0232665 - Gamma=30.4562705 (M=  37) - s=0.0100\n",
      " 104 - L= 3.0232680 - Gamma=30.4431837 (M=  37) - s=0.0100\n",
      " 105 - L= 3.0232700 - Gamma=30.4590654 (M=  37) - s=0.0100\n",
      " 106 - L= 3.0232725 - Gamma=30.4389273 (M=  37) - s=0.0100\n",
      " 107 - L= 3.0232742 - Gamma=30.4421676 (M=  37) - s=0.0100\n",
      " 108 - L= 3.0232765 - Gamma=30.4330589 (M=  37) - s=0.0100\n",
      " 109 - L= 3.0232780 - Gamma=30.4324007 (M=  37) - s=0.0100\n",
      " 110 - L= 3.0232795 - Gamma=30.4189362 (M=  37) - s=0.0100\n",
      " 111 - L= 3.0232823 - Gamma=30.4428435 (M=  37) - s=0.0100\n",
      " 112 - L= 3.0232843 - Gamma=30.4424923 (M=  37) - s=0.0100\n",
      " 113 - L= 3.0232860 - Gamma=30.4486259 (M=  37) - s=0.0100\n",
      " 114 - L= 3.0232877 - Gamma=30.4487238 (M=  37) - s=0.0100\n",
      " 115 - L= 3.0232893 - Gamma=30.4501885 (M=  37) - s=0.0100\n",
      " 116 - L= 3.0232909 - Gamma=30.4365502 (M=  37) - s=0.0100\n",
      " 117 - L= 3.0232933 - Gamma=30.4540488 (M=  37) - s=0.0100\n",
      " 118 - L= 3.0232953 - Gamma=30.4455415 (M=  37) - s=0.0100\n",
      " 119 - L= 3.0232976 - Gamma=30.4666027 (M=  37) - s=0.0100\n",
      " 120 - L= 3.0233003 - Gamma=30.4484177 (M=  37) - s=0.0100\n",
      " 121 - L= 3.0233020 - Gamma=30.4507395 (M=  37) - s=0.0100\n",
      " 122 - L= 3.0233036 - Gamma=30.4341577 (M=  37) - s=0.0100\n",
      " 123 - L= 3.0233060 - Gamma=30.4558438 (M=  37) - s=0.0100\n",
      " 124 - L= 3.0233084 - Gamma=30.4494791 (M=  37) - s=0.0100\n",
      " 125 - L= 3.0233104 - Gamma=30.4405185 (M=  37) - s=0.0100\n",
      " 126 - L= 3.0233125 - Gamma=30.4440672 (M=  37) - s=0.0100\n",
      " 127 - L= 3.0233139 - Gamma=30.4428724 (M=  37) - s=0.0100\n",
      " 128 - L= 3.0233152 - Gamma=30.4430056 (M=  37) - s=0.0100\n",
      " 129 - L= 3.0233165 - Gamma=30.4581475 (M=  37) - s=0.0100\n",
      " 130 - L= 3.0233185 - Gamma=30.4420551 (M=  37) - s=0.0100\n",
      " 131 - L= 3.0233202 - Gamma=30.4562533 (M=  37) - s=0.0100\n",
      " 132 - L= 3.0233217 - Gamma=30.4483694 (M=  37) - s=0.0100\n",
      " 133 - L= 3.0233234 - Gamma=30.4498199 (M=  37) - s=0.0100\n",
      " 134 - L= 3.0233251 - Gamma=30.4675641 (M=  37) - s=0.0100\n",
      " 135 - L= 3.0233282 - Gamma=30.4469078 (M=  37) - s=0.0100\n",
      " 136 - L= 3.0233301 - Gamma=30.4283968 (M=  37) - s=0.0100\n",
      " 137 - L= 3.0233318 - Gamma=30.4280619 (M=  37) - s=0.0100\n",
      " 138 - L= 3.0233334 - Gamma=30.4447234 (M=  37) - s=0.0100\n",
      " 139 - L= 3.0233349 - Gamma=30.4393966 (M=  37) - s=0.0100\n",
      " 140 - L= 3.0233370 - Gamma=30.4577459 (M=  37) - s=0.0100\n",
      " 141 - L= 3.0233398 - Gamma=30.4468909 (M=  37) - s=0.0100\n",
      " 142 - L= 3.0233415 - Gamma=30.4310210 (M=  37) - s=0.0100\n",
      " 143 - L= 3.0233438 - Gamma=30.4471508 (M=  37) - s=0.0100\n",
      " 144 - L= 3.0233456 - Gamma=30.4467125 (M=  37) - s=0.0100\n",
      " 145 - L= 3.0233474 - Gamma=30.4527031 (M=  37) - s=0.0100\n",
      " 146 - L= 3.0233488 - Gamma=30.4683301 (M=  37) - s=0.0100\n",
      " 147 - L= 3.0233506 - Gamma=30.4519690 (M=  37) - s=0.0100\n",
      " 148 - L= 3.0233527 - Gamma=30.4423299 (M=  37) - s=0.0100\n",
      " 149 - L= 3.0233550 - Gamma=30.4439987 (M=  37) - s=0.0100\n",
      " 150 - L= 3.0233579 - Gamma=30.4662442 (M=  37) - s=0.0100\n",
      " 151 - L= 3.0233601 - Gamma=30.4476900 (M=  37) - s=0.0100\n",
      " 152 - L= 3.0233620 - Gamma=30.4469163 (M=  37) - s=0.0100\n",
      " 153 - L= 3.0233639 - Gamma=30.4493402 (M=  37) - s=0.0100\n",
      " 154 - L= 3.0233659 - Gamma=30.4432078 (M=  37) - s=0.0100\n",
      " 155 - L= 3.0233682 - Gamma=30.4629175 (M=  37) - s=0.0100\n",
      " 156 - L= 3.0233709 - Gamma=30.4517361 (M=  37) - s=0.0100\n",
      " 157 - L= 3.0233742 - Gamma=30.4560287 (M=  37) - s=0.0100\n",
      " 158 - L= 3.0233759 - Gamma=30.4382470 (M=  37) - s=0.0100\n",
      " 159 - L= 3.0233773 - Gamma=30.4379394 (M=  37) - s=0.0100\n",
      " 160 - L= 3.0233788 - Gamma=30.4219142 (M=  37) - s=0.0100\n",
      " 161 - L= 3.0233819 - Gamma=30.4443044 (M=  37) - s=0.0100\n",
      " 162 - L= 3.0233842 - Gamma=30.4598363 (M=  37) - s=0.0100\n",
      " 163 - L= 3.0233865 - Gamma=30.4397739 (M=  37) - s=0.0100\n",
      " 164 - L= 3.0233895 - Gamma=30.4276681 (M=  37) - s=0.0100\n",
      " 165 - L= 3.0233916 - Gamma=30.4292409 (M=  37) - s=0.0100\n",
      " 166 - L= 3.0233937 - Gamma=30.4356011 (M=  37) - s=0.0100\n",
      " 167 - L= 3.0233956 - Gamma=30.4294606 (M=  37) - s=0.0100\n",
      " 168 - L= 3.0233984 - Gamma=30.4503913 (M=  37) - s=0.0100\n",
      " 169 - L= 3.0234005 - Gamma=30.4488531 (M=  37) - s=0.0100\n",
      " 170 - L= 3.0234022 - Gamma=30.4317780 (M=  37) - s=0.0100\n",
      " 171 - L= 3.0234049 - Gamma=30.4199127 (M=  37) - s=0.0100\n",
      " 172 - L= 3.0234084 - Gamma=30.4430760 (M=  37) - s=0.0100\n",
      " 173 - L= 3.0234106 - Gamma=30.4225643 (M=  37) - s=0.0100\n",
      " 174 - L= 3.0234130 - Gamma=30.4416539 (M=  37) - s=0.0100\n",
      " 175 - L= 3.0234158 - Gamma=30.4338923 (M=  37) - s=0.0100\n",
      " 176 - L= 3.0234186 - Gamma=30.4215819 (M=  37) - s=0.0100\n",
      " 177 - L= 3.0234209 - Gamma=30.4444266 (M=  38) - s=0.0100\n",
      " 178 - L= 3.0234229 - Gamma=30.4645943 (M=  39) - s=0.0100\n",
      " 179 - L= 3.0234264 - Gamma=30.4386165 (M=  39) - s=0.0100\n",
      " 180 - L= 3.0234289 - Gamma=30.4402641 (M=  39) - s=0.0100\n",
      " 181 - L= 3.0234314 - Gamma=30.4429826 (M=  39) - s=0.0100\n",
      " 182 - L= 3.0234342 - Gamma=30.4467657 (M=  39) - s=0.0100\n",
      " 183 - L= 3.0234366 - Gamma=30.4251108 (M=  39) - s=0.0100\n",
      " 184 - L= 3.0234411 - Gamma=30.4505506 (M=  39) - s=0.0100\n",
      " 185 - L= 3.0234469 - Gamma=30.4319239 (M=  39) - s=0.0100\n",
      " 186 - L= 3.0234527 - Gamma=30.4655106 (M=  39) - s=0.0100\n",
      " 187 - L= 3.0234588 - Gamma=30.4300693 (M=  39) - s=0.0100\n",
      " 188 - L= 3.0234637 - Gamma=30.4196162 (M=  39) - s=0.0100\n",
      " 189 - L= 3.0234673 - Gamma=30.4415068 (M=  39) - s=0.0100\n",
      " 190 - L= 3.0234738 - Gamma=30.4046484 (M=  39) - s=0.0100\n",
      " 191 - L= 3.0234827 - Gamma=30.3805828 (M=  39) - s=0.0100\n",
      " 192 - L= 3.0234837 - Gamma=30.3738850 (M=  38) - s=0.0100\n",
      " 193 - L= 3.0234943 - Gamma=30.4175359 (M=  38) - s=0.0100\n",
      " 194 - L= 3.0235037 - Gamma=30.4544274 (M=  38) - s=0.0100\n",
      " 195 - L= 3.0235113 - Gamma=30.4916477 (M=  39) - s=0.0100\n",
      " 196 - L= 3.0235215 - Gamma=30.4438270 (M=  39) - s=0.0100\n",
      " 197 - L= 3.0235296 - Gamma=30.4297937 (M=  39) - s=0.0100\n",
      " 198 - L= 3.0235400 - Gamma=30.4709241 (M=  39) - s=0.0100\n",
      " 199 - L= 3.0235405 - Gamma=30.4681687 (M=  38) - s=0.0100\n",
      " 200 - L= 3.0235569 - Gamma=30.4338518 (M=  38) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial alpha = [[ 0.02904541]]\n",
      "   1 - L=-352.9273753 - Gamma= 1.9999537 (M=   2) - s=0.0100\n",
      "   2 - L=-207.2018583 - Gamma= 2.9998995 (M=   3) - s=0.0100\n",
      "   3 - L=-153.0920664 - Gamma= 3.9997489 (M=   4) - s=0.0100\n",
      "   4 - L=-124.4064926 - Gamma= 4.9994796 (M=   5) - s=0.0100\n",
      "   5 - L=-102.6330224 - Gamma= 5.9990989 (M=   6) - s=0.0100\n",
      "   6 - L=-76.5721592 - Gamma= 6.9987746 (M=   7) - s=0.0100\n",
      "   7 - L=-55.0552879 - Gamma= 7.9983692 (M=   8) - s=0.0100\n",
      "   8 - L=-41.1174108 - Gamma= 8.9977822 (M=   9) - s=0.0100\n",
      "   9 - L=-31.6027175 - Gamma= 9.9968386 (M=  10) - s=0.0100\n",
      "  10 - L=-24.2756048 - Gamma=10.9952961 (M=  11) - s=0.0100\n",
      "  11 - L=-18.6161156 - Gamma=11.9938301 (M=  12) - s=0.0100\n",
      "  12 - L=-12.4128536 - Gamma=12.9925156 (M=  13) - s=0.0100\n",
      "  13 - L=-9.5576301 - Gamma=13.9898016 (M=  14) - s=0.0100\n",
      "  14 - L=-6.4839210 - Gamma=14.9872293 (M=  15) - s=0.0100\n",
      "  15 - L=-4.1472933 - Gamma=15.9837145 (M=  16) - s=0.0100\n",
      "  16 - L=-1.8832313 - Gamma=16.9798360 (M=  17) - s=0.0100\n",
      "  17 - L=-0.3383307 - Gamma=17.9748783 (M=  18) - s=0.0100\n",
      "  18 - L= 0.5553095 - Gamma=18.9665305 (M=  19) - s=0.0100\n",
      "  19 - L= 1.2439017 - Gamma=19.9560156 (M=  20) - s=0.0100\n",
      "  20 - L= 1.7577725 - Gamma=20.9390606 (M=  21) - s=0.0100\n",
      "  21 - L= 2.1955224 - Gamma=21.9226028 (M=  22) - s=0.0100\n",
      "  22 - L= 2.4462043 - Gamma=22.8937007 (M=  23) - s=0.0100\n",
      "  23 - L= 2.6287341 - Gamma=23.8524349 (M=  24) - s=0.0100\n",
      "  24 - L= 2.7460805 - Gamma=24.7992086 (M=  25) - s=0.0100\n",
      "  25 - L= 2.8343855 - Gamma=25.7309367 (M=  26) - s=0.0100\n",
      "  26 - L= 2.9059439 - Gamma=26.6457972 (M=  27) - s=0.0100\n",
      "  27 - L= 2.9506526 - Gamma=27.5291184 (M=  28) - s=0.0100\n",
      "  28 - L= 2.9694945 - Gamma=28.3333865 (M=  29) - s=0.0100\n",
      "  29 - L= 2.9799630 - Gamma=28.3331175 (M=  29) - s=0.0100\n",
      "  30 - L= 2.9892774 - Gamma=28.3357013 (M=  29) - s=0.0100\n",
      "  31 - L= 2.9945543 - Gamma=28.3364722 (M=  29) - s=0.0100\n",
      "  32 - L= 2.9997795 - Gamma=28.9550135 (M=  30) - s=0.0100\n",
      "  33 - L= 3.0042243 - Gamma=28.9547452 (M=  30) - s=0.0100\n",
      "  34 - L= 3.0074403 - Gamma=29.4895949 (M=  31) - s=0.0100\n",
      "  35 - L= 3.0102158 - Gamma=29.4967839 (M=  31) - s=0.0100\n",
      "  36 - L= 3.0120351 - Gamma=29.4701370 (M=  31) - s=0.0100\n",
      "  37 - L= 3.0136835 - Gamma=29.4789483 (M=  31) - s=0.0100\n",
      "  38 - L= 3.0152062 - Gamma=29.5024225 (M=  31) - s=0.0100\n",
      "  39 - L= 3.0167077 - Gamma=29.5028825 (M=  31) - s=0.0100\n",
      "  40 - L= 3.0181259 - Gamma=29.5033896 (M=  31) - s=0.0100\n",
      "  41 - L= 3.0193410 - Gamma=29.5041153 (M=  31) - s=0.0100\n",
      "  42 - L= 3.0205327 - Gamma=29.5049526 (M=  31) - s=0.0100\n",
      "  43 - L= 3.0215299 - Gamma=29.8710189 (M=  32) - s=0.0100\n",
      "  44 - L= 3.0228492 - Gamma=29.6827070 (M=  32) - s=0.0100\n",
      "  45 - L= 3.0240001 - Gamma=30.0488140 (M=  33) - s=0.0100\n",
      "  46 - L= 3.0247271 - Gamma=30.0755766 (M=  33) - s=0.0100\n",
      "  47 - L= 3.0252856 - Gamma=30.0415477 (M=  33) - s=0.0100\n",
      "  48 - L= 3.0257034 - Gamma=30.0417211 (M=  33) - s=0.0100\n",
      "  49 - L= 3.0260456 - Gamma=30.0396865 (M=  33) - s=0.0100\n",
      "  50 - L= 3.0263822 - Gamma=29.8942279 (M=  33) - s=0.0100\n",
      "  51 - L= 3.0265641 - Gamma=29.8918315 (M=  33) - s=0.0100\n",
      "  52 - L= 3.0266950 - Gamma=29.8684353 (M=  33) - s=0.0100\n",
      "  53 - L= 3.0268206 - Gamma=29.8683712 (M=  33) - s=0.0100\n",
      "  54 - L= 3.0269068 - Gamma=29.8675882 (M=  33) - s=0.0100\n",
      "  55 - L= 3.0269937 - Gamma=29.9491775 (M=  33) - s=0.0100\n",
      "  56 - L= 3.0270398 - Gamma=29.9492923 (M=  33) - s=0.0100\n",
      "  57 - L= 3.0270826 - Gamma=29.9853847 (M=  33) - s=0.0100\n",
      "  58 - L= 3.0271231 - Gamma=29.9853367 (M=  33) - s=0.0100\n",
      "  59 - L= 3.0271512 - Gamma=29.9856516 (M=  33) - s=0.0100\n",
      "  60 - L= 3.0271762 - Gamma=29.9878393 (M=  33) - s=0.0100\n",
      "  61 - L= 3.0271930 - Gamma=29.9500172 (M=  33) - s=0.0100\n",
      "  62 - L= 3.0272159 - Gamma=29.9904026 (M=  33) - s=0.0100\n",
      "  63 - L= 3.0272325 - Gamma=29.9929706 (M=  33) - s=0.0100\n",
      "  64 - L= 3.0272465 - Gamma=30.0177570 (M=  33) - s=0.0100\n",
      "  65 - L= 3.0272588 - Gamma=30.0179884 (M=  33) - s=0.0100\n",
      "  66 - L= 3.0272710 - Gamma=29.9835527 (M=  33) - s=0.0100\n",
      "  67 - L= 3.0272838 - Gamma=29.9805662 (M=  33) - s=0.0100\n",
      "  68 - L= 3.0272927 - Gamma=30.0206262 (M=  34) - s=0.0100\n",
      "  69 - L= 3.0272996 - Gamma=29.9937853 (M=  34) - s=0.0100\n",
      "  70 - L= 3.0273046 - Gamma=29.9937725 (M=  34) - s=0.0100\n",
      "  71 - L= 3.0273091 - Gamma=30.0213771 (M=  34) - s=0.0100\n",
      "  72 - L= 3.0273142 - Gamma=30.0163288 (M=  34) - s=0.0100\n",
      "  73 - L= 3.0273178 - Gamma=29.9963932 (M=  34) - s=0.0100\n",
      "  74 - L= 3.0273255 - Gamma=30.0184450 (M=  34) - s=0.0100\n",
      "  75 - L= 3.0273277 - Gamma=30.0184863 (M=  34) - s=0.0100\n",
      "  76 - L= 3.0273299 - Gamma=30.0262194 (M=  34) - s=0.0100\n",
      "  77 - L= 3.0273328 - Gamma=30.0078292 (M=  34) - s=0.0100\n",
      "  78 - L= 3.0273408 - Gamma=30.0433513 (M=  34) - s=0.0100\n",
      "  79 - L= 3.0273464 - Gamma=30.0172892 (M=  34) - s=0.0100\n",
      "  80 - L= 3.0273502 - Gamma=30.0411776 (M=  34) - s=0.0100\n",
      "  81 - L= 3.0273528 - Gamma=30.0228350 (M=  34) - s=0.0100\n",
      "  82 - L= 3.0273567 - Gamma=30.0379859 (M=  34) - s=0.0100\n",
      "  83 - L= 3.0273599 - Gamma=30.0338567 (M=  34) - s=0.0100\n",
      "  84 - L= 3.0273632 - Gamma=30.0368068 (M=  34) - s=0.0100\n",
      "  85 - L= 3.0273659 - Gamma=30.0566137 (M=  34) - s=0.0100\n",
      "  86 - L= 3.0273724 - Gamma=30.0273261 (M=  34) - s=0.0100\n",
      "  87 - L= 3.0273767 - Gamma=30.0517321 (M=  34) - s=0.0100\n",
      "  88 - L= 3.0273797 - Gamma=30.0310001 (M=  34) - s=0.0100\n",
      "  89 - L= 3.0273833 - Gamma=30.0451256 (M=  34) - s=0.0100\n",
      "  90 - L= 3.0273857 - Gamma=30.0451094 (M=  34) - s=0.0100\n",
      "  91 - L= 3.0273881 - Gamma=30.0450831 (M=  34) - s=0.0100\n",
      "  92 - L= 3.0273901 - Gamma=30.0438294 (M=  34) - s=0.0100\n",
      "  93 - L= 3.0273919 - Gamma=30.0507303 (M=  34) - s=0.0100\n",
      "  94 - L= 3.0273938 - Gamma=30.0507066 (M=  34) - s=0.0100\n",
      "  95 - L= 3.0273955 - Gamma=30.0345289 (M=  34) - s=0.0100\n",
      "  96 - L= 3.0274007 - Gamma=30.0605553 (M=  34) - s=0.0100\n",
      "  97 - L= 3.0274043 - Gamma=30.0368296 (M=  34) - s=0.0100\n",
      "  98 - L= 3.0274074 - Gamma=30.0326705 (M=  34) - s=0.0100\n",
      "  99 - L= 3.0274106 - Gamma=30.0527612 (M=  34) - s=0.0100\n",
      " 100 - L= 3.0274141 - Gamma=30.0557137 (M=  34) - s=0.0100\n",
      " 101 - L= 3.0274169 - Gamma=30.0342537 (M=  34) - s=0.0100\n",
      " 102 - L= 3.0274214 - Gamma=30.0495840 (M=  34) - s=0.0100\n",
      " 103 - L= 3.0274244 - Gamma=30.0487070 (M=  34) - s=0.0100\n",
      " 104 - L= 3.0274267 - Gamma=30.0654285 (M=  34) - s=0.0100\n",
      " 105 - L= 3.0274325 - Gamma=30.0385223 (M=  33) - s=0.0100\n",
      " 106 - L= 3.0274354 - Gamma=30.0569896 (M=  33) - s=0.0100\n",
      " 107 - L= 3.0274376 - Gamma=30.0563428 (M=  33) - s=0.0100\n",
      " 108 - L= 3.0274395 - Gamma=30.0562152 (M=  33) - s=0.0100\n",
      " 109 - L= 3.0274411 - Gamma=30.0531506 (M=  33) - s=0.0100\n",
      " 110 - L= 3.0274423 - Gamma=30.0531730 (M=  33) - s=0.0100\n",
      " 111 - L= 3.0274434 - Gamma=30.0522381 (M=  33) - s=0.0100\n",
      " 112 - L= 3.0274443 - Gamma=30.0521360 (M=  33) - s=0.0100\n",
      " 113 - L= 3.0274451 - Gamma=30.0579998 (M=  33) - s=0.0100\n",
      " 114 - L= 3.0274458 - Gamma=30.0620458 (M=  33) - s=0.0100\n",
      " 115 - L= 3.0274464 - Gamma=30.0632250 (M=  33) - s=0.0100\n",
      " 116 - L= 3.0274468 - Gamma=30.0688076 (M=  33) - s=0.0100\n",
      " 117 - L= 3.0274472 - Gamma=30.0689099 (M=  33) - s=0.0100\n",
      " 118 - L= 3.0274475 - Gamma=30.0689423 (M=  33) - s=0.0100\n",
      " 119 - L= 3.0274478 - Gamma=30.0688765 (M=  33) - s=0.0100\n",
      " 120 - L= 3.0274480 - Gamma=30.0688754 (M=  33) - s=0.0100\n",
      " 121 - L= 3.0274482 - Gamma=30.0688439 (M=  33) - s=0.0100\n",
      " 122 - L= 3.0274485 - Gamma=30.0722897 (M=  33) - s=0.0100\n",
      " 123 - L= 3.0274486 - Gamma=30.0722967 (M=  33) - s=0.0100\n",
      " 124 - L= 3.0274488 - Gamma=30.0721103 (M=  33) - s=0.0100\n",
      " 125 - L= 3.0274489 - Gamma=30.0721075 (M=  33) - s=0.0100\n",
      " 126 - L= 3.0274490 - Gamma=30.0699166 (M=  33) - s=0.0100\n",
      " 127 - L= 3.0274491 - Gamma=30.0698582 (M=  33) - s=0.0100\n",
      " 128 - L= 3.0274492 - Gamma=30.0698564 (M=  33) - s=0.0100\n",
      " 129 - L= 3.0274492 - Gamma=30.0698628 (M=  33) - s=0.0100\n",
      " 130 - L= 3.0274492 - Gamma=30.0698596 (M=  33) - s=0.0100\n",
      " 131 - L= 3.0274493 - Gamma=30.0694137 (M=  33) - s=0.0100\n",
      " 132 - L= 3.0274493 - Gamma=30.0694147 (M=  33) - s=0.0100\n",
      " 133 - L= 3.0274493 - Gamma=30.0694164 (M=  33) - s=0.0100\n",
      " 134 - L= 3.0274494 - Gamma=30.0693218 (M=  33) - s=0.0100\n",
      " 135 - L= 3.0274494 - Gamma=30.0693226 (M=  33) - s=0.0100\n",
      " 136 - L= 3.0274494 - Gamma=30.0694170 (M=  33) - s=0.0100\n",
      " 137 - L= 3.0274494 - Gamma=30.0701368 (M=  33) - s=0.0100\n",
      " 138 - L= 3.0274494 - Gamma=30.0701359 (M=  33) - s=0.0100\n",
      " 139 - L= 3.0274494 - Gamma=30.0701347 (M=  33) - s=0.0100\n",
      " 140 - L= 3.0274494 - Gamma=30.0701342 (M=  33) - s=0.0100\n",
      " 141 - L= 3.0274494 - Gamma=30.0701636 (M=  33) - s=0.0100\n",
      " 142 - L= 3.0274494 - Gamma=30.0702489 (M=  33) - s=0.0100\n",
      " 143 - L= 3.0274494 - Gamma=30.0702489 (M=  33) - s=0.0100\n",
      "Stopping at iteration 143 - max_delta_ml=1.353885562685518e-07\n",
      "L=3.027449419689881 - Gamma=30.070248863532306 (M=33) - s=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial alpha = [[ 0.02885493]]\n",
      "   1 - L=-367.9897813 - Gamma= 1.9999459 (M=   2) - s=0.0100\n",
      "   2 - L=-254.5565052 - Gamma= 2.9998799 (M=   3) - s=0.0100\n",
      "   3 - L=-168.4304293 - Gamma= 3.9997506 (M=   4) - s=0.0100\n",
      "   4 - L=-136.1547580 - Gamma= 4.9994981 (M=   5) - s=0.0100\n",
      "   5 - L=-100.3669511 - Gamma= 5.9992619 (M=   6) - s=0.0100\n",
      "   6 - L=-74.5393437 - Gamma= 6.9988928 (M=   7) - s=0.0100\n",
      "   7 - L=-50.4271872 - Gamma= 7.9985315 (M=   8) - s=0.0100\n",
      "   8 - L=-37.1887910 - Gamma= 8.9979141 (M=   9) - s=0.0100\n",
      "   9 - L=-28.7989179 - Gamma= 9.9968054 (M=  10) - s=0.0100\n",
      "  10 - L=-22.7487156 - Gamma=10.9955143 (M=  11) - s=0.0100\n",
      "  11 - L=-16.2554968 - Gamma=11.9943299 (M=  12) - s=0.0100\n",
      "  12 - L=-10.4757248 - Gamma=12.9929381 (M=  13) - s=0.0100\n",
      "  13 - L=-7.1789044 - Gamma=13.9902729 (M=  14) - s=0.0100\n",
      "  14 - L=-4.9160036 - Gamma=14.9868606 (M=  15) - s=0.0100\n",
      "  15 - L=-3.1019619 - Gamma=15.9826243 (M=  16) - s=0.0100\n",
      "  16 - L=-1.7737541 - Gamma=16.9769078 (M=  17) - s=0.0100\n",
      "  17 - L=-0.5144340 - Gamma=17.9706598 (M=  18) - s=0.0100\n",
      "  18 - L= 0.1969816 - Gamma=18.9598975 (M=  19) - s=0.0100\n",
      "  19 - L= 0.8037237 - Gamma=19.9479611 (M=  20) - s=0.0100\n",
      "  20 - L= 1.4255150 - Gamma=20.9352596 (M=  21) - s=0.0100\n",
      "  21 - L= 1.8518818 - Gamma=21.9149919 (M=  22) - s=0.0100\n",
      "  22 - L= 2.2568377 - Gamma=22.8976077 (M=  23) - s=0.0100\n",
      "  23 - L= 2.4877575 - Gamma=23.8669292 (M=  24) - s=0.0100\n",
      "  24 - L= 2.6930002 - Gamma=24.8309475 (M=  25) - s=0.0100\n",
      "  25 - L= 2.7799731 - Gamma=25.7646488 (M=  26) - s=0.0100\n",
      "  26 - L= 2.8517912 - Gamma=26.6855588 (M=  27) - s=0.0100\n",
      "  27 - L= 2.9181070 - Gamma=27.5985102 (M=  28) - s=0.0100\n",
      "  28 - L= 2.9371518 - Gamma=28.3781312 (M=  29) - s=0.0100\n",
      "  29 - L= 2.9542275 - Gamma=28.3769334 (M=  29) - s=0.0100\n",
      "  30 - L= 2.9667105 - Gamma=29.1253051 (M=  30) - s=0.0100\n",
      "  31 - L= 2.9724886 - Gamma=29.7589566 (M=  31) - s=0.0100\n",
      "  32 - L= 2.9768953 - Gamma=29.7592233 (M=  31) - s=0.0100\n",
      "  33 - L= 2.9811497 - Gamma=29.7684556 (M=  31) - s=0.0100\n",
      "  34 - L= 2.9852671 - Gamma=29.7687603 (M=  31) - s=0.0100\n",
      "  35 - L= 2.9888548 - Gamma=29.7670955 (M=  31) - s=0.0100\n",
      "  36 - L= 2.9923674 - Gamma=29.7485117 (M=  31) - s=0.0100\n",
      "  37 - L= 2.9948389 - Gamma=30.2493609 (M=  32) - s=0.0100\n",
      "  38 - L= 2.9973021 - Gamma=30.2491741 (M=  32) - s=0.0100\n",
      "  39 - L= 2.9997062 - Gamma=30.2594175 (M=  32) - s=0.0100\n",
      "  40 - L= 3.0015379 - Gamma=30.2600305 (M=  32) - s=0.0100\n",
      "  41 - L= 3.0031706 - Gamma=30.2601543 (M=  32) - s=0.0100\n",
      "  42 - L= 3.0046468 - Gamma=30.2603817 (M=  32) - s=0.0100\n",
      "  43 - L= 3.0059563 - Gamma=30.2564039 (M=  32) - s=0.0100\n",
      "  44 - L= 3.0072612 - Gamma=30.2572778 (M=  32) - s=0.0100\n",
      "  45 - L= 3.0084584 - Gamma=30.2629169 (M=  32) - s=0.0100\n",
      "  46 - L= 3.0095807 - Gamma=30.6449190 (M=  33) - s=0.0100\n",
      "  47 - L= 3.0105946 - Gamma=30.6425668 (M=  33) - s=0.0100\n",
      "  48 - L= 3.0114590 - Gamma=30.5121092 (M=  33) - s=0.0100\n",
      "  49 - L= 3.0119006 - Gamma=30.5139805 (M=  33) - s=0.0100\n",
      "  50 - L= 3.0122620 - Gamma=30.7572619 (M=  34) - s=0.0100\n",
      "  51 - L= 3.0125287 - Gamma=30.7777194 (M=  34) - s=0.0100\n",
      "  52 - L= 3.0127323 - Gamma=30.7775633 (M=  34) - s=0.0100\n",
      "  53 - L= 3.0129325 - Gamma=30.7738957 (M=  34) - s=0.0100\n",
      "  54 - L= 3.0130756 - Gamma=30.9377920 (M=  35) - s=0.0100\n",
      "  55 - L= 3.0132163 - Gamma=30.9379547 (M=  35) - s=0.0100\n",
      "  56 - L= 3.0133298 - Gamma=30.9318422 (M=  35) - s=0.0100\n",
      "  57 - L= 3.0134256 - Gamma=30.9368390 (M=  35) - s=0.0100\n",
      "  58 - L= 3.0135038 - Gamma=30.8978527 (M=  35) - s=0.0100\n",
      "  59 - L= 3.0135822 - Gamma=30.8969440 (M=  35) - s=0.0100\n",
      "  60 - L= 3.0136551 - Gamma=31.0116868 (M=  36) - s=0.0100\n",
      "  61 - L= 3.0137153 - Gamma=31.0113929 (M=  36) - s=0.0100\n",
      "  62 - L= 3.0137540 - Gamma=31.0982805 (M=  37) - s=0.0100\n",
      "  63 - L= 3.0137922 - Gamma=31.0606204 (M=  37) - s=0.0100\n",
      "  64 - L= 3.0138275 - Gamma=31.0607439 (M=  37) - s=0.0100\n",
      "  65 - L= 3.0138435 - Gamma=31.1153062 (M=  38) - s=0.0100\n",
      "  66 - L= 3.0138572 - Gamma=31.0974677 (M=  38) - s=0.0100\n",
      "  67 - L= 3.0138668 - Gamma=31.0983599 (M=  38) - s=0.0100\n",
      "  68 - L= 3.0138760 - Gamma=31.0818512 (M=  38) - s=0.0100\n",
      "  69 - L= 3.0138862 - Gamma=31.1285520 (M=  39) - s=0.0100\n",
      "  70 - L= 3.0138978 - Gamma=31.1245555 (M=  39) - s=0.0100\n",
      "  71 - L= 3.0139047 - Gamma=31.0943368 (M=  39) - s=0.0100\n",
      "  72 - L= 3.0139119 - Gamma=31.1297652 (M=  39) - s=0.0100\n",
      "  73 - L= 3.0139254 - Gamma=31.1058297 (M=  39) - s=0.0100\n",
      "  74 - L= 3.0139341 - Gamma=31.0704235 (M=  39) - s=0.0100\n",
      "  75 - L= 3.0139471 - Gamma=31.1206378 (M=  39) - s=0.0100\n",
      "  76 - L= 3.0139557 - Gamma=31.1467423 (M=  39) - s=0.0100\n",
      "  77 - L= 3.0139631 - Gamma=31.1430009 (M=  39) - s=0.0100\n",
      "  78 - L= 3.0139721 - Gamma=31.1056370 (M=  39) - s=0.0100\n",
      "  79 - L= 3.0139894 - Gamma=31.1579033 (M=  39) - s=0.0100\n",
      "  80 - L= 3.0140107 - Gamma=31.1255881 (M=  39) - s=0.0100\n",
      "  81 - L= 3.0140338 - Gamma=31.1866437 (M=  39) - s=0.0100\n",
      "  82 - L= 3.0140638 - Gamma=31.1132484 (M=  39) - s=0.0100\n",
      "  83 - L= 3.0140852 - Gamma=31.0782215 (M=  39) - s=0.0100\n",
      "  84 - L= 3.0141175 - Gamma=31.1445624 (M=  39) - s=0.0100\n",
      "  85 - L= 3.0141428 - Gamma=31.1002594 (M=  38) - s=0.0100\n",
      "  86 - L= 3.0141621 - Gamma=31.1369583 (M=  38) - s=0.0100\n",
      "  87 - L= 3.0141902 - Gamma=31.0647725 (M=  38) - s=0.0100\n",
      "  88 - L= 3.0142041 - Gamma=31.0593529 (M=  38) - s=0.0100\n",
      "  89 - L= 3.0142228 - Gamma=31.1106034 (M=  38) - s=0.0100\n",
      "  90 - L= 3.0142579 - Gamma=31.0613681 (M=  38) - s=0.0100\n",
      "  91 - L= 3.0142775 - Gamma=31.0847909 (M=  38) - s=0.0100\n",
      "  92 - L= 3.0142956 - Gamma=31.1311553 (M=  38) - s=0.0100\n",
      "  93 - L= 3.0143089 - Gamma=31.1332664 (M=  38) - s=0.0100\n",
      "  94 - L= 3.0143220 - Gamma=31.1348122 (M=  38) - s=0.0100\n",
      "  95 - L= 3.0143331 - Gamma=31.1829029 (M=  39) - s=0.0100\n",
      "  96 - L= 3.0143462 - Gamma=31.1347990 (M=  39) - s=0.0100\n",
      "  97 - L= 3.0143574 - Gamma=31.1721461 (M=  39) - s=0.0100\n",
      "  98 - L= 3.0143787 - Gamma=31.1304762 (M=  39) - s=0.0100\n",
      "  99 - L= 3.0143997 - Gamma=31.1661952 (M=  39) - s=0.0100\n",
      " 100 - L= 3.0144132 - Gamma=31.1671734 (M=  39) - s=0.0100\n",
      " 101 - L= 3.0144252 - Gamma=31.1617895 (M=  39) - s=0.0100\n",
      " 102 - L= 3.0144410 - Gamma=31.2033892 (M=  39) - s=0.0100\n",
      " 103 - L= 3.0144527 - Gamma=31.1542853 (M=  39) - s=0.0100\n",
      " 104 - L= 3.0144681 - Gamma=31.0985329 (M=  39) - s=0.0100\n",
      " 105 - L= 3.0144802 - Gamma=31.1242158 (M=  39) - s=0.0100\n",
      " 106 - L= 3.0144822 - Gamma=31.1161650 (M=  38) - s=0.0100\n",
      " 107 - L= 3.0144932 - Gamma=31.1160281 (M=  38) - s=0.0100\n",
      " 108 - L= 3.0145025 - Gamma=31.0868369 (M=  38) - s=0.0100\n",
      " 109 - L= 3.0145141 - Gamma=31.1206553 (M=  38) - s=0.0100\n",
      " 110 - L= 3.0145222 - Gamma=31.1597845 (M=  38) - s=0.0100\n",
      " 111 - L= 3.0145301 - Gamma=31.1172516 (M=  38) - s=0.0100\n",
      " 112 - L= 3.0145394 - Gamma=31.0730923 (M=  38) - s=0.0100\n",
      " 113 - L= 3.0145499 - Gamma=31.1064531 (M=  38) - s=0.0100\n",
      " 114 - L= 3.0145513 - Gamma=31.0978108 (M=  37) - s=0.0100\n",
      " 115 - L= 3.0145622 - Gamma=31.1144691 (M=  37) - s=0.0100\n",
      " 116 - L= 3.0145744 - Gamma=31.0792753 (M=  37) - s=0.0100\n",
      " 117 - L= 3.0145937 - Gamma=31.1379982 (M=  38) - s=0.0100\n",
      " 118 - L= 3.0146025 - Gamma=31.1067153 (M=  38) - s=0.0100\n",
      " 119 - L= 3.0146122 - Gamma=31.1102718 (M=  38) - s=0.0100\n",
      " 120 - L= 3.0146213 - Gamma=31.1053148 (M=  38) - s=0.0100\n",
      " 121 - L= 3.0146301 - Gamma=31.1048103 (M=  38) - s=0.0100\n",
      " 122 - L= 3.0146386 - Gamma=31.1321492 (M=  38) - s=0.0100\n",
      " 123 - L= 3.0146435 - Gamma=31.0984738 (M=  38) - s=0.0100\n",
      " 124 - L= 3.0146479 - Gamma=31.0993301 (M=  38) - s=0.0100\n",
      " 125 - L= 3.0146518 - Gamma=31.0893275 (M=  38) - s=0.0100\n",
      " 126 - L= 3.0146569 - Gamma=31.1180866 (M=  38) - s=0.0100\n",
      " 127 - L= 3.0146689 - Gamma=31.0796703 (M=  38) - s=0.0100\n",
      " 128 - L= 3.0146742 - Gamma=31.0907031 (M=  38) - s=0.0100\n",
      " 129 - L= 3.0146810 - Gamma=31.0936970 (M=  38) - s=0.0100\n",
      " 130 - L= 3.0146882 - Gamma=31.1316391 (M=  39) - s=0.0100\n",
      " 131 - L= 3.0146893 - Gamma=31.1193823 (M=  38) - s=0.0100\n",
      " 132 - L= 3.0146942 - Gamma=31.1190735 (M=  38) - s=0.0100\n",
      " 133 - L= 3.0146978 - Gamma=31.1190621 (M=  38) - s=0.0100\n",
      " 134 - L= 3.0147014 - Gamma=31.1188078 (M=  38) - s=0.0100\n",
      " 135 - L= 3.0147048 - Gamma=31.1193169 (M=  38) - s=0.0100\n",
      " 136 - L= 3.0147082 - Gamma=31.1191129 (M=  38) - s=0.0100\n",
      " 137 - L= 3.0147106 - Gamma=31.1015125 (M=  38) - s=0.0100\n",
      " 138 - L= 3.0147167 - Gamma=31.1319312 (M=  38) - s=0.0100\n",
      " 139 - L= 3.0147217 - Gamma=31.1014865 (M=  38) - s=0.0100\n",
      " 140 - L= 3.0147248 - Gamma=31.0923528 (M=  38) - s=0.0100\n",
      " 141 - L= 3.0147288 - Gamma=31.1164098 (M=  38) - s=0.0100\n",
      " 142 - L= 3.0147349 - Gamma=31.0873257 (M=  38) - s=0.0100\n",
      " 143 - L= 3.0147410 - Gamma=31.1097438 (M=  38) - s=0.0100\n",
      " 144 - L= 3.0147458 - Gamma=31.1198985 (M=  38) - s=0.0100\n",
      " 145 - L= 3.0147486 - Gamma=31.1364371 (M=  38) - s=0.0100\n",
      " 146 - L= 3.0147533 - Gamma=31.1101260 (M=  38) - s=0.0100\n",
      " 147 - L= 3.0147574 - Gamma=31.1340018 (M=  38) - s=0.0100\n",
      " 148 - L= 3.0147619 - Gamma=31.1043058 (M=  38) - s=0.0100\n",
      " 149 - L= 3.0147646 - Gamma=31.1052191 (M=  38) - s=0.0100\n",
      " 150 - L= 3.0147677 - Gamma=31.1291535 (M=  38) - s=0.0100\n",
      " 151 - L= 3.0147712 - Gamma=31.1190132 (M=  38) - s=0.0100\n",
      " 152 - L= 3.0147741 - Gamma=31.0977560 (M=  38) - s=0.0100\n",
      " 153 - L= 3.0147784 - Gamma=31.1212423 (M=  38) - s=0.0100\n",
      " 154 - L= 3.0147824 - Gamma=31.1005501 (M=  37) - s=0.0100\n",
      " 155 - L= 3.0147855 - Gamma=31.1024789 (M=  37) - s=0.0100\n",
      " 156 - L= 3.0147877 - Gamma=31.1024141 (M=  37) - s=0.0100\n",
      " 157 - L= 3.0147898 - Gamma=31.1088498 (M=  37) - s=0.0100\n",
      " 158 - L= 3.0147921 - Gamma=31.0891873 (M=  37) - s=0.0100\n",
      " 159 - L= 3.0147964 - Gamma=31.1121849 (M=  37) - s=0.0100\n",
      " 160 - L= 3.0147991 - Gamma=31.1246450 (M=  37) - s=0.0100\n",
      " 161 - L= 3.0148019 - Gamma=31.1217931 (M=  37) - s=0.0100\n",
      " 162 - L= 3.0148049 - Gamma=31.0989680 (M=  37) - s=0.0100\n",
      " 163 - L= 3.0148078 - Gamma=31.0994035 (M=  37) - s=0.0100\n",
      " 164 - L= 3.0148109 - Gamma=31.1148106 (M=  37) - s=0.0100\n",
      " 165 - L= 3.0148134 - Gamma=31.1152354 (M=  37) - s=0.0100\n",
      " 166 - L= 3.0148151 - Gamma=31.1153618 (M=  37) - s=0.0100\n",
      " 167 - L= 3.0148166 - Gamma=31.1285853 (M=  37) - s=0.0100\n",
      " 168 - L= 3.0148205 - Gamma=31.1018860 (M=  37) - s=0.0100\n",
      " 169 - L= 3.0148223 - Gamma=31.0944575 (M=  37) - s=0.0100\n",
      " 170 - L= 3.0148247 - Gamma=31.1111949 (M=  37) - s=0.0100\n",
      " 171 - L= 3.0148269 - Gamma=31.1128549 (M=  37) - s=0.0100\n",
      " 172 - L= 3.0148286 - Gamma=31.1126313 (M=  37) - s=0.0100\n",
      " 173 - L= 3.0148300 - Gamma=31.1125825 (M=  37) - s=0.0100\n",
      " 174 - L= 3.0148314 - Gamma=31.1124132 (M=  37) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 175 - L= 3.0148328 - Gamma=31.0958329 (M=  37) - s=0.0100\n",
      " 176 - L= 3.0148343 - Gamma=31.0958092 (M=  37) - s=0.0100\n",
      " 177 - L= 3.0148354 - Gamma=31.1062970 (M=  37) - s=0.0100\n",
      " 178 - L= 3.0148366 - Gamma=31.1110995 (M=  37) - s=0.0100\n",
      " 179 - L= 3.0148376 - Gamma=31.1110789 (M=  37) - s=0.0100\n",
      " 180 - L= 3.0148384 - Gamma=31.1231331 (M=  37) - s=0.0100\n",
      " 181 - L= 3.0148393 - Gamma=31.1098280 (M=  37) - s=0.0100\n",
      " 182 - L= 3.0148411 - Gamma=31.1216269 (M=  37) - s=0.0100\n",
      " 183 - L= 3.0148413 - Gamma=31.1191317 (M=  36) - s=0.0100\n",
      " 184 - L= 3.0148426 - Gamma=31.1171210 (M=  36) - s=0.0100\n",
      " 185 - L= 3.0148439 - Gamma=31.1109477 (M=  36) - s=0.0100\n",
      " 186 - L= 3.0148446 - Gamma=31.1198484 (M=  36) - s=0.0100\n",
      " 187 - L= 3.0148452 - Gamma=31.1197560 (M=  36) - s=0.0100\n",
      " 188 - L= 3.0148459 - Gamma=31.1196896 (M=  36) - s=0.0100\n",
      " 189 - L= 3.0148465 - Gamma=31.1196988 (M=  36) - s=0.0100\n",
      " 190 - L= 3.0148471 - Gamma=31.1200977 (M=  36) - s=0.0100\n",
      " 191 - L= 3.0148476 - Gamma=31.1208723 (M=  36) - s=0.0100\n",
      " 192 - L= 3.0148481 - Gamma=31.1158768 (M=  36) - s=0.0100\n",
      " 193 - L= 3.0148485 - Gamma=31.1161412 (M=  36) - s=0.0100\n",
      " 194 - L= 3.0148489 - Gamma=31.1161122 (M=  36) - s=0.0100\n",
      " 195 - L= 3.0148492 - Gamma=31.1237992 (M=  36) - s=0.0100\n",
      " 196 - L= 3.0148497 - Gamma=31.1266219 (M=  36) - s=0.0100\n",
      " 197 - L= 3.0148499 - Gamma=31.1267619 (M=  36) - s=0.0100\n",
      " 198 - L= 3.0148502 - Gamma=31.1273277 (M=  36) - s=0.0100\n",
      " 199 - L= 3.0148504 - Gamma=31.1228745 (M=  36) - s=0.0100\n",
      " 200 - L= 3.0148506 - Gamma=31.1228661 (M=  36) - s=0.0100\n",
      "MODEL: RVM accuracy:  0.734939759036 +/-: 0.00782500424566\n",
      "Initial alpha = [[ 0.02358065]]\n",
      "   1 - L=-417.8044950 - Gamma= 1.9999581 (M=   2) - s=0.0100\n",
      "   2 - L=-247.8326549 - Gamma= 2.9999154 (M=   3) - s=0.0100\n",
      "   3 - L=-207.9998705 - Gamma= 3.9997591 (M=   4) - s=0.0100\n",
      "   4 - L=-172.6273341 - Gamma= 4.9995612 (M=   5) - s=0.0100\n",
      "   5 - L=-137.1635220 - Gamma= 5.9993804 (M=   6) - s=0.0100\n",
      "   6 - L=-111.6903459 - Gamma= 6.9991179 (M=   7) - s=0.0100\n",
      "   7 - L=-91.4036104 - Gamma= 7.9988007 (M=   8) - s=0.0100\n",
      "   8 - L=-72.1083001 - Gamma= 8.9984804 (M=   9) - s=0.0100\n",
      "   9 - L=-56.4100458 - Gamma= 9.9980328 (M=  10) - s=0.0100\n",
      "  10 - L=-43.7536692 - Gamma=10.9974904 (M=  11) - s=0.0100\n",
      "  11 - L=-34.8565873 - Gamma=11.9967415 (M=  12) - s=0.0100\n",
      "  12 - L=-28.7495713 - Gamma=12.9955799 (M=  13) - s=0.0100\n",
      "  13 - L=-22.7015313 - Gamma=13.9944863 (M=  14) - s=0.0100\n",
      "  14 - L=-18.3125930 - Gamma=14.9930570 (M=  15) - s=0.0100\n",
      "  15 - L=-13.9312777 - Gamma=15.9915631 (M=  16) - s=0.0100\n",
      "  16 - L=-11.3605541 - Gamma=16.9891476 (M=  17) - s=0.0100\n",
      "  17 - L=-8.8955713 - Gamma=17.9865585 (M=  18) - s=0.0100\n",
      "  18 - L=-6.7775206 - Gamma=18.9835038 (M=  19) - s=0.0100\n",
      "  19 - L=-4.4708707 - Gamma=19.9805790 (M=  20) - s=0.0100\n",
      "  20 - L=-2.8547028 - Gamma=20.9768175 (M=  21) - s=0.0100\n",
      "  21 - L=-1.5486732 - Gamma=21.9718312 (M=  22) - s=0.0100\n",
      "  22 - L=-0.5967144 - Gamma=22.9653690 (M=  23) - s=0.0100\n",
      "  23 - L= 0.3384057 - Gamma=23.9588068 (M=  24) - s=0.0100\n",
      "  24 - L= 0.9389708 - Gamma=24.9485240 (M=  25) - s=0.0100\n",
      "  25 - L= 1.4716965 - Gamma=25.9372448 (M=  26) - s=0.0100\n",
      "  26 - L= 1.8721882 - Gamma=26.9223016 (M=  27) - s=0.0100\n",
      "  27 - L= 2.2494144 - Gamma=27.9061914 (M=  28) - s=0.0100\n",
      "  28 - L= 2.4384107 - Gamma=28.8769934 (M=  29) - s=0.0100\n",
      "  29 - L= 2.5795406 - Gamma=29.8380678 (M=  30) - s=0.0100\n",
      "  30 - L= 2.6743860 - Gamma=30.7844563 (M=  31) - s=0.0100\n",
      "  31 - L= 2.7452092 - Gamma=31.7179649 (M=  32) - s=0.0100\n",
      "  32 - L= 2.7964335 - Gamma=32.6250977 (M=  33) - s=0.0100\n",
      "  33 - L= 2.8379276 - Gamma=33.5185157 (M=  34) - s=0.0100\n",
      "  34 - L= 2.8725628 - Gamma=34.4005875 (M=  35) - s=0.0100\n",
      "  35 - L= 2.9066801 - Gamma=35.2675310 (M=  36) - s=0.0100\n",
      "  36 - L= 2.9207917 - Gamma=36.0520328 (M=  37) - s=0.0100\n",
      "  37 - L= 2.9302678 - Gamma=36.7903698 (M=  38) - s=0.0100\n",
      "  38 - L= 2.9387264 - Gamma=37.5123026 (M=  39) - s=0.0100\n",
      "  39 - L= 2.9442966 - Gamma=37.5115798 (M=  39) - s=0.0100\n",
      "  40 - L= 2.9487675 - Gamma=37.5121202 (M=  39) - s=0.0100\n",
      "  41 - L= 2.9522720 - Gamma=37.5113044 (M=  39) - s=0.0100\n",
      "  42 - L= 2.9555724 - Gamma=38.0937097 (M=  40) - s=0.0100\n",
      "  43 - L= 2.9588375 - Gamma=38.6604217 (M=  41) - s=0.0100\n",
      "  44 - L= 2.9612332 - Gamma=38.7222739 (M=  41) - s=0.0100\n",
      "  45 - L= 2.9631652 - Gamma=38.7222176 (M=  41) - s=0.0100\n",
      "  46 - L= 2.9650141 - Gamma=38.7223546 (M=  41) - s=0.0100\n",
      "  47 - L= 2.9668614 - Gamma=38.7231245 (M=  41) - s=0.0100\n",
      "  48 - L= 2.9685651 - Gamma=38.7239403 (M=  41) - s=0.0100\n",
      "  49 - L= 2.9702596 - Gamma=38.7232501 (M=  41) - s=0.0100\n",
      "  50 - L= 2.9714090 - Gamma=39.1352621 (M=  42) - s=0.0100\n",
      "  51 - L= 2.9724593 - Gamma=39.1353919 (M=  42) - s=0.0100\n",
      "  52 - L= 2.9732563 - Gamma=39.1369155 (M=  42) - s=0.0100\n",
      "  53 - L= 2.9739892 - Gamma=39.0541671 (M=  42) - s=0.0100\n",
      "  54 - L= 2.9745809 - Gamma=39.0611932 (M=  42) - s=0.0100\n",
      "  55 - L= 2.9751287 - Gamma=39.0651341 (M=  42) - s=0.0100\n",
      "  56 - L= 2.9756560 - Gamma=39.0655972 (M=  42) - s=0.0100\n",
      "  57 - L= 2.9760868 - Gamma=39.0644858 (M=  42) - s=0.0100\n",
      "  58 - L= 2.9764052 - Gamma=39.0727787 (M=  42) - s=0.0100\n",
      "  59 - L= 2.9767083 - Gamma=39.0660891 (M=  42) - s=0.0100\n",
      "  60 - L= 2.9770024 - Gamma=39.0659731 (M=  42) - s=0.0100\n",
      "  61 - L= 2.9772422 - Gamma=38.9780584 (M=  42) - s=0.0100\n",
      "  62 - L= 2.9774747 - Gamma=38.9791558 (M=  42) - s=0.0100\n",
      "  63 - L= 2.9776945 - Gamma=39.0051963 (M=  42) - s=0.0100\n",
      "  64 - L= 2.9778764 - Gamma=39.2047068 (M=  43) - s=0.0100\n",
      "  65 - L= 2.9780209 - Gamma=39.2048169 (M=  43) - s=0.0100\n",
      "  66 - L= 2.9781378 - Gamma=39.2049092 (M=  43) - s=0.0100\n",
      "  67 - L= 2.9782460 - Gamma=39.1930723 (M=  43) - s=0.0100\n",
      "  68 - L= 2.9783455 - Gamma=39.3420734 (M=  44) - s=0.0100\n",
      "  69 - L= 2.9784406 - Gamma=39.3426344 (M=  44) - s=0.0100\n",
      "  70 - L= 2.9785298 - Gamma=39.3435796 (M=  44) - s=0.0100\n",
      "  71 - L= 2.9786163 - Gamma=39.3431972 (M=  44) - s=0.0100\n",
      "  72 - L= 2.9786976 - Gamma=39.3438270 (M=  44) - s=0.0100\n",
      "  73 - L= 2.9787706 - Gamma=39.3311145 (M=  44) - s=0.0100\n",
      "  74 - L= 2.9788530 - Gamma=39.4704736 (M=  45) - s=0.0100\n",
      "  75 - L= 2.9789254 - Gamma=39.4705732 (M=  45) - s=0.0100\n",
      "  76 - L= 2.9789826 - Gamma=39.3584232 (M=  45) - s=0.0100\n",
      "  77 - L= 2.9790279 - Gamma=39.3568265 (M=  45) - s=0.0100\n",
      "  78 - L= 2.9790712 - Gamma=39.3509301 (M=  45) - s=0.0100\n",
      "  79 - L= 2.9791220 - Gamma=39.4428754 (M=  45) - s=0.0100\n",
      "  80 - L= 2.9791632 - Gamma=39.4436709 (M=  45) - s=0.0100\n",
      "  81 - L= 2.9792004 - Gamma=39.4116334 (M=  45) - s=0.0100\n",
      "  82 - L= 2.9792440 - Gamma=39.5006976 (M=  45) - s=0.0100\n",
      "  83 - L= 2.9792729 - Gamma=39.4917126 (M=  45) - s=0.0100\n",
      "  84 - L= 2.9793003 - Gamma=39.4119198 (M=  44) - s=0.0100\n",
      "  85 - L= 2.9793241 - Gamma=39.4289136 (M=  44) - s=0.0100\n",
      "  86 - L= 2.9793441 - Gamma=39.3993893 (M=  44) - s=0.0100\n",
      "  87 - L= 2.9793572 - Gamma=39.4446282 (M=  44) - s=0.0100\n",
      "  88 - L= 2.9793744 - Gamma=39.4934597 (M=  44) - s=0.0100\n",
      "  89 - L= 2.9793923 - Gamma=39.4864412 (M=  44) - s=0.0100\n",
      "  90 - L= 2.9794108 - Gamma=39.4809023 (M=  44) - s=0.0100\n",
      "  91 - L= 2.9794224 - Gamma=39.4775632 (M=  44) - s=0.0100\n",
      "  92 - L= 2.9794310 - Gamma=39.4773695 (M=  44) - s=0.0100\n",
      "  93 - L= 2.9794368 - Gamma=39.4596065 (M=  44) - s=0.0100\n",
      "  94 - L= 2.9794431 - Gamma=39.4893882 (M=  44) - s=0.0100\n",
      "  95 - L= 2.9794475 - Gamma=39.4856562 (M=  44) - s=0.0100\n",
      "  96 - L= 2.9794517 - Gamma=39.5084261 (M=  44) - s=0.0100\n",
      "  97 - L= 2.9794548 - Gamma=39.5081989 (M=  44) - s=0.0100\n",
      "  98 - L= 2.9794572 - Gamma=39.4967567 (M=  44) - s=0.0100\n",
      "  99 - L= 2.9794594 - Gamma=39.4981510 (M=  44) - s=0.0100\n",
      " 100 - L= 2.9794611 - Gamma=39.4983418 (M=  44) - s=0.0100\n",
      " 101 - L= 2.9794629 - Gamma=39.4892003 (M=  44) - s=0.0100\n",
      " 102 - L= 2.9794655 - Gamma=39.5047626 (M=  44) - s=0.0100\n",
      " 103 - L= 2.9794669 - Gamma=39.5066682 (M=  44) - s=0.0100\n",
      " 104 - L= 2.9794681 - Gamma=39.5063924 (M=  44) - s=0.0100\n",
      " 105 - L= 2.9794693 - Gamma=39.5064147 (M=  44) - s=0.0100\n",
      " 106 - L= 2.9794702 - Gamma=39.5175265 (M=  44) - s=0.0100\n",
      " 107 - L= 2.9794711 - Gamma=39.5177487 (M=  44) - s=0.0100\n",
      " 108 - L= 2.9794718 - Gamma=39.5168846 (M=  44) - s=0.0100\n",
      " 109 - L= 2.9794725 - Gamma=39.5259807 (M=  44) - s=0.0100\n",
      " 110 - L= 2.9794731 - Gamma=39.5248920 (M=  44) - s=0.0100\n",
      " 111 - L= 2.9794738 - Gamma=39.5248987 (M=  44) - s=0.0100\n",
      " 112 - L= 2.9794743 - Gamma=39.5222909 (M=  44) - s=0.0100\n",
      " 113 - L= 2.9794747 - Gamma=39.5222795 (M=  44) - s=0.0100\n",
      " 114 - L= 2.9794752 - Gamma=39.5219896 (M=  44) - s=0.0100\n",
      " 115 - L= 2.9794756 - Gamma=39.5172104 (M=  44) - s=0.0100\n",
      " 116 - L= 2.9794760 - Gamma=39.5127023 (M=  44) - s=0.0100\n",
      " 117 - L= 2.9794764 - Gamma=39.5196081 (M=  44) - s=0.0100\n",
      " 118 - L= 2.9794767 - Gamma=39.5186063 (M=  44) - s=0.0100\n",
      " 119 - L= 2.9794771 - Gamma=39.5248507 (M=  44) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120 - L= 2.9794774 - Gamma=39.5237678 (M=  44) - s=0.0100\n",
      " 121 - L= 2.9794777 - Gamma=39.5240069 (M=  44) - s=0.0100\n",
      " 122 - L= 2.9794780 - Gamma=39.5239988 (M=  44) - s=0.0100\n",
      " 123 - L= 2.9794782 - Gamma=39.5239777 (M=  44) - s=0.0100\n",
      " 124 - L= 2.9794784 - Gamma=39.5240072 (M=  44) - s=0.0100\n",
      " 125 - L= 2.9794786 - Gamma=39.5240096 (M=  44) - s=0.0100\n",
      " 126 - L= 2.9794788 - Gamma=39.5240025 (M=  44) - s=0.0100\n",
      " 127 - L= 2.9794789 - Gamma=39.5212484 (M=  44) - s=0.0100\n",
      " 128 - L= 2.9794790 - Gamma=39.5187293 (M=  44) - s=0.0100\n",
      " 129 - L= 2.9794791 - Gamma=39.5222321 (M=  44) - s=0.0100\n",
      " 130 - L= 2.9794792 - Gamma=39.5233106 (M=  44) - s=0.0100\n",
      " 131 - L= 2.9794793 - Gamma=39.5264296 (M=  44) - s=0.0100\n",
      " 132 - L= 2.9794795 - Gamma=39.5260806 (M=  44) - s=0.0100\n",
      " 133 - L= 2.9794795 - Gamma=39.5260970 (M=  44) - s=0.0100\n",
      " 134 - L= 2.9794796 - Gamma=39.5244300 (M=  44) - s=0.0100\n",
      " 135 - L= 2.9794796 - Gamma=39.5244541 (M=  44) - s=0.0100\n",
      " 136 - L= 2.9794797 - Gamma=39.5241878 (M=  44) - s=0.0100\n",
      " 137 - L= 2.9794797 - Gamma=39.5264397 (M=  44) - s=0.0100\n",
      " 138 - L= 2.9794797 - Gamma=39.5286576 (M=  44) - s=0.0100\n",
      " 139 - L= 2.9794798 - Gamma=39.5288257 (M=  44) - s=0.0100\n",
      " 140 - L= 2.9794798 - Gamma=39.5288415 (M=  44) - s=0.0100\n",
      " 141 - L= 2.9794798 - Gamma=39.5275949 (M=  44) - s=0.0100\n",
      " 142 - L= 2.9794798 - Gamma=39.5276014 (M=  44) - s=0.0100\n",
      " 143 - L= 2.9794799 - Gamma=39.5266610 (M=  44) - s=0.0100\n",
      " 144 - L= 2.9794799 - Gamma=39.5266318 (M=  44) - s=0.0100\n",
      " 145 - L= 2.9794799 - Gamma=39.5266464 (M=  44) - s=0.0100\n",
      " 146 - L= 2.9794799 - Gamma=39.5266523 (M=  44) - s=0.0100\n",
      " 147 - L= 2.9794799 - Gamma=39.5266534 (M=  44) - s=0.0100\n",
      " 148 - L= 2.9794799 - Gamma=39.5261374 (M=  44) - s=0.0100\n",
      " 149 - L= 2.9794799 - Gamma=39.5261505 (M=  44) - s=0.0100\n",
      " 150 - L= 2.9794799 - Gamma=39.5261513 (M=  44) - s=0.0100\n",
      " 151 - L= 2.9794799 - Gamma=39.5271112 (M=  44) - s=0.0100\n",
      " 152 - L= 2.9794799 - Gamma=39.5280801 (M=  44) - s=0.0100\n",
      " 153 - L= 2.9794800 - Gamma=39.5279144 (M=  44) - s=0.0100\n",
      " 154 - L= 2.9794800 - Gamma=39.5277544 (M=  44) - s=0.0100\n",
      " 155 - L= 2.9794800 - Gamma=39.5276638 (M=  44) - s=0.0100\n",
      " 156 - L= 2.9794800 - Gamma=39.5275602 (M=  44) - s=0.0100\n",
      " 157 - L= 2.9794800 - Gamma=39.5275613 (M=  44) - s=0.0100\n",
      " 158 - L= 2.9794800 - Gamma=39.5275611 (M=  44) - s=0.0100\n",
      " 159 - L= 2.9794800 - Gamma=39.5273152 (M=  44) - s=0.0100\n",
      " 160 - L= 2.9794800 - Gamma=39.5273195 (M=  44) - s=0.0100\n",
      " 161 - L= 2.9794800 - Gamma=39.5273158 (M=  44) - s=0.0100\n",
      " 162 - L= 2.9794800 - Gamma=39.5279675 (M=  44) - s=0.0100\n",
      " 163 - L= 2.9794800 - Gamma=39.5274761 (M=  44) - s=0.0100\n",
      " 164 - L= 2.9794800 - Gamma=39.5269036 (M=  44) - s=0.0100\n",
      " 165 - L= 2.9794800 - Gamma=39.5276458 (M=  44) - s=0.0100\n",
      " 166 - L= 2.9794800 - Gamma=39.5283407 (M=  44) - s=0.0100\n",
      " 167 - L= 2.9794800 - Gamma=39.5283162 (M=  44) - s=0.0100\n",
      " 168 - L= 2.9794800 - Gamma=39.5283296 (M=  44) - s=0.0100\n",
      " 169 - L= 2.9794800 - Gamma=39.5283296 (M=  44) - s=0.0100\n",
      "Stopping at iteration 169 - max_delta_ml=2.419402013886405e-07\n",
      "L=2.979480033163092 - Gamma=39.52832956418557 (M=44) - s=0.01\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Train on 65 samples, validate on 18 samples\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 1s - loss: 0.6910 - acc: 0.6308 - val_loss: 0.6457 - val_acc: 0.6111\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 0s - loss: 0.6099 - acc: 0.6308 - val_loss: 0.6002 - val_acc: 0.6111\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 0s - loss: 0.5610 - acc: 0.6308 - val_loss: 0.5323 - val_acc: 0.6111\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 0s - loss: 0.4991 - acc: 0.6308 - val_loss: 0.4879 - val_acc: 0.6111\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 0s - loss: 0.4444 - acc: 0.7538 - val_loss: 0.4398 - val_acc: 0.8889\n",
      "Train on 66 samples, validate on 17 samples\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 0s - loss: 0.4237 - acc: 0.9545 - val_loss: 0.3648 - val_acc: 0.9412\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 0s - loss: 0.3872 - acc: 0.9394 - val_loss: 0.3455 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 0s - loss: 0.3409 - acc: 0.9848 - val_loss: 0.3553 - val_acc: 0.9412\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 0s - loss: 0.3400 - acc: 0.9697 - val_loss: 0.4363 - val_acc: 0.9412\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 0s - loss: 0.3411 - acc: 0.9394 - val_loss: 0.3390 - val_acc: 0.9412\n",
      "Train on 67 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "67/67 [==============================] - 0s - loss: 0.2927 - acc: 0.9851 - val_loss: 0.2718 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "67/67 [==============================] - 0s - loss: 0.2634 - acc: 1.0000 - val_loss: 0.2395 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "67/67 [==============================] - 0s - loss: 0.2317 - acc: 1.0000 - val_loss: 0.2215 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "67/67 [==============================] - 0s - loss: 0.2146 - acc: 1.0000 - val_loss: 0.2068 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "67/67 [==============================] - 0s - loss: 0.2028 - acc: 1.0000 - val_loss: 0.1995 - val_acc: 1.0000\n",
      "Train on 67 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "67/67 [==============================] - 0s - loss: 0.1964 - acc: 1.0000 - val_loss: 0.1873 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "67/67 [==============================] - 0s - loss: 0.1872 - acc: 1.0000 - val_loss: 0.1819 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "67/67 [==============================] - 0s - loss: 0.1813 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "67/67 [==============================] - 0s - loss: 0.1750 - acc: 1.0000 - val_loss: 0.1717 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "67/67 [==============================] - 0s - loss: 0.1700 - acc: 1.0000 - val_loss: 0.1673 - val_acc: 1.0000\n",
      "Train on 67 samples, validate on 16 samples\n",
      "Epoch 1/5\n",
      "67/67 [==============================] - 0s - loss: 0.1658 - acc: 1.0000 - val_loss: 0.1635 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "67/67 [==============================] - 0s - loss: 0.1619 - acc: 1.0000 - val_loss: 0.1600 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "67/67 [==============================] - 0s - loss: 0.1585 - acc: 1.0000 - val_loss: 0.1566 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "67/67 [==============================] - 0s - loss: 0.1552 - acc: 1.0000 - val_loss: 0.1535 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "67/67 [==============================] - 0s - loss: 0.1521 - acc: 1.0000 - val_loss: 0.1505 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  0.963855421687 +/-: 0.00207966285174\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "METHOD_LIST = ['ExtraTrees', 'CART', 'RandomForest', 'GBM', 'AdaBoost', 'LR', 'SVM', 'NaiveBayes', 'MLNN'] # XGB\n",
    "Runs = []\n",
    "nruns = 1\n",
    "SCALER = \"minmax\"\n",
    "GROUPING = \"mean\"\n",
    "DIM_TYPE = None # \"LDA\" # \"LDA\" # \"LDA\"\n",
    "DIM_NUM = 1000\n",
    "Results = None\n",
    "ACC = pd.DataFrame()\n",
    "Rocket.VIZ = False\n",
    "for i in range(0, nruns):\n",
    "    Rocket.SEED = np.random.randint(0,10000)\n",
    "    MODELS  = []\n",
    "    for idx, METHOD in enumerate(METHOD_LIST):\n",
    "        preds, class_model, accuracy = Rocket.classify_treatment(model_type = METHOD, \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "        MODELS.append({'method': METHOD, 'model': class_model})\n",
    "        ACC = ACC.append(accuracy, ignore_index= True)\n",
    "        preds = [pred_[1]for pred_ in preds]\n",
    "        #len(Rocket.DATA_merged[Rocket.DATA_merged[\"array-batch\"].isin([\"cohort 1\", \"cohort 2\", \"JB\", \"IA\", \"ALL-10\"])])\n",
    "        if Results is None:\n",
    "            Results = Rocket.DATA_merged_processed.copy()\n",
    "        Results['pred'] = preds\n",
    "        Results['method'] = METHOD\n",
    "        if idx == 0:\n",
    "            AllResults = Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]]\n",
    "        else:\n",
    "            AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], \n",
    "                                                    'pred', \n",
    "                                                    'method', \n",
    "                                                    Rocket.MODEL_PARAMETERS['target']]], \n",
    "                                      ignore_index = True)\n",
    "\n",
    "    ####\n",
    "    ####\n",
    "\n",
    "    preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"RVM\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "    MODELS.append({'method': METHOD, 'model': class_model})\n",
    "    ACC = ACC.append(accuracy, ignore_index = True)\n",
    "    Results = Rocket.DATA_merged_processed.copy()\n",
    "    preds = [pred_ for pred_ in preds]\n",
    "    Results['pred'] = preds\n",
    "    Results['method'] = \"RVM\"\n",
    "    AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]], ignore_index = True)\n",
    "\n",
    "    ####\n",
    "    ####\n",
    "\n",
    "    preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"DNN\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "    MODELS.append({'method': METHOD, 'model': class_model})\n",
    "    ACC = ACC.append(accuracy, ignore_index = True)\n",
    "    Results = Rocket.DATA_merged_processed.copy()\n",
    "    preds = [pred_ for pred_ in preds]\n",
    "    Results['pred'] = preds\n",
    "    Results['method'] = \"DNN\"\n",
    "    AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]],\n",
    "                                   ignore_index = True)\n",
    "\n",
    "    AllResults[Rocket.MODEL_PARAMETERS['ID']] = AllResults[Rocket.MODEL_PARAMETERS['ID']].astype('str')\n",
    "    AllResults = AllResults.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "    #AllResults[AllResults['Treatment_risk_group_in_ALL10'].notnull()]\n",
    "    ####\n",
    "    ####\n",
    "    Runs.append(AllResults)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on average: 0.8904709748083243 +- 0.005878252826179187, median: 0.9036144578313253+-0.004178419323407495\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "         acc model       var\n",
      "0   0.891566    ET  0.004178\n",
      "1   0.831325  CART  0.022921\n",
      "2   0.903614    RF  0.004973\n",
      "3   0.891566   GBM  0.004828\n",
      "4   0.927711   ADA  0.002260\n",
      "5   0.927711    LR  0.003766\n",
      "6   0.927711   SVM  0.003766\n",
      "7   0.903614   GNB  0.002630\n",
      "8   0.891566  MLNN  0.005433\n",
      "9   0.734940   RVM  0.007825\n",
      "10  0.963855   DNN  0.002080\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on average: {} +- {}, median: {}+-{}\".format(ACC.mean()[0], ACC.mean()[1], ACC.median()[0], ACC.median()[1]))\n",
    "print(\"+\"*40)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "###########\n",
    "##Runs.append(AllResults)\n",
    "final_df = pandas.DataFrame()\n",
    "for idx, df in enumerate(Runs):\n",
    "    df['run'] = idx\n",
    "    final_df = final_df.append(df, ignore_index = True)\n",
    "final_df = final_df.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "final_df['pred']= pandas.to_numeric(final_df['pred'])\n",
    "final_df.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/patient_results_\"+Rocket.SET_NAME+\".csv\")\n",
    "final_df = final_df.groupby([Rocket.MODEL_PARAMETERS['ID'], 'method']).agg({'pred': [numpy.mean, numpy.median, numpy.std]})\n",
    "final_df.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/patient_results_agg_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through models..\n",
    "### Tree weights\n",
    "top_genomes_weights = pandas.DataFrame()\n",
    "#top_genomes.index = Rocket.DATA_merged_processed\n",
    "for mod in MODELS:\n",
    "    if(mod['method'] in ['RandomForest', 'GBM', 'AdaBoost', 'ExtraTrees']): # RF, ET, GBM, ADA\n",
    "        top_genomes_weights[mod['method']]=mod['model'].feature_importances_\n",
    "        # column normalise\n",
    "        top_genomes_weights[mod['method']] = top_genomes_weights[mod['method']]/top_genomes_weights[mod['method']].max()\n",
    "        \n",
    "top_genomes_weights.index = Rocket.DATA_merged_processed.drop(['target', 'ID'], axis=1).columns\n",
    "#top_genomes['ALL'] = top_genomes.sum(axis=1)\n",
    "top_genomes_weights['MED'] = top_genomes_weights.median(axis=1)\n",
    "top_genomes_weights = top_genomes_weights.sort_values(by='MED', ascending=False)\n",
    "       \n",
    "### Coefficients\n",
    "top_genomes_coeffs = pandas.DataFrame()\n",
    "for mod in MODELS:\n",
    "    if(mod['method'] in ['LR', 'SVM']):\n",
    "        top_genomes_coeffs[mod['method']] = mod['model'].coef_[0,:]\n",
    "        top_genomes_coeffs[mod['method']] = top_genomes_coeffs[mod['method']]/top_genomes_coeffs[mod['method']].max() #\\\n",
    "                                                               #  -top_genomes[mod['method']].min())\n",
    "                                                                 #+numpy.abs(top_genomes[mod['method']].min())\n",
    "top_genomes_coeffs.index = Rocket.DATA_merged_processed.drop(['target', 'ID'], axis=1).columns\n",
    "#top_genomes['ALL'] = top_genomes.sum(axis=1)\n",
    "top_genomes_coeffs['MEAN'] = top_genomes_coeffs.mean(axis=1)\n",
    "top_genomes_coeffs = top_genomes_coeffs.sort_values(by='MEAN', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genomes_coeffs.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/coeffs_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genomes_weights.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/weights_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF9 = top_genomes_weights['RandomForest'].quantile(q=0.9)\n",
    "GBM9 = top_genomes_weights['GBM'].quantile(q=0.9)\n",
    "ADA9 = top_genomes_weights['AdaBoost'].quantile(q=0.9)\n",
    "ET9 = top_genomes_weights['ExtraTrees'].quantile(q=0.9)\n",
    "Overlapping_genomes = set(top_genomes_weights.loc[top_genomes_weights['RandomForest']>RF9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['GBM']>GBM9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['AdaBoost']>ADA9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['ExtraTrees']>ET9].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "#from scipy.dspatial.distance import cosine\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cdist\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TransPosed = Rocket.DATA_all_samples.T # all microarrays, may be multiple per patient versus all probesets, may be multiple per genome\n",
    "Normal = Rocket.DATA_merged_processed.loc[:, (Rocket.DATA_merged_processed.columns !='target') & \n",
    "                                             (Rocket.DATA_merged_processed.columns !='ID')]\n",
    "#AllNormal = Rocket.DATA_merged\n",
    "#probeset_weights = Rocket.get_probeset_weights(method = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 9827_corr2.CEL, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'cosine', normalised = True, inflation = 2, minkowski_dim=1)\n",
    "##### apply Markov clustering\n",
    "#######################\n",
    "# non-distributed, non-sparse version, only for small-sized problems (N is order 1000)\n",
    "e = 2\n",
    "r = 2 \n",
    "epsilon = 1e-7\n",
    "convergence = 0.001\n",
    "num_iter = 10\n",
    "Orientation = 'col' # columnwise or rowwise\n",
    "\n",
    "# add loop\n",
    "def add_loop(df_matrix, value=0): \n",
    "    for i in df_matrix.index:\n",
    "        df_matrix.loc[i, i] = value\n",
    "    return df_matrix\n",
    "patient_sim = add_loop(patient_sim, 1)\n",
    "patient_sim = patient_sim - epsilon\n",
    "\n",
    "def normalise(sim, type = 'col'):\n",
    "    if(type == 'col'):\n",
    "        # column normalisation\n",
    "        for variable in sim.keys():\n",
    "            col_vec = sim[variable]\n",
    "            sum_val = sum([p for p in col_vec])\n",
    "            sim[variable] = sim[variable]/sum_val\n",
    "    elif (type == 'row'):\n",
    "        # row normalisation\n",
    "        for variable in sim.keys():\n",
    "            row_vec = sim.loc[variable, :]\n",
    "            sum_val = sum([p for p in row_vec])\n",
    "            sim.loc[variable,:] = sim.loc[variable,:]/sum_val\n",
    "    return sim\n",
    "\n",
    "# step E: expansion, get the nth power of the matrix\n",
    "def expansion(sim):\n",
    "    X = numpy.array(sim)\n",
    "    VarList = sim.keys()\n",
    "    if e == 1:\n",
    "        return sim\n",
    "    elif e > 1:        \n",
    "        return pandas.DataFrame(numpy.linalg.matrix_power(X, e), index = VarList, columns = VarList)\n",
    "     \n",
    "# step I: inflation, per column raise by rth power and column normalise\n",
    "def inflation(sim, type = 'col'):    \n",
    "    if type == 'col':\n",
    "        Axis = 0\n",
    "    elif type == 'row':\n",
    "        Axis = 1\n",
    "    return sim.apply(lambda x: x**r/sum(x**r), axis = Axis)\n",
    "\n",
    "# remove weak connections, values < epsilon\n",
    "def clean(sim):\n",
    "    return sim.applymap(lambda x:0 if x<epsilon else x)\n",
    "    \n",
    "def difference(old, new):\n",
    "    # relative zeroes over entire array\n",
    "    #return (new.apply(lambda x: numpy.ceil(x-epsilon)) - old.apply(lambda x: numpy.ceil(x-epsilon))).sum().sum()/len(old)**2    \n",
    "    return abs(new - old).sum().sum()/len(old)**2    \n",
    "\n",
    "#patient_sim = normalise(patient_sim, type = Orientation)\n",
    "_sim_a = patient_sim\n",
    "for i in range(0,num_iter):\n",
    "    # repeat E and I until convergence, the row-wise elements form the clusters.\n",
    "    _sim_b = clean(inflation(expansion(_sim_a), type = Orientation))\n",
    "    _sim_a = normalise(_sim_a, type = Orientation)\n",
    "    #if ((difference(_sim_a, _sim_b)) < convergence) & (i>0):\n",
    "    #    print(difference(_sim_a, _sim_b))\n",
    "    #    print(\"CONVERGED after \", i, \" iterations\")\n",
    "    #    break;\n",
    "    _sim_a = _sim_b\n",
    "\n",
    "result_mcl = clean(_sim_b)\n",
    "result_mcl.loc[result_mcl.loc['9827_corr2.CEL',:]>epsilon, '9827_corr2.CEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 patient clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'pearson', normalised = False, inflation=1, minkowski_dim=1)\n",
    "##### apply Affinity Propagation\n",
    "#######################\n",
    "X = numpy.array(patient_sim)\n",
    "af = AffinityPropagation(preference=-10).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "patient_clusters = patient_sim.keys()[cluster_centers_indices].values\n",
    "patient_cluster_members = af.labels_\n",
    "print(\"There are {} patient clusters\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AggResults = Rocket.DATA_merged\n",
    "AggResults = _helpers._preprocess(AggResults, Rclass = Rocket)\n",
    "#AggResults = _helpers._group_patients(AggResults, method = 'mean')\n",
    "AggResults['cluster_ap'] = patient_cluster_members\n",
    "\n",
    "#AggResults.groupby(['Treatment risk group in ALL10', 'cluster_ap']).agg({'Microarray file': pandas.Series.nunique})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "AggResults['FU_RFS'] = pandas.to_numeric(AggResults['FU_RFS'])\n",
    "AggResults['FU_EFS'] = pandas.to_numeric(AggResults['FU_EFS'])\n",
    "AggResults['FU_OS'] = pandas.to_numeric(AggResults['FU_OS'])\n",
    "AggResults['WhiteBloodCellcount'] = pandas.to_numeric(AggResults['WhiteBloodCellcount'])\n",
    "AggResults['Age'] = pandas.to_numeric(AggResults['Age'])\n",
    "AggResults['Gender'] = pandas.to_numeric(AggResults['Gender'])\n",
    "AggResults['code_RFS']= pandas.to_numeric(AggResults['code_RFS'])\n",
    "AggResults['code_EFS']= pandas.to_numeric(AggResults['code_EFS'])\n",
    "AggResults['code_OS']= pandas.to_numeric(AggResults['code_OS'])\n",
    "\n",
    "AggResults['mutations_NOTCH_pathway'] = pandas.to_numeric(AggResults['mutations_NOTCH_pathway'])\n",
    "AggResults['mutations_PTEN_AKT_pathway'] = pandas.to_numeric(AggResults['mutations_PTEN_AKT_pathway'])\n",
    "AggResults['mutations_IL7R_pathway'] = pandas.to_numeric(AggResults['mutations_IL7R_pathway'])\n",
    "#AggResults.replace(to_replace=9999, value=0.5, inplace=True)\n",
    "AggResults[['mutations_NOTCH_pathway', \n",
    "            'mutations_PTEN_AKT_pathway', \n",
    "            'mutations_IL7R_pathway']] = AggResults[['mutations_NOTCH_pathway', \n",
    "                                                    'mutations_PTEN_AKT_pathway', \n",
    "                                                    'mutations_IL7R_pathway']].replace([9999],[numpy.nan],\n",
    "                                                                                       inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "AggResults['comb_mutations_NOTCH_IL7R'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_NOTCH_PTEN'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_PTEN_AKT_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN'] =  AggResults['mutations_PTEN_AKT_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN_NOTCH'] =  AggResults['mutations_PTEN_AKT_pathway']\\\n",
    "                                                + AggResults['mutations_IL7R_pathway']\\\n",
    "                                                + AggResults['mutations_NOTCH_pathway']\n",
    "\n",
    "\n",
    "patient_count = AggResults.groupby(['cluster_ap']).agg({'labnr_patient': pandas.Series.nunique})\n",
    "Clustered_by_patients_whitebloodcells = AggResults[AggResults['WhiteBloodCellcount'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'WhiteBloodCellcount': numpy.mean,\n",
    "    'Age': numpy.mean, \n",
    "    'Gender': numpy.mean})\n",
    "\n",
    "# Cancer_gene\n",
    "# Treatment_protocol\n",
    "# Treatment_risk_group_in_ALL_10\n",
    "\n",
    "Clustered_by_patients_CODE = AggResults.groupby(['cluster_ap']).agg(\n",
    "    {'code_RFS': numpy.mean, \n",
    "     'code_EFS': numpy.mean,\n",
    "     'code_OS': numpy.mean})\n",
    "\n",
    "Clustered_by_patients_FU_RFS = AggResults[AggResults['FU_RFS'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'FU_RFS': numpy.median, \n",
    "     'FU_EFS': numpy.median,\n",
    "     'FU_OS': numpy.median})\n",
    "Clustered_by_patients_NotchPath = AggResults[AggResults['mutations_NOTCH_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_NOTCH_pathway': numpy.mean})\n",
    "Clustered_by_patients_IL7RPath = AggResults[AggResults['mutations_IL7R_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_IL7R_pathway': numpy.mean})\n",
    "Clustered_by_patients_PTENAKTPath = AggResults[AggResults['mutations_PTEN_AKT_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_PTEN_AKT_pathway': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_IL7R = AggResults[AggResults['comb_mutations_NOTCH_IL7R'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_IL7R': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_PTEN = AggResults[AggResults['comb_mutations_NOTCH_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN = AggResults[AggResults['comb_mutations_IL7R_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN_NOTCH = AggResults[AggResults['comb_mutations_IL7R_PTEN_NOTCH'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN_NOTCH': numpy.mean})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_agg = pandas.merge(Clustered_by_patients_whitebloodcells, Clustered_by_patients_CODE, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN_NOTCH, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_IL7R, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_PTEN, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_FU_RFS, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_IL7RPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_NotchPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_PTENAKTPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, patient_count, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster centers:\",patient_sim.keys()[cluster_centers_indices].values)\n",
    "print(patient_cluster_members)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    class_members = patient_cluster_members == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.', \n",
    "             label = patient_sim.keys()[cluster_centers_indices[k]])\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.legend()\n",
    "        \n",
    "plt.title('Estimated number of clusters from Affinity Propagation: %d' % n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATE graph from similarity matrix\n",
    "##################\n",
    "# nodes\n",
    "VarList = TransPosed.keys()\n",
    "nodes = []\n",
    "node_index = 0\n",
    "for patient_name in VarList:\n",
    "    nodes.append((node_index, {'name': patient_name}))\n",
    "    node_index = node_index + 1\n",
    "\n",
    "edges = []\n",
    "# edges\n",
    "patient_sim = patient_similarity(Normal, sim_type = 'pearson', normalised = True, inflation=2)\n",
    "node_index_x = 0\n",
    "node_index_y = 0\n",
    "for patient_name_x in VarList:\n",
    "    for patient_name_y in VarList:        \n",
    "        edges.append((node_index_x, node_index_y, patient_sim.iloc[node_index_x, node_index_y]))\n",
    "        node_index_y = node_index_y + 1\n",
    "    node_index_x = node_index_x + 1\n",
    "    node_index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_weighted_edges_from(edges, weight = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:126: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  b = plt.ishold()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:138: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold(b)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFCCAYAAABSJMy8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f31beac128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### apply Spring-force\n",
    "#######################\n",
    "pos = nx.spring_layout(G, k = None, dim = 3, scale = 1.0)\n",
    "nx.draw_spring(G, k = 30, dim = 2, scale = 1.0, iterations =1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### APPLY community detector\n",
    "# maximize betweenness and modularity\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### LOAD IN DATA\n",
    "###################\n",
    "# https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
