{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ Firing up RexR! ++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from RexR import *\n",
    "import _helpers\n",
    "Rocket = RexR(datalocation = None, #'_data/genomic_data/data.pkl', \n",
    "              seed = 3123, \n",
    "              debug = False, \n",
    "              write_out=True,\n",
    "              set_name = 'ALL_10') # data to read in ALL_10, or MELA\n",
    "Rocket.load_probeset_data();\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD_LIST = ['RF','XGB', 'LGBM', 'ExtraTrees','SVM', 'LR', 'MLNN', 'RVM', 'DNN', 'CNN'] #, 'NaiveBayes','MLNN', 'XGB'] # \n",
    "Runs = []\n",
    "nruns = 1\n",
    "SCALER = \"minmax\" # minmax, standard, normaliser\n",
    "GROUPING = \"mean\"\n",
    "FEAT_SELECTOR = None # \"low_variance\" None\n",
    "SELECTOR_METHOD = \"FDR\" # mannwhitney, FDR\n",
    "SELECTOR_ALPHA = 0.025 # see this as the maximum p-value to classify \n",
    "DIM_TYPE =  \"PCA\" #\"LDA\", \"PCA\", \"PLS\" \n",
    "DIM_NUM = 1000\n",
    "Results = None\n",
    "ACC = pd.DataFrame()\n",
    "Rocket.VIZ = False\n",
    "Rocket.DATA_merged_processed = None\n",
    "PREPROC_DICT = {\"patient_grouping\": GROUPING, \"bias_removal\": False, \"noise\": True}\n",
    "FSELECT_DICT = {\"type\": FEAT_SELECTOR, \"pvalue\": SELECTOR_ALPHA, \"method\": SELECTOR_METHOD}\n",
    "DIMRED_DICT = {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Prepping data, this may take a while..\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Grouping probesets\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "MODEL: RF accuracy:  0.5344827586206896 +/-: 0.0024682016358591884\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: XGB accuracy:  0.49999999999999994 +/-: 0.017371995820271685\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: LGBM accuracy:  0.5344827586206896 +/-: 0.032248765899182075\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "MODEL: ET accuracy:  0.5517241379310345 +/-: 0.004116672071487769\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "MODEL: SVM accuracy:  0.7241379310344825 +/-: 0.02499729758944979\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "MODEL: LR accuracy:  0.603448275862069 +/-: 0.017642236875292764\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "MODEL: MLNN accuracy:  0.5862068965517243 +/-: 0.032275790004684175\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "Initial alpha = [[0.12029864]]\n",
      "   1 - L=-971.8666411 - Gamma= 1.9999509 (M=   2) - s=0.0100\n",
      "   2 - L=-874.2586878 - Gamma= 2.9998395 (M=   3) - s=0.0100\n",
      "   3 - L=-792.7541083 - Gamma= 3.9997063 (M=   4) - s=0.0100\n",
      "   4 - L=-709.0706295 - Gamma= 4.9995751 (M=   5) - s=0.0100\n",
      "   5 - L=-632.1044126 - Gamma= 5.9994336 (M=   6) - s=0.0100\n",
      "   6 - L=-562.0039454 - Gamma= 6.9992768 (M=   7) - s=0.0100\n",
      "   7 - L=-492.2049690 - Gamma= 7.9991165 (M=   8) - s=0.0100\n",
      "   8 - L=-434.8521493 - Gamma= 8.9989252 (M=   9) - s=0.0100\n",
      "   9 - L=-383.2074862 - Gamma= 9.9987120 (M=  10) - s=0.0100\n",
      "  10 - L=-321.7614028 - Gamma=10.9985240 (M=  11) - s=0.0100\n",
      "  11 - L=-277.8569718 - Gamma=11.9982687 (M=  12) - s=0.0100\n",
      "  12 - L=-242.0328917 - Gamma=12.9979641 (M=  13) - s=0.0100\n",
      "  13 - L=-214.1130867 - Gamma=13.9975687 (M=  14) - s=0.0100\n",
      "  14 - L=-177.9647369 - Gamma=14.9972373 (M=  15) - s=0.0100\n",
      "  15 - L=-154.4758143 - Gamma=15.9967678 (M=  16) - s=0.0100\n",
      "  16 - L=-121.9173322 - Gamma=16.9964067 (M=  17) - s=0.0100\n",
      "  17 - L=-99.9284351 - Gamma=17.9959014 (M=  18) - s=0.0100\n",
      "  18 - L=-82.4929913 - Gamma=18.9952586 (M=  19) - s=0.0100\n",
      "  19 - L=-70.2922665 - Gamma=19.9943550 (M=  20) - s=0.0100\n",
      "  20 - L=-59.6318633 - Gamma=20.9932908 (M=  21) - s=0.0100\n",
      "  21 - L=-46.3418612 - Gamma=21.9924285 (M=  22) - s=0.0100\n",
      "  22 - L=-39.6719667 - Gamma=22.9907418 (M=  23) - s=0.0100\n",
      "  23 - L=-34.5860996 - Gamma=23.9884306 (M=  24) - s=0.0100\n",
      "  24 - L=-28.8133250 - Gamma=24.9865126 (M=  25) - s=0.0100\n",
      "  25 - L=-23.2281192 - Gamma=25.9845517 (M=  26) - s=0.0100\n",
      "  26 - L=-18.4146659 - Gamma=26.9822031 (M=  27) - s=0.0100\n",
      "  27 - L=-12.2933969 - Gamma=27.9802950 (M=  28) - s=0.0100\n",
      "  28 - L=-8.0096145 - Gamma=28.9774040 (M=  29) - s=0.0100\n",
      "  29 - L=-5.2511599 - Gamma=29.9733676 (M=  30) - s=0.0100\n",
      "  30 - L=-3.1444944 - Gamma=30.9680519 (M=  31) - s=0.0100\n",
      "  31 - L=-0.9973434 - Gamma=31.9625812 (M=  32) - s=0.0100\n",
      "  32 - L=-0.3158107 - Gamma=32.9472742 (M=  33) - s=0.0100\n",
      "  33 - L= 0.4461910 - Gamma=33.9317698 (M=  34) - s=0.0100\n",
      "  34 - L= 0.9662594 - Gamma=34.9081275 (M=  35) - s=0.0100\n",
      "  35 - L= 1.0262715 - Gamma=35.7895407 (M=  36) - s=0.0100\n",
      "  36 - L= 1.0628708 - Gamma=36.6228019 (M=  37) - s=0.0100\n",
      "  37 - L= 1.0801982 - Gamma=37.3493231 (M=  38) - s=0.0100\n",
      "  38 - L= 1.0958714 - Gamma=38.0752096 (M=  39) - s=0.0100\n",
      "  39 - L= 1.1123918 - Gamma=38.8050677 (M=  40) - s=0.0100\n",
      "  40 - L= 1.1236580 - Gamma=38.8055296 (M=  40) - s=0.0100\n",
      "  41 - L= 1.1295795 - Gamma=38.8046864 (M=  40) - s=0.0100\n",
      "  42 - L= 1.1352559 - Gamma=38.8048465 (M=  40) - s=0.0100\n",
      "  43 - L= 1.1408672 - Gamma=38.8162198 (M=  40) - s=0.0100\n",
      "  44 - L= 1.1461751 - Gamma=38.8163530 (M=  40) - s=0.0100\n",
      "  45 - L= 1.1509936 - Gamma=38.8184520 (M=  40) - s=0.0100\n",
      "  46 - L= 1.1548655 - Gamma=38.8185653 (M=  40) - s=0.0100\n",
      "  47 - L= 1.1584845 - Gamma=38.8178111 (M=  40) - s=0.0100\n",
      "  48 - L= 1.1613007 - Gamma=38.8189579 (M=  40) - s=0.0100\n",
      "  49 - L= 1.1638828 - Gamma=38.8191374 (M=  40) - s=0.0100\n",
      "  50 - L= 1.1662107 - Gamma=38.8192067 (M=  40) - s=0.0100\n",
      "  51 - L= 1.1684428 - Gamma=38.8197858 (M=  40) - s=0.0100\n",
      "  52 - L= 1.1705551 - Gamma=38.8202554 (M=  40) - s=0.0100\n",
      "  53 - L= 1.1725939 - Gamma=39.2012316 (M=  41) - s=0.0100\n",
      "  54 - L= 1.1743690 - Gamma=39.2017418 (M=  41) - s=0.0100\n",
      "  55 - L= 1.1758911 - Gamma=39.1974040 (M=  41) - s=0.0100\n",
      "  56 - L= 1.1770205 - Gamma=39.1975101 (M=  41) - s=0.0100\n",
      "  57 - L= 1.1780323 - Gamma=39.1988112 (M=  41) - s=0.0100\n",
      "  58 - L= 1.1788995 - Gamma=39.2377357 (M=  41) - s=0.0100\n",
      "  59 - L= 1.1797602 - Gamma=39.2378407 (M=  41) - s=0.0100\n",
      "  60 - L= 1.1805137 - Gamma=39.2378164 (M=  41) - s=0.0100\n",
      "  61 - L= 1.1811986 - Gamma=39.2378706 (M=  41) - s=0.0100\n",
      "  62 - L= 1.1818609 - Gamma=39.2312352 (M=  41) - s=0.0100\n",
      "  63 - L= 1.1823404 - Gamma=39.2312966 (M=  41) - s=0.0100\n",
      "  64 - L= 1.1826388 - Gamma=39.2314258 (M=  41) - s=0.0100\n",
      "  65 - L= 1.1829141 - Gamma=39.2294715 (M=  41) - s=0.0100\n",
      "  66 - L= 1.1831271 - Gamma=39.2296338 (M=  41) - s=0.0100\n",
      "  67 - L= 1.1832358 - Gamma=39.2304483 (M=  41) - s=0.0100\n",
      "  68 - L= 1.1833406 - Gamma=39.2306369 (M=  41) - s=0.0100\n",
      "  69 - L= 1.1834245 - Gamma=39.2082001 (M=  41) - s=0.0100\n",
      "  70 - L= 1.1835065 - Gamma=39.2084334 (M=  41) - s=0.0100\n",
      "  71 - L= 1.1835409 - Gamma=39.2084609 (M=  41) - s=0.0100\n",
      "  72 - L= 1.1835742 - Gamma=39.2084367 (M=  41) - s=0.0100\n",
      "  73 - L= 1.1836067 - Gamma=39.1666048 (M=  41) - s=0.0100\n",
      "  74 - L= 1.1836397 - Gamma=39.1862162 (M=  41) - s=0.0100\n",
      "  75 - L= 1.1836528 - Gamma=39.1860853 (M=  41) - s=0.0100\n",
      "  76 - L= 1.1836629 - Gamma=39.1859923 (M=  41) - s=0.0100\n",
      "  77 - L= 1.1836716 - Gamma=39.1863149 (M=  41) - s=0.0100\n",
      "  78 - L= 1.1836756 - Gamma=39.1869754 (M=  41) - s=0.0100\n",
      "  79 - L= 1.1836793 - Gamma=39.1891764 (M=  41) - s=0.0100\n",
      "  80 - L= 1.1836814 - Gamma=39.1892017 (M=  41) - s=0.0100\n",
      "  81 - L= 1.1836833 - Gamma=39.1856490 (M=  41) - s=0.0100\n",
      "  82 - L= 1.1836849 - Gamma=39.1761775 (M=  41) - s=0.0100\n",
      "  83 - L= 1.1836863 - Gamma=39.1720473 (M=  41) - s=0.0100\n",
      "  84 - L= 1.1836875 - Gamma=39.1720564 (M=  41) - s=0.0100\n",
      "  85 - L= 1.1836885 - Gamma=39.1721900 (M=  41) - s=0.0100\n",
      "  86 - L= 1.1836894 - Gamma=39.1721343 (M=  41) - s=0.0100\n",
      "  87 - L= 1.1836901 - Gamma=39.1721526 (M=  41) - s=0.0100\n",
      "  88 - L= 1.1836907 - Gamma=39.1695433 (M=  41) - s=0.0100\n",
      "  89 - L= 1.1836913 - Gamma=39.1695305 (M=  41) - s=0.0100\n",
      "  90 - L= 1.1836916 - Gamma=39.1695314 (M=  41) - s=0.0100\n",
      "  91 - L= 1.1836918 - Gamma=39.1694765 (M=  41) - s=0.0100\n",
      "  92 - L= 1.1836920 - Gamma=39.1694831 (M=  41) - s=0.0100\n",
      "  93 - L= 1.1836921 - Gamma=39.1694836 (M=  41) - s=0.0100\n",
      "  94 - L= 1.1836922 - Gamma=39.1694846 (M=  41) - s=0.0100\n",
      "  95 - L= 1.1836923 - Gamma=39.1705780 (M=  41) - s=0.0100\n",
      "  96 - L= 1.1836924 - Gamma=39.1704827 (M=  41) - s=0.0100\n",
      "  97 - L= 1.1836925 - Gamma=39.1704729 (M=  41) - s=0.0100\n",
      "  98 - L= 1.1836925 - Gamma=39.1704753 (M=  41) - s=0.0100\n",
      "  99 - L= 1.1836926 - Gamma=39.1704115 (M=  41) - s=0.0100\n",
      " 100 - L= 1.1836926 - Gamma=39.1698570 (M=  41) - s=0.0100\n",
      " 101 - L= 1.1836927 - Gamma=39.1700778 (M=  41) - s=0.0100\n",
      " 102 - L= 1.1836927 - Gamma=39.1700731 (M=  41) - s=0.0100\n",
      " 103 - L= 1.1836927 - Gamma=39.1700736 (M=  41) - s=0.0100\n",
      " 104 - L= 1.1836928 - Gamma=39.1700738 (M=  41) - s=0.0100\n",
      " 105 - L= 1.1836928 - Gamma=39.1700728 (M=  41) - s=0.0100\n",
      " 106 - L= 1.1836928 - Gamma=39.1700728 (M=  41) - s=0.0100\n",
      " 107 - L= 1.1836928 - Gamma=39.1700742 (M=  41) - s=0.0100\n",
      " 108 - L= 1.1836928 - Gamma=39.1704238 (M=  41) - s=0.0100\n",
      " 109 - L= 1.1836928 - Gamma=39.1694801 (M=  41) - s=0.0100\n",
      " 110 - L= 1.1836929 - Gamma=39.1694799 (M=  41) - s=0.0100\n",
      " 111 - L= 1.1836929 - Gamma=39.1694802 (M=  41) - s=0.0100\n",
      " 112 - L= 1.1836929 - Gamma=39.1694802 (M=  41) - s=0.0100\n",
      "Stopping at iteration 112 - max_delta_ml=2.4116382166218184e-07\n",
      "L=1.183692866438792 - Gamma=39.169480241681356 (M=41) - s=0.01\n",
      "Initial alpha = [[0.11655583]]\n",
      "   1 - L=-976.7126170 - Gamma= 1.9999466 (M=   2) - s=0.0100\n",
      "   2 - L=-838.8291728 - Gamma= 2.9998671 (M=   3) - s=0.0100\n",
      "   3 - L=-733.4385522 - Gamma= 3.9997638 (M=   4) - s=0.0100\n",
      "   4 - L=-637.9965372 - Gamma= 4.9996486 (M=   5) - s=0.0100\n",
      "   5 - L=-554.5503643 - Gamma= 5.9995166 (M=   6) - s=0.0100\n",
      "   6 - L=-485.5853749 - Gamma= 6.9993574 (M=   7) - s=0.0100\n",
      "   7 - L=-433.6752491 - Gamma= 7.9991476 (M=   8) - s=0.0100\n",
      "   8 - L=-379.5952634 - Gamma= 8.9989417 (M=   9) - s=0.0100\n",
      "   9 - L=-330.1529162 - Gamma= 9.9987198 (M=  10) - s=0.0100\n",
      "  10 - L=-285.5710291 - Gamma=10.9984727 (M=  11) - s=0.0100\n",
      "  11 - L=-258.9129824 - Gamma=11.9980585 (M=  12) - s=0.0100\n",
      "  12 - L=-228.3606903 - Gamma=12.9976874 (M=  13) - s=0.0100\n",
      "  13 - L=-202.1253003 - Gamma=13.9972466 (M=  14) - s=0.0100\n",
      "  14 - L=-178.4538570 - Gamma=14.9967575 (M=  15) - s=0.0100\n",
      "  15 - L=-155.5760019 - Gamma=15.9962731 (M=  16) - s=0.0100\n",
      "  16 - L=-135.8782033 - Gamma=16.9957039 (M=  17) - s=0.0100\n",
      "  17 - L=-118.1732216 - Gamma=17.9950727 (M=  18) - s=0.0100\n",
      "  18 - L=-107.2114248 - Gamma=18.9940600 (M=  19) - s=0.0100\n",
      "  19 - L=-90.5746332 - Gamma=19.9931443 (M=  20) - s=0.0100\n",
      "  20 - L=-77.6381357 - Gamma=20.9922237 (M=  21) - s=0.0100\n",
      "  21 - L=-65.7172604 - Gamma=21.9912051 (M=  22) - s=0.0100\n",
      "  22 - L=-54.7632695 - Gamma=22.9901351 (M=  23) - s=0.0100\n",
      "  23 - L=-43.8876125 - Gamma=23.9890942 (M=  24) - s=0.0100\n",
      "  24 - L=-36.2529925 - Gamma=24.9875641 (M=  25) - s=0.0100\n",
      "  25 - L=-29.8775889 - Gamma=25.9856636 (M=  26) - s=0.0100\n",
      "  26 - L=-23.5107232 - Gamma=26.9838299 (M=  27) - s=0.0100\n",
      "  27 - L=-19.0267286 - Gamma=27.9809679 (M=  28) - s=0.0100\n",
      "  28 - L=-15.2726665 - Gamma=28.9774166 (M=  29) - s=0.0100\n",
      "  29 - L=-10.6376394 - Gamma=29.9749114 (M=  30) - s=0.0100\n",
      "  30 - L=-7.0504989 - Gamma=30.9707104 (M=  31) - s=0.0100\n",
      "  31 - L=-5.2798010 - Gamma=31.9644583 (M=  32) - s=0.0100\n",
      "  32 - L=-3.5029975 - Gamma=32.9578441 (M=  33) - s=0.0100\n",
      "  33 - L=-2.4879300 - Gamma=33.9463890 (M=  34) - s=0.0100\n",
      "  34 - L=-1.6886433 - Gamma=34.9309451 (M=  35) - s=0.0100\n",
      "  35 - L=-1.0483360 - Gamma=35.9130564 (M=  36) - s=0.0100\n",
      "  36 - L=-0.3755689 - Gamma=36.8957352 (M=  37) - s=0.0100\n",
      "  37 - L= 0.5966277 - Gamma=37.8706005 (M=  38) - s=0.0100\n",
      "  38 - L= 0.7358193 - Gamma=38.8032511 (M=  39) - s=0.0100\n",
      "  39 - L= 0.8209123 - Gamma=39.7079370 (M=  40) - s=0.0100\n",
      "  40 - L= 0.8645057 - Gamma=39.7090140 (M=  40) - s=0.0100\n",
      "  41 - L= 0.8864347 - Gamma=39.7093441 (M=  40) - s=0.0100\n",
      "  42 - L= 0.9060925 - Gamma=39.7101025 (M=  40) - s=0.0100\n",
      "  43 - L= 0.9230722 - Gamma=39.7111908 (M=  40) - s=0.0100\n",
      "  44 - L= 0.9341498 - Gamma=39.7123706 (M=  40) - s=0.0100\n",
      "  45 - L= 0.9443705 - Gamma=39.7126552 (M=  40) - s=0.0100\n",
      "  46 - L= 0.9537719 - Gamma=39.7155463 (M=  40) - s=0.0100\n",
      "  47 - L= 0.9622765 - Gamma=39.7095620 (M=  40) - s=0.0100\n",
      "  48 - L= 0.9707840 - Gamma=39.7108907 (M=  40) - s=0.0100\n",
      "  49 - L= 0.9789211 - Gamma=39.7122024 (M=  40) - s=0.0100\n",
      "  50 - L= 0.9856582 - Gamma=39.7118689 (M=  40) - s=0.0100\n",
      "  51 - L= 0.9923535 - Gamma=39.7083994 (M=  40) - s=0.0100\n",
      "  52 - L= 0.9986424 - Gamma=39.7048448 (M=  40) - s=0.0100\n",
      "  53 - L= 1.0049276 - Gamma=39.7122958 (M=  40) - s=0.0100\n",
      "  54 - L= 1.0106510 - Gamma=40.2885260 (M=  41) - s=0.0100\n",
      "  55 - L= 1.0162239 - Gamma=40.2846334 (M=  41) - s=0.0100\n",
      "  56 - L= 1.0217758 - Gamma=40.2841783 (M=  41) - s=0.0100\n",
      "  57 - L= 1.0272464 - Gamma=40.2832385 (M=  41) - s=0.0100\n",
      "  58 - L= 1.0318217 - Gamma=40.2950450 (M=  41) - s=0.0100\n",
      "  59 - L= 1.0364636 - Gamma=40.3055259 (M=  41) - s=0.0100\n",
      "  60 - L= 1.0410235 - Gamma=40.3019119 (M=  41) - s=0.0100\n",
      "  61 - L= 1.0454738 - Gamma=40.3046443 (M=  41) - s=0.0100\n",
      "  62 - L= 1.0480656 - Gamma=40.3084389 (M=  41) - s=0.0100\n",
      "  63 - L= 1.0504850 - Gamma=40.3095974 (M=  41) - s=0.0100\n",
      "  64 - L= 1.0525604 - Gamma=40.3166377 (M=  41) - s=0.0100\n",
      "  65 - L= 1.0543313 - Gamma=40.3048051 (M=  41) - s=0.0100\n",
      "  66 - L= 1.0560308 - Gamma=40.3058427 (M=  41) - s=0.0100\n",
      "  67 - L= 1.0575174 - Gamma=40.3060129 (M=  41) - s=0.0100\n",
      "  68 - L= 1.0585588 - Gamma=40.3070842 (M=  41) - s=0.0100\n",
      "  69 - L= 1.0593654 - Gamma=40.3082161 (M=  41) - s=0.0100\n",
      "  70 - L= 1.0600743 - Gamma=40.3047712 (M=  41) - s=0.0100\n",
      "  71 - L= 1.0607690 - Gamma=40.3047428 (M=  41) - s=0.0100\n",
      "  72 - L= 1.0613830 - Gamma=40.2784450 (M=  41) - s=0.0100\n",
      "  73 - L= 1.0618507 - Gamma=40.2786406 (M=  41) - s=0.0100\n",
      "  74 - L= 1.0620189 - Gamma=40.2787243 (M=  41) - s=0.0100\n",
      "  75 - L= 1.0621548 - Gamma=40.2786298 (M=  41) - s=0.0100\n",
      "  76 - L= 1.0622824 - Gamma=40.3354184 (M=  41) - s=0.0100\n",
      "  77 - L= 1.0623763 - Gamma=40.3353945 (M=  41) - s=0.0100\n",
      "  78 - L= 1.0624341 - Gamma=40.3348007 (M=  41) - s=0.0100\n",
      "  79 - L= 1.0624869 - Gamma=40.3348296 (M=  41) - s=0.0100\n",
      "  80 - L= 1.0625322 - Gamma=40.3348020 (M=  41) - s=0.0100\n",
      "  81 - L= 1.0625654 - Gamma=40.3343740 (M=  41) - s=0.0100\n",
      "  82 - L= 1.0625850 - Gamma=40.3344676 (M=  41) - s=0.0100\n",
      "  83 - L= 1.0625934 - Gamma=40.3344996 (M=  41) - s=0.0100\n",
      "  84 - L= 1.0626003 - Gamma=40.3345211 (M=  41) - s=0.0100\n",
      "  85 - L= 1.0626034 - Gamma=40.3343842 (M=  41) - s=0.0100\n",
      "  86 - L= 1.0626065 - Gamma=40.3343903 (M=  41) - s=0.0100\n",
      "  87 - L= 1.0626093 - Gamma=40.3346055 (M=  41) - s=0.0100\n",
      "  88 - L= 1.0626119 - Gamma=40.3344370 (M=  41) - s=0.0100\n",
      "  89 - L= 1.0626145 - Gamma=40.3363254 (M=  41) - s=0.0100\n",
      "  90 - L= 1.0626165 - Gamma=40.3363283 (M=  41) - s=0.0100\n",
      "  91 - L= 1.0626180 - Gamma=40.3362351 (M=  41) - s=0.0100\n",
      "  92 - L= 1.0626192 - Gamma=40.3362405 (M=  41) - s=0.0100\n",
      "  93 - L= 1.0626202 - Gamma=40.3410415 (M=  41) - s=0.0100\n",
      "  94 - L= 1.0626213 - Gamma=40.3406863 (M=  41) - s=0.0100\n",
      "  95 - L= 1.0626223 - Gamma=40.3406888 (M=  41) - s=0.0100\n",
      "  96 - L= 1.0626232 - Gamma=40.3395537 (M=  41) - s=0.0100\n",
      "  97 - L= 1.0626237 - Gamma=40.3395515 (M=  41) - s=0.0100\n",
      "  98 - L= 1.0626241 - Gamma=40.3395648 (M=  41) - s=0.0100\n",
      "  99 - L= 1.0626244 - Gamma=40.3395318 (M=  41) - s=0.0100\n",
      " 100 - L= 1.0626246 - Gamma=40.3395033 (M=  41) - s=0.0100\n",
      " 101 - L= 1.0626248 - Gamma=40.3394451 (M=  41) - s=0.0100\n",
      " 102 - L= 1.0626250 - Gamma=40.3394308 (M=  41) - s=0.0100\n",
      " 103 - L= 1.0626251 - Gamma=40.3394273 (M=  41) - s=0.0100\n",
      " 104 - L= 1.0626253 - Gamma=40.3393709 (M=  41) - s=0.0100\n",
      " 105 - L= 1.0626254 - Gamma=40.3393714 (M=  41) - s=0.0100\n",
      " 106 - L= 1.0626254 - Gamma=40.3393761 (M=  41) - s=0.0100\n",
      " 107 - L= 1.0626255 - Gamma=40.3393835 (M=  41) - s=0.0100\n",
      " 108 - L= 1.0626255 - Gamma=40.3403204 (M=  41) - s=0.0100\n",
      " 109 - L= 1.0626256 - Gamma=40.3403197 (M=  41) - s=0.0100\n",
      " 110 - L= 1.0626256 - Gamma=40.3403211 (M=  41) - s=0.0100\n",
      " 111 - L= 1.0626256 - Gamma=40.3403108 (M=  41) - s=0.0100\n",
      " 112 - L= 1.0626256 - Gamma=40.3403118 (M=  41) - s=0.0100\n",
      " 113 - L= 1.0626256 - Gamma=40.3404198 (M=  41) - s=0.0100\n",
      " 114 - L= 1.0626256 - Gamma=40.3404224 (M=  41) - s=0.0100\n",
      " 115 - L= 1.0626256 - Gamma=40.3404219 (M=  41) - s=0.0100\n",
      " 116 - L= 1.0626256 - Gamma=40.3404163 (M=  41) - s=0.0100\n",
      " 117 - L= 1.0626256 - Gamma=40.3404163 (M=  41) - s=0.0100\n",
      "Stopping at iteration 117 - max_delta_ml=1.8427157763150097e-07\n",
      "L=1.0626256322271042 - Gamma=40.340416252842964 (M=41) - s=0.01\n",
      "Initial alpha = [[0.12494576]]\n",
      "   1 - L=-1075.7084334 - Gamma= 1.9999390 (M=   2) - s=0.0100\n",
      "   2 - L=-968.2307178 - Gamma= 2.9998379 (M=   3) - s=0.0100\n",
      "   3 - L=-866.1449852 - Gamma= 3.9997312 (M=   4) - s=0.0100\n",
      "   4 - L=-762.7114104 - Gamma= 4.9996262 (M=   5) - s=0.0100\n",
      "   5 - L=-691.4310327 - Gamma= 5.9994732 (M=   6) - s=0.0100\n",
      "   6 - L=-632.9178723 - Gamma= 6.9992869 (M=   7) - s=0.0100\n",
      "   7 - L=-580.7802182 - Gamma= 7.9990766 (M=   8) - s=0.0100\n",
      "   8 - L=-525.2252665 - Gamma= 8.9988791 (M=   9) - s=0.0100\n",
      "   9 - L=-475.9638088 - Gamma= 9.9986578 (M=  10) - s=0.0100\n",
      "  10 - L=-426.3863743 - Gamma=10.9984353 (M=  11) - s=0.0100\n",
      "  11 - L=-381.6780164 - Gamma=11.9981788 (M=  12) - s=0.0100\n",
      "  12 - L=-327.8178113 - Gamma=12.9979659 (M=  13) - s=0.0100\n",
      "  13 - L=-271.8339649 - Gamma=13.9977539 (M=  14) - s=0.0100\n",
      "  14 - L=-232.6588507 - Gamma=14.9974747 (M=  15) - s=0.0100\n",
      "  15 - L=-199.1840422 - Gamma=15.9971391 (M=  16) - s=0.0100\n",
      "  16 - L=-166.6820808 - Gamma=16.9967821 (M=  17) - s=0.0100\n",
      "  17 - L=-139.4717696 - Gamma=17.9963811 (M=  18) - s=0.0100\n",
      "  18 - L=-116.7942818 - Gamma=18.9958888 (M=  19) - s=0.0100\n",
      "  19 - L=-98.9001322 - Gamma=19.9952713 (M=  20) - s=0.0100\n",
      "  20 - L=-75.6467038 - Gamma=20.9945598 (M=  21) - s=0.0100\n",
      "  21 - L=-64.5230836 - Gamma=21.9935657 (M=  22) - s=0.0100\n",
      "  22 - L=-54.2371502 - Gamma=22.9924410 (M=  23) - s=0.0100\n",
      "  23 - L=-44.0619101 - Gamma=23.9913543 (M=  24) - s=0.0100\n",
      "  24 - L=-36.6458689 - Gamma=24.9894349 (M=  25) - s=0.0100\n",
      "  25 - L=-28.5294177 - Gamma=25.9878505 (M=  26) - s=0.0100\n",
      "  26 - L=-20.9649712 - Gamma=26.9862078 (M=  27) - s=0.0100\n",
      "  27 - L=-15.0668913 - Gamma=27.9840012 (M=  28) - s=0.0100\n",
      "  28 - L=-12.1177833 - Gamma=28.9803722 (M=  29) - s=0.0100\n",
      "  29 - L=-9.8106825 - Gamma=29.9752954 (M=  30) - s=0.0100\n",
      "  30 - L=-8.0908463 - Gamma=30.9687818 (M=  31) - s=0.0100\n",
      "  31 - L=-6.2857406 - Gamma=31.9622494 (M=  32) - s=0.0100\n",
      "  32 - L=-4.8208532 - Gamma=32.9548723 (M=  33) - s=0.0100\n",
      "  33 - L=-3.6066827 - Gamma=33.9446510 (M=  34) - s=0.0100\n",
      "  34 - L=-2.0956769 - Gamma=34.9368742 (M=  35) - s=0.0100\n",
      "  35 - L=-1.2371564 - Gamma=35.9236827 (M=  36) - s=0.0100\n",
      "  36 - L=-0.4197125 - Gamma=36.9055981 (M=  37) - s=0.0100\n",
      "  37 - L= 0.3149618 - Gamma=37.8913822 (M=  38) - s=0.0100\n",
      "  38 - L= 0.5688205 - Gamma=38.8534552 (M=  39) - s=0.0100\n",
      "  39 - L= 0.7294569 - Gamma=39.7822753 (M=  40) - s=0.0100\n",
      "  40 - L= 0.7876162 - Gamma=40.6566960 (M=  41) - s=0.0100\n",
      "  41 - L= 0.8174840 - Gamma=41.4702275 (M=  42) - s=0.0100\n",
      "  42 - L= 0.8374084 - Gamma=41.4726845 (M=  42) - s=0.0100\n",
      "  43 - L= 0.8523543 - Gamma=41.4734617 (M=  42) - s=0.0100\n",
      "  44 - L= 0.8656811 - Gamma=41.4707002 (M=  42) - s=0.0100\n",
      "  45 - L= 0.8785959 - Gamma=41.4769507 (M=  42) - s=0.0100\n",
      "  46 - L= 0.8905422 - Gamma=41.4183206 (M=  42) - s=0.0100\n",
      "  47 - L= 0.9016920 - Gamma=41.4186625 (M=  42) - s=0.0100\n",
      "  48 - L= 0.9086020 - Gamma=41.4201007 (M=  42) - s=0.0100\n",
      "  49 - L= 0.9154854 - Gamma=41.4251182 (M=  42) - s=0.0100\n",
      "  50 - L= 0.9220340 - Gamma=42.0111533 (M=  43) - s=0.0100\n",
      "  51 - L= 0.9267159 - Gamma=42.0113808 (M=  43) - s=0.0100\n",
      "  52 - L= 0.9310308 - Gamma=42.0104486 (M=  43) - s=0.0100\n",
      "  53 - L= 0.9352870 - Gamma=41.9785796 (M=  43) - s=0.0100\n",
      "  54 - L= 0.9393124 - Gamma=41.9839509 (M=  43) - s=0.0100\n",
      "  55 - L= 0.9426417 - Gamma=41.9865652 (M=  43) - s=0.0100\n",
      "  56 - L= 0.9455661 - Gamma=41.9867528 (M=  43) - s=0.0100\n",
      "  57 - L= 0.9483972 - Gamma=41.9859148 (M=  43) - s=0.0100\n",
      "  58 - L= 0.9511080 - Gamma=41.9867365 (M=  43) - s=0.0100\n",
      "  59 - L= 0.9536813 - Gamma=41.9844156 (M=  43) - s=0.0100\n",
      "  60 - L= 0.9560812 - Gamma=41.9847010 (M=  43) - s=0.0100\n",
      "  61 - L= 0.9584440 - Gamma=41.9849333 (M=  43) - s=0.0100\n",
      "  62 - L= 0.9604373 - Gamma=41.9903924 (M=  43) - s=0.0100\n",
      "  63 - L= 0.9618783 - Gamma=41.9909579 (M=  43) - s=0.0100\n",
      "  64 - L= 0.9632184 - Gamma=41.9912680 (M=  43) - s=0.0100\n",
      "  65 - L= 0.9644082 - Gamma=41.9908854 (M=  43) - s=0.0100\n",
      "  66 - L= 0.9653615 - Gamma=41.9896243 (M=  43) - s=0.0100\n",
      "  67 - L= 0.9662437 - Gamma=41.9893518 (M=  43) - s=0.0100\n",
      "  68 - L= 0.9671037 - Gamma=42.0064671 (M=  43) - s=0.0100\n",
      "  69 - L= 0.9679560 - Gamma=42.0060621 (M=  43) - s=0.0100\n",
      "  70 - L= 0.9684115 - Gamma=42.0060707 (M=  43) - s=0.0100\n",
      "  71 - L= 0.9688021 - Gamma=42.0047415 (M=  43) - s=0.0100\n",
      "  72 - L= 0.9690910 - Gamma=42.0046875 (M=  43) - s=0.0100\n",
      "  73 - L= 0.9693485 - Gamma=42.0007356 (M=  43) - s=0.0100\n",
      "  74 - L= 0.9695413 - Gamma=42.0008289 (M=  43) - s=0.0100\n",
      "  75 - L= 0.9696926 - Gamma=42.0283635 (M=  43) - s=0.0100\n",
      "  76 - L= 0.9698091 - Gamma=42.0286421 (M=  43) - s=0.0100\n",
      "  77 - L= 0.9699210 - Gamma=42.0285932 (M=  43) - s=0.0100\n",
      "  78 - L= 0.9700269 - Gamma=42.0767382 (M=  43) - s=0.0100\n",
      "  79 - L= 0.9701233 - Gamma=42.0777010 (M=  43) - s=0.0100\n",
      "  80 - L= 0.9702050 - Gamma=42.0776853 (M=  43) - s=0.0100\n",
      "  81 - L= 0.9702557 - Gamma=42.0763004 (M=  43) - s=0.0100\n",
      "  82 - L= 0.9702953 - Gamma=42.0792866 (M=  43) - s=0.0100\n",
      "  83 - L= 0.9703232 - Gamma=42.0792948 (M=  43) - s=0.0100\n",
      "  84 - L= 0.9703385 - Gamma=42.0729324 (M=  43) - s=0.0100\n",
      "  85 - L= 0.9703510 - Gamma=42.0730769 (M=  43) - s=0.0100\n",
      "  86 - L= 0.9703635 - Gamma=42.0729379 (M=  43) - s=0.0100\n",
      "  87 - L= 0.9703731 - Gamma=42.0730717 (M=  43) - s=0.0100\n",
      "  88 - L= 0.9703815 - Gamma=42.0702870 (M=  43) - s=0.0100\n",
      "  89 - L= 0.9703887 - Gamma=42.0702914 (M=  43) - s=0.0100\n",
      "  90 - L= 0.9703930 - Gamma=42.0703169 (M=  43) - s=0.0100\n",
      "  91 - L= 0.9703971 - Gamma=42.0702949 (M=  43) - s=0.0100\n",
      "  92 - L= 0.9703995 - Gamma=42.0710950 (M=  43) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  93 - L= 0.9704021 - Gamma=42.0699665 (M=  43) - s=0.0100\n",
      "  94 - L= 0.9704033 - Gamma=42.0700401 (M=  43) - s=0.0100\n",
      "  95 - L= 0.9704043 - Gamma=42.0701416 (M=  43) - s=0.0100\n",
      "  96 - L= 0.9704053 - Gamma=42.0743861 (M=  43) - s=0.0100\n",
      "  97 - L= 0.9704058 - Gamma=42.0743582 (M=  43) - s=0.0100\n",
      "  98 - L= 0.9704064 - Gamma=42.0758833 (M=  43) - s=0.0100\n",
      "  99 - L= 0.9704067 - Gamma=42.0758704 (M=  43) - s=0.0100\n",
      " 100 - L= 0.9704071 - Gamma=42.0758826 (M=  43) - s=0.0100\n",
      " 101 - L= 0.9704075 - Gamma=42.0758466 (M=  43) - s=0.0100\n",
      " 102 - L= 0.9704078 - Gamma=42.0758390 (M=  43) - s=0.0100\n",
      " 103 - L= 0.9704080 - Gamma=42.0758279 (M=  43) - s=0.0100\n",
      " 104 - L= 0.9704082 - Gamma=42.0758247 (M=  43) - s=0.0100\n",
      " 105 - L= 0.9704083 - Gamma=42.0758240 (M=  43) - s=0.0100\n",
      " 106 - L= 0.9704085 - Gamma=42.0758226 (M=  43) - s=0.0100\n",
      " 107 - L= 0.9704086 - Gamma=42.0758238 (M=  43) - s=0.0100\n",
      " 108 - L= 0.9704086 - Gamma=42.0758225 (M=  43) - s=0.0100\n",
      " 109 - L= 0.9704087 - Gamma=42.0758218 (M=  43) - s=0.0100\n",
      " 110 - L= 0.9704088 - Gamma=42.0758885 (M=  43) - s=0.0100\n",
      " 111 - L= 0.9704088 - Gamma=42.0760112 (M=  43) - s=0.0100\n",
      " 112 - L= 0.9704089 - Gamma=42.0758448 (M=  43) - s=0.0100\n",
      " 113 - L= 0.9704089 - Gamma=42.0758425 (M=  43) - s=0.0100\n",
      " 114 - L= 0.9704089 - Gamma=42.0765732 (M=  43) - s=0.0100\n",
      " 115 - L= 0.9704090 - Gamma=42.0765709 (M=  43) - s=0.0100\n",
      " 116 - L= 0.9704090 - Gamma=42.0764391 (M=  43) - s=0.0100\n",
      " 117 - L= 0.9704090 - Gamma=42.0764442 (M=  43) - s=0.0100\n",
      " 118 - L= 0.9704090 - Gamma=42.0764434 (M=  43) - s=0.0100\n",
      " 119 - L= 0.9704090 - Gamma=42.0764482 (M=  43) - s=0.0100\n",
      " 120 - L= 0.9704090 - Gamma=42.0764264 (M=  43) - s=0.0100\n",
      " 121 - L= 0.9704090 - Gamma=42.0764261 (M=  43) - s=0.0100\n",
      " 122 - L= 0.9704090 - Gamma=42.0764265 (M=  43) - s=0.0100\n",
      " 123 - L= 0.9704091 - Gamma=42.0764345 (M=  43) - s=0.0100\n",
      " 124 - L= 0.9704091 - Gamma=42.0765527 (M=  43) - s=0.0100\n",
      " 125 - L= 0.9704091 - Gamma=42.0765527 (M=  43) - s=0.0100\n",
      "Stopping at iteration 125 - max_delta_ml=2.1919180775113864e-07\n",
      "L=0.9704090622675364 - Gamma=42.07655267183454 (M=43) - s=0.01\n",
      "Initial alpha = [[0.118738]]\n",
      "   1 - L=-1041.6112951 - Gamma= 1.9999308 (M=   2) - s=0.0100\n",
      "   2 - L=-893.1934375 - Gamma= 2.9998589 (M=   3) - s=0.0100\n",
      "   3 - L=-757.2749946 - Gamma= 3.9997806 (M=   4) - s=0.0100\n",
      "   4 - L=-649.2303872 - Gamma= 4.9996822 (M=   5) - s=0.0100\n",
      "   5 - L=-558.2993680 - Gamma= 5.9995639 (M=   6) - s=0.0100\n",
      "   6 - L=-480.2968623 - Gamma= 6.9994247 (M=   7) - s=0.0100\n",
      "   7 - L=-411.4232547 - Gamma= 7.9992526 (M=   8) - s=0.0100\n",
      "   8 - L=-353.8882953 - Gamma= 8.9990640 (M=   9) - s=0.0100\n",
      "   9 - L=-312.0088087 - Gamma= 9.9988091 (M=  10) - s=0.0100\n",
      "  10 - L=-273.5066272 - Gamma=10.9985302 (M=  11) - s=0.0100\n",
      "  11 - L=-238.9648918 - Gamma=11.9982123 (M=  12) - s=0.0100\n",
      "  12 - L=-211.1073401 - Gamma=12.9978221 (M=  13) - s=0.0100\n",
      "  13 - L=-183.3280022 - Gamma=13.9974366 (M=  14) - s=0.0100\n",
      "  14 - L=-158.4466697 - Gamma=14.9969989 (M=  15) - s=0.0100\n",
      "  15 - L=-136.4293134 - Gamma=15.9964691 (M=  16) - s=0.0100\n",
      "  16 - L=-115.1545459 - Gamma=16.9959258 (M=  17) - s=0.0100\n",
      "  17 - L=-98.3122243 - Gamma=17.9952635 (M=  18) - s=0.0100\n",
      "  18 - L=-82.8248844 - Gamma=18.9945286 (M=  19) - s=0.0100\n",
      "  19 - L=-68.2245047 - Gamma=19.9937486 (M=  20) - s=0.0100\n",
      "  20 - L=-56.1738388 - Gamma=20.9928549 (M=  21) - s=0.0100\n",
      "  21 - L=-43.8531441 - Gamma=21.9919554 (M=  22) - s=0.0100\n",
      "  22 - L=-32.3104086 - Gamma=22.9910110 (M=  23) - s=0.0100\n",
      "  23 - L=-26.5313432 - Gamma=23.9891238 (M=  24) - s=0.0100\n",
      "  24 - L=-21.5380925 - Gamma=24.9869974 (M=  25) - s=0.0100\n",
      "  25 - L=-17.0249234 - Gamma=25.9846662 (M=  26) - s=0.0100\n",
      "  26 - L=-13.5744320 - Gamma=26.9814469 (M=  27) - s=0.0100\n",
      "  27 - L=-10.6951376 - Gamma=27.9760161 (M=  28) - s=0.0100\n",
      "  28 - L=-8.4219621 - Gamma=28.9692499 (M=  29) - s=0.0100\n",
      "  29 - L=-6.4155362 - Gamma=29.9636025 (M=  30) - s=0.0100\n",
      "  30 - L=-4.7994996 - Gamma=30.9569566 (M=  31) - s=0.0100\n",
      "  31 - L=-3.2431459 - Gamma=31.9503363 (M=  32) - s=0.0100\n",
      "  32 - L=-2.2276068 - Gamma=32.9397431 (M=  33) - s=0.0100\n",
      "  33 - L=-1.2585499 - Gamma=33.9269256 (M=  34) - s=0.0100\n",
      "  34 - L=-0.6478062 - Gamma=34.9069453 (M=  35) - s=0.0100\n",
      "  35 - L=-0.1328236 - Gamma=35.8866327 (M=  36) - s=0.0100\n",
      "  36 - L= 0.5020111 - Gamma=36.8663789 (M=  37) - s=0.0100\n",
      "  37 - L= 0.7432429 - Gamma=37.8284373 (M=  38) - s=0.0100\n",
      "  38 - L= 0.9439983 - Gamma=38.7625952 (M=  39) - s=0.0100\n",
      "  39 - L= 1.0147254 - Gamma=39.6556600 (M=  40) - s=0.0100\n",
      "  40 - L= 1.0346419 - Gamma=39.6610575 (M=  40) - s=0.0100\n",
      "  41 - L= 1.0495519 - Gamma=40.3860521 (M=  41) - s=0.0100\n",
      "  42 - L= 1.0608158 - Gamma=41.0731718 (M=  42) - s=0.0100\n",
      "  43 - L= 1.0687440 - Gamma=41.0909816 (M=  42) - s=0.0100\n",
      "  44 - L= 1.0742092 - Gamma=41.0914414 (M=  42) - s=0.0100\n",
      "  45 - L= 1.0795176 - Gamma=41.0957940 (M=  42) - s=0.0100\n",
      "  46 - L= 1.0844969 - Gamma=41.0966203 (M=  42) - s=0.0100\n",
      "  47 - L= 1.0885333 - Gamma=41.0971024 (M=  42) - s=0.0100\n",
      "  48 - L= 1.0921808 - Gamma=41.0974802 (M=  42) - s=0.0100\n",
      "  49 - L= 1.0957522 - Gamma=41.0975619 (M=  42) - s=0.0100\n",
      "  50 - L= 1.0993247 - Gamma=41.0981690 (M=  42) - s=0.0100\n",
      "  51 - L= 1.1024745 - Gamma=41.5850561 (M=  43) - s=0.0100\n",
      "  52 - L= 1.1055504 - Gamma=41.5857443 (M=  43) - s=0.0100\n",
      "  53 - L= 1.1079558 - Gamma=41.5818831 (M=  43) - s=0.0100\n",
      "  54 - L= 1.1100760 - Gamma=41.5632448 (M=  43) - s=0.0100\n",
      "  55 - L= 1.1119931 - Gamma=41.5632741 (M=  43) - s=0.0100\n",
      "  56 - L= 1.1139077 - Gamma=41.5526294 (M=  43) - s=0.0100\n",
      "  57 - L= 1.1157693 - Gamma=41.5528230 (M=  43) - s=0.0100\n",
      "  58 - L= 1.1171413 - Gamma=41.5606846 (M=  43) - s=0.0100\n",
      "  59 - L= 1.1184940 - Gamma=41.5602739 (M=  43) - s=0.0100\n",
      "  60 - L= 1.1195506 - Gamma=41.5627236 (M=  43) - s=0.0100\n",
      "  61 - L= 1.1206055 - Gamma=41.5652745 (M=  43) - s=0.0100\n",
      "  62 - L= 1.1216551 - Gamma=41.5653860 (M=  43) - s=0.0100\n",
      "  63 - L= 1.1226938 - Gamma=41.5663203 (M=  43) - s=0.0100\n",
      "  64 - L= 1.1236537 - Gamma=41.5682795 (M=  43) - s=0.0100\n",
      "  65 - L= 1.1245995 - Gamma=41.5682218 (M=  43) - s=0.0100\n",
      "  66 - L= 1.1252164 - Gamma=41.5680754 (M=  43) - s=0.0100\n",
      "  67 - L= 1.1258318 - Gamma=41.5675544 (M=  43) - s=0.0100\n",
      "  68 - L= 1.1264311 - Gamma=41.4528138 (M=  43) - s=0.0100\n",
      "  69 - L= 1.1270282 - Gamma=41.4524495 (M=  43) - s=0.0100\n",
      "  70 - L= 1.1276038 - Gamma=41.4524551 (M=  43) - s=0.0100\n",
      "  71 - L= 1.1281423 - Gamma=41.4527715 (M=  43) - s=0.0100\n",
      "  72 - L= 1.1285615 - Gamma=41.4524442 (M=  43) - s=0.0100\n",
      "  73 - L= 1.1289640 - Gamma=41.4525105 (M=  43) - s=0.0100\n",
      "  74 - L= 1.1292782 - Gamma=41.4527033 (M=  43) - s=0.0100\n",
      "  75 - L= 1.1295033 - Gamma=41.4509628 (M=  43) - s=0.0100\n",
      "  76 - L= 1.1295955 - Gamma=41.4510315 (M=  43) - s=0.0100\n",
      "  77 - L= 1.1296785 - Gamma=41.4625322 (M=  43) - s=0.0100\n",
      "  78 - L= 1.1297592 - Gamma=41.4212277 (M=  43) - s=0.0100\n",
      "  79 - L= 1.1298236 - Gamma=41.4208737 (M=  43) - s=0.0100\n",
      "  80 - L= 1.1298638 - Gamma=41.4173167 (M=  43) - s=0.0100\n",
      "  81 - L= 1.1298907 - Gamma=41.4173023 (M=  43) - s=0.0100\n",
      "  82 - L= 1.1299108 - Gamma=41.4714994 (M=  44) - s=0.0100\n",
      "  83 - L= 1.1299222 - Gamma=41.4715212 (M=  44) - s=0.0100\n",
      "  84 - L= 1.1299319 - Gamma=41.4706790 (M=  44) - s=0.0100\n",
      "  85 - L= 1.1299409 - Gamma=41.4498748 (M=  44) - s=0.0100\n",
      "  86 - L= 1.1299496 - Gamma=41.4483474 (M=  44) - s=0.0100\n",
      "  87 - L= 1.1299535 - Gamma=41.4484326 (M=  44) - s=0.0100\n",
      "  88 - L= 1.1299574 - Gamma=41.4482351 (M=  44) - s=0.0100\n",
      "  89 - L= 1.1299608 - Gamma=41.4484847 (M=  44) - s=0.0100\n",
      "  90 - L= 1.1299631 - Gamma=41.4487493 (M=  44) - s=0.0100\n",
      "  91 - L= 1.1299654 - Gamma=41.4558667 (M=  44) - s=0.0100\n",
      "  92 - L= 1.1299677 - Gamma=41.4736199 (M=  44) - s=0.0100\n",
      "  93 - L= 1.1299715 - Gamma=41.4712065 (M=  44) - s=0.0100\n",
      "  94 - L= 1.1299728 - Gamma=41.4632694 (M=  44) - s=0.0100\n",
      "  95 - L= 1.1299741 - Gamma=41.4762206 (M=  44) - s=0.0100\n",
      "  96 - L= 1.1299750 - Gamma=41.4813912 (M=  44) - s=0.0100\n",
      "  97 - L= 1.1299761 - Gamma=41.4807735 (M=  44) - s=0.0100\n",
      "  98 - L= 1.1299770 - Gamma=41.4807508 (M=  44) - s=0.0100\n",
      "  99 - L= 1.1299778 - Gamma=41.4807583 (M=  44) - s=0.0100\n",
      " 100 - L= 1.1299785 - Gamma=41.4805137 (M=  44) - s=0.0100\n",
      " 101 - L= 1.1299790 - Gamma=41.4803047 (M=  44) - s=0.0100\n",
      " 102 - L= 1.1299795 - Gamma=41.4803111 (M=  44) - s=0.0100\n",
      " 103 - L= 1.1299799 - Gamma=41.4757865 (M=  44) - s=0.0100\n",
      " 104 - L= 1.1299803 - Gamma=41.4758210 (M=  44) - s=0.0100\n",
      " 105 - L= 1.1299807 - Gamma=41.4758238 (M=  44) - s=0.0100\n",
      " 106 - L= 1.1299809 - Gamma=41.4809343 (M=  44) - s=0.0100\n",
      " 107 - L= 1.1299812 - Gamma=41.4802427 (M=  44) - s=0.0100\n",
      " 108 - L= 1.1299814 - Gamma=41.4802098 (M=  44) - s=0.0100\n",
      " 109 - L= 1.1299816 - Gamma=41.4823834 (M=  44) - s=0.0100\n",
      " 110 - L= 1.1299818 - Gamma=41.4823935 (M=  44) - s=0.0100\n",
      " 111 - L= 1.1299819 - Gamma=41.4823953 (M=  44) - s=0.0100\n",
      " 112 - L= 1.1299821 - Gamma=41.4823981 (M=  44) - s=0.0100\n",
      " 113 - L= 1.1299822 - Gamma=41.4823775 (M=  44) - s=0.0100\n",
      " 114 - L= 1.1299822 - Gamma=41.4823810 (M=  44) - s=0.0100\n",
      " 115 - L= 1.1299823 - Gamma=41.4802992 (M=  44) - s=0.0100\n",
      " 116 - L= 1.1299825 - Gamma=41.4848122 (M=  44) - s=0.0100\n",
      " 117 - L= 1.1299826 - Gamma=41.4848150 (M=  44) - s=0.0100\n",
      " 118 - L= 1.1299826 - Gamma=41.4861693 (M=  44) - s=0.0100\n",
      " 119 - L= 1.1299827 - Gamma=41.4860194 (M=  44) - s=0.0100\n",
      " 120 - L= 1.1299828 - Gamma=41.4860449 (M=  44) - s=0.0100\n",
      " 121 - L= 1.1299828 - Gamma=41.4860342 (M=  44) - s=0.0100\n",
      " 122 - L= 1.1299829 - Gamma=41.4844739 (M=  44) - s=0.0100\n",
      " 123 - L= 1.1299829 - Gamma=41.4844126 (M=  44) - s=0.0100\n",
      " 124 - L= 1.1299830 - Gamma=41.4844391 (M=  44) - s=0.0100\n",
      " 125 - L= 1.1299830 - Gamma=41.4844378 (M=  44) - s=0.0100\n",
      " 126 - L= 1.1299831 - Gamma=41.4866751 (M=  44) - s=0.0100\n",
      " 127 - L= 1.1299831 - Gamma=41.4863955 (M=  44) - s=0.0100\n",
      " 128 - L= 1.1299831 - Gamma=41.4863944 (M=  44) - s=0.0100\n",
      " 129 - L= 1.1299832 - Gamma=41.4863971 (M=  44) - s=0.0100\n",
      " 130 - L= 1.1299832 - Gamma=41.4863997 (M=  44) - s=0.0100\n",
      " 131 - L= 1.1299832 - Gamma=41.4863978 (M=  44) - s=0.0100\n",
      " 132 - L= 1.1299832 - Gamma=41.4870166 (M=  44) - s=0.0100\n",
      " 133 - L= 1.1299832 - Gamma=41.4870399 (M=  44) - s=0.0100\n",
      " 134 - L= 1.1299833 - Gamma=41.4860953 (M=  44) - s=0.0100\n",
      " 135 - L= 1.1299833 - Gamma=41.4878484 (M=  44) - s=0.0100\n",
      " 136 - L= 1.1299833 - Gamma=41.4878516 (M=  44) - s=0.0100\n",
      " 137 - L= 1.1299833 - Gamma=41.4879245 (M=  44) - s=0.0100\n",
      " 138 - L= 1.1299833 - Gamma=41.4879253 (M=  44) - s=0.0100\n",
      " 139 - L= 1.1299833 - Gamma=41.4879250 (M=  44) - s=0.0100\n",
      " 140 - L= 1.1299834 - Gamma=41.4879253 (M=  44) - s=0.0100\n",
      " 141 - L= 1.1299834 - Gamma=41.4879300 (M=  44) - s=0.0100\n",
      " 142 - L= 1.1299834 - Gamma=41.4884492 (M=  44) - s=0.0100\n",
      " 143 - L= 1.1299834 - Gamma=41.4884369 (M=  44) - s=0.0100\n",
      " 144 - L= 1.1299834 - Gamma=41.4884368 (M=  44) - s=0.0100\n",
      " 145 - L= 1.1299834 - Gamma=41.4884348 (M=  44) - s=0.0100\n",
      " 146 - L= 1.1299834 - Gamma=41.4884350 (M=  44) - s=0.0100\n",
      " 147 - L= 1.1299834 - Gamma=41.4878396 (M=  44) - s=0.0100\n",
      " 148 - L= 1.1299834 - Gamma=41.4878014 (M=  44) - s=0.0100\n",
      " 149 - L= 1.1299834 - Gamma=41.4877811 (M=  44) - s=0.0100\n",
      " 150 - L= 1.1299834 - Gamma=41.4886042 (M=  44) - s=0.0100\n",
      " 151 - L= 1.1299834 - Gamma=41.4884996 (M=  44) - s=0.0100\n",
      " 152 - L= 1.1299834 - Gamma=41.4884996 (M=  44) - s=0.0100\n",
      "Stopping at iteration 152 - max_delta_ml=2.2794744276047286e-07\n",
      "L=1.1299834379998612 - Gamma=41.488499578180715 (M=44) - s=0.01\n",
      "Initial alpha = [[0.1189873]]\n",
      "   1 - L=-1014.2138648 - Gamma= 1.9999386 (M=   2) - s=0.0100\n",
      "   2 - L=-910.3465973 - Gamma= 2.9998359 (M=   3) - s=0.0100\n",
      "   3 - L=-812.9344269 - Gamma= 3.9997267 (M=   4) - s=0.0100\n",
      "   4 - L=-730.6265106 - Gamma= 4.9995943 (M=   5) - s=0.0100\n",
      "   5 - L=-650.2556585 - Gamma= 5.9994587 (M=   6) - s=0.0100\n",
      "   6 - L=-592.6508804 - Gamma= 6.9992717 (M=   7) - s=0.0100\n",
      "   7 - L=-537.0439844 - Gamma= 7.9990662 (M=   8) - s=0.0100\n",
      "   8 - L=-483.0592866 - Gamma= 8.9988572 (M=   9) - s=0.0100\n",
      "   9 - L=-426.2640997 - Gamma= 9.9986675 (M=  10) - s=0.0100\n",
      "  10 - L=-386.9141033 - Gamma=10.9983837 (M=  11) - s=0.0100\n",
      "  11 - L=-350.2368193 - Gamma=11.9980879 (M=  12) - s=0.0100\n",
      "  12 - L=-318.6301403 - Gamma=12.9977264 (M=  13) - s=0.0100\n",
      "  13 - L=-290.9022392 - Gamma=13.9973395 (M=  14) - s=0.0100\n",
      "  14 - L=-260.5324590 - Gamma=14.9969457 (M=  15) - s=0.0100\n",
      "  15 - L=-230.9267390 - Gamma=15.9964526 (M=  16) - s=0.0100\n",
      "  16 - L=-208.8726991 - Gamma=16.9959328 (M=  17) - s=0.0100\n",
      "  17 - L=-187.3201290 - Gamma=17.9954348 (M=  18) - s=0.0100\n",
      "  18 - L=-166.6303992 - Gamma=18.9948787 (M=  19) - s=0.0100\n",
      "  19 - L=-145.0675219 - Gamma=19.9943709 (M=  20) - s=0.0100\n",
      "  20 - L=-126.1313895 - Gamma=20.9937929 (M=  21) - s=0.0100\n",
      "  21 - L=-107.8510250 - Gamma=21.9930886 (M=  22) - s=0.0100\n",
      "  22 - L=-92.2306890 - Gamma=22.9921708 (M=  23) - s=0.0100\n",
      "  23 - L=-80.2623091 - Gamma=23.9912575 (M=  24) - s=0.0100\n",
      "  24 - L=-68.8139383 - Gamma=24.9901508 (M=  25) - s=0.0100\n",
      "  25 - L=-57.0730905 - Gamma=25.9891993 (M=  26) - s=0.0100\n",
      "  26 - L=-48.9252568 - Gamma=26.9873649 (M=  27) - s=0.0100\n",
      "  27 - L=-38.6602345 - Gamma=27.9858654 (M=  28) - s=0.0100\n",
      "  28 - L=-30.9771834 - Gamma=28.9843484 (M=  29) - s=0.0100\n",
      "  29 - L=-25.8886607 - Gamma=29.9819546 (M=  30) - s=0.0100\n",
      "  30 - L=-20.3839782 - Gamma=30.9786980 (M=  31) - s=0.0100\n",
      "  31 - L=-14.7468129 - Gamma=31.9761064 (M=  32) - s=0.0100\n",
      "  32 - L=-9.2919774 - Gamma=32.9738531 (M=  33) - s=0.0100\n",
      "  33 - L=-6.3143143 - Gamma=33.9698686 (M=  34) - s=0.0100\n",
      "  34 - L=-4.7427898 - Gamma=34.9616501 (M=  35) - s=0.0100\n",
      "  35 - L=-3.6756671 - Gamma=35.9515685 (M=  36) - s=0.0100\n",
      "  36 - L=-2.4725007 - Gamma=36.9421529 (M=  37) - s=0.0100\n",
      "  37 - L=-1.1618894 - Gamma=37.9329319 (M=  38) - s=0.0100\n",
      "  38 - L=-0.3771973 - Gamma=38.9198963 (M=  39) - s=0.0100\n",
      "  39 - L= 0.0074724 - Gamma=39.8923146 (M=  40) - s=0.0100\n",
      "  40 - L= 0.0841335 - Gamma=39.8934389 (M=  40) - s=0.0100\n",
      "  41 - L= 0.1599783 - Gamma=39.8942426 (M=  40) - s=0.0100\n",
      "  42 - L= 0.2204020 - Gamma=40.7749711 (M=  41) - s=0.0100\n",
      "  43 - L= 0.2641924 - Gamma=40.7752547 (M=  41) - s=0.0100\n",
      "  44 - L= 0.3014150 - Gamma=40.7774595 (M=  41) - s=0.0100\n",
      "  45 - L= 0.3371917 - Gamma=40.7779735 (M=  41) - s=0.0100\n",
      "  46 - L= 0.3685650 - Gamma=41.5887200 (M=  42) - s=0.0100\n",
      "  47 - L= 0.3937142 - Gamma=41.5904609 (M=  42) - s=0.0100\n",
      "  48 - L= 0.4183044 - Gamma=41.5917542 (M=  42) - s=0.0100\n",
      "  49 - L= 0.4416670 - Gamma=41.5925955 (M=  42) - s=0.0100\n",
      "  50 - L= 0.4643557 - Gamma=41.5928432 (M=  42) - s=0.0100\n",
      "  51 - L= 0.4832150 - Gamma=41.5936896 (M=  42) - s=0.0100\n",
      "  52 - L= 0.5016547 - Gamma=41.5940589 (M=  42) - s=0.0100\n",
      "  53 - L= 0.5193240 - Gamma=41.5942784 (M=  42) - s=0.0100\n",
      "  54 - L= 0.5362136 - Gamma=41.5972280 (M=  42) - s=0.0100\n",
      "  55 - L= 0.5511823 - Gamma=41.5980258 (M=  42) - s=0.0100\n",
      "  56 - L= 0.5627506 - Gamma=41.5989888 (M=  42) - s=0.0100\n",
      "  57 - L= 0.5734349 - Gamma=42.2621632 (M=  43) - s=0.0100\n",
      "  58 - L= 0.5831578 - Gamma=42.2628299 (M=  43) - s=0.0100\n",
      "  59 - L= 0.5926256 - Gamma=42.2642588 (M=  43) - s=0.0100\n",
      "  60 - L= 0.6020598 - Gamma=42.2646038 (M=  43) - s=0.0100\n",
      "  61 - L= 0.6108018 - Gamma=42.2639528 (M=  43) - s=0.0100\n",
      "  62 - L= 0.6185960 - Gamma=42.2641884 (M=  43) - s=0.0100\n",
      "  63 - L= 0.6252896 - Gamma=42.2649106 (M=  43) - s=0.0100\n",
      "  64 - L= 0.6305427 - Gamma=42.2673006 (M=  43) - s=0.0100\n",
      "  65 - L= 0.6351439 - Gamma=42.2664926 (M=  43) - s=0.0100\n",
      "  66 - L= 0.6388435 - Gamma=42.2658344 (M=  43) - s=0.0100\n",
      "  67 - L= 0.6424518 - Gamma=42.2661061 (M=  43) - s=0.0100\n",
      "  68 - L= 0.6447156 - Gamma=42.2664619 (M=  43) - s=0.0100\n",
      "  69 - L= 0.6465745 - Gamma=42.2676110 (M=  43) - s=0.0100\n",
      "  70 - L= 0.6484070 - Gamma=42.2678641 (M=  43) - s=0.0100\n",
      "  71 - L= 0.6502371 - Gamma=42.2686108 (M=  43) - s=0.0100\n",
      "  72 - L= 0.6517176 - Gamma=42.2695194 (M=  43) - s=0.0100\n",
      "  73 - L= 0.6526530 - Gamma=42.2731442 (M=  43) - s=0.0100\n",
      "  74 - L= 0.6535736 - Gamma=42.2729792 (M=  43) - s=0.0100\n",
      "  75 - L= 0.6540903 - Gamma=42.5273186 (M=  44) - s=0.0100\n",
      "  76 - L= 0.6546176 - Gamma=42.5166311 (M=  44) - s=0.0100\n",
      "  77 - L= 0.6548631 - Gamma=42.5387837 (M=  44) - s=0.0100\n",
      "  78 - L= 0.6550236 - Gamma=42.5394064 (M=  44) - s=0.0100\n",
      "  79 - L= 0.6551762 - Gamma=42.5394115 (M=  44) - s=0.0100\n",
      "  80 - L= 0.6552929 - Gamma=42.4887351 (M=  44) - s=0.0100\n",
      "  81 - L= 0.6553388 - Gamma=42.4874749 (M=  44) - s=0.0100\n",
      "  82 - L= 0.6553796 - Gamma=42.4865996 (M=  44) - s=0.0100\n",
      "  83 - L= 0.6553982 - Gamma=42.4967896 (M=  44) - s=0.0100\n",
      "  84 - L= 0.6554087 - Gamma=42.4967781 (M=  44) - s=0.0100\n",
      "  85 - L= 0.6554188 - Gamma=42.4967555 (M=  44) - s=0.0100\n",
      "  86 - L= 0.6554285 - Gamma=42.4963415 (M=  44) - s=0.0100\n",
      "  87 - L= 0.6554380 - Gamma=42.4963345 (M=  44) - s=0.0100\n",
      "  88 - L= 0.6554450 - Gamma=42.4962925 (M=  44) - s=0.0100\n",
      "  89 - L= 0.6554512 - Gamma=42.5209807 (M=  44) - s=0.0100\n",
      "  90 - L= 0.6554547 - Gamma=42.5209682 (M=  44) - s=0.0100\n",
      "  91 - L= 0.6554574 - Gamma=42.5209660 (M=  44) - s=0.0100\n",
      "  92 - L= 0.6554595 - Gamma=42.5209636 (M=  44) - s=0.0100\n",
      "  93 - L= 0.6554614 - Gamma=42.5209574 (M=  44) - s=0.0100\n",
      "  94 - L= 0.6554628 - Gamma=42.5210756 (M=  44) - s=0.0100\n",
      "  95 - L= 0.6554641 - Gamma=42.5212258 (M=  44) - s=0.0100\n",
      "  96 - L= 0.6554646 - Gamma=42.5246610 (M=  44) - s=0.0100\n",
      "  97 - L= 0.6554651 - Gamma=42.5246491 (M=  44) - s=0.0100\n",
      "  98 - L= 0.6554655 - Gamma=42.5246365 (M=  44) - s=0.0100\n",
      "  99 - L= 0.6554658 - Gamma=42.5246501 (M=  44) - s=0.0100\n",
      " 100 - L= 0.6554661 - Gamma=42.5246482 (M=  44) - s=0.0100\n",
      " 101 - L= 0.6554663 - Gamma=42.5246371 (M=  44) - s=0.0100\n",
      " 102 - L= 0.6554665 - Gamma=42.5252313 (M=  44) - s=0.0100\n",
      " 103 - L= 0.6554667 - Gamma=42.5252354 (M=  44) - s=0.0100\n",
      " 104 - L= 0.6554668 - Gamma=42.5252352 (M=  44) - s=0.0100\n",
      " 105 - L= 0.6554669 - Gamma=42.5252370 (M=  44) - s=0.0100\n",
      " 106 - L= 0.6554670 - Gamma=42.5252412 (M=  44) - s=0.0100\n",
      " 107 - L= 0.6554670 - Gamma=42.5252400 (M=  44) - s=0.0100\n",
      " 108 - L= 0.6554671 - Gamma=42.5258348 (M=  44) - s=0.0100\n",
      " 109 - L= 0.6554672 - Gamma=42.5258344 (M=  44) - s=0.0100\n",
      " 110 - L= 0.6554672 - Gamma=42.5257122 (M=  44) - s=0.0100\n",
      " 111 - L= 0.6554673 - Gamma=42.5257101 (M=  44) - s=0.0100\n",
      " 112 - L= 0.6554673 - Gamma=42.5257087 (M=  44) - s=0.0100\n",
      " 113 - L= 0.6554674 - Gamma=42.5257100 (M=  44) - s=0.0100\n",
      " 114 - L= 0.6554674 - Gamma=42.5257088 (M=  44) - s=0.0100\n",
      " 115 - L= 0.6554674 - Gamma=42.5257090 (M=  44) - s=0.0100\n",
      " 116 - L= 0.6554675 - Gamma=42.5257096 (M=  44) - s=0.0100\n",
      " 117 - L= 0.6554675 - Gamma=42.5257087 (M=  44) - s=0.0100\n",
      " 118 - L= 0.6554675 - Gamma=42.5257330 (M=  44) - s=0.0100\n",
      " 119 - L= 0.6554676 - Gamma=42.5257327 (M=  44) - s=0.0100\n",
      " 120 - L= 0.6554676 - Gamma=42.5257329 (M=  44) - s=0.0100\n",
      " 121 - L= 0.6554676 - Gamma=42.5257325 (M=  44) - s=0.0100\n",
      " 122 - L= 0.6554676 - Gamma=42.5257322 (M=  44) - s=0.0100\n",
      " 123 - L= 0.6554676 - Gamma=42.5264143 (M=  44) - s=0.0100\n",
      " 124 - L= 0.6554676 - Gamma=42.5264143 (M=  44) - s=0.0100\n",
      "Stopping at iteration 124 - max_delta_ml=1.7602790737644876e-07\n",
      "L=0.6554676111593011 - Gamma=42.526414317089916 (M=44) - s=0.01\n",
      "MODEL: RVM accuracy:  0.41379310344827586 +/-: 0.023002017799877494\n",
      "Initial alpha = [[0.09600941]]\n",
      "   1 - L=-1051.6008229 - Gamma= 1.9999476 (M=   2) - s=0.0100\n",
      "   2 - L=-949.5465053 - Gamma= 2.9998632 (M=   3) - s=0.0100\n",
      "   3 - L=-849.1668982 - Gamma= 3.9997774 (M=   4) - s=0.0100\n",
      "   4 - L=-767.7766230 - Gamma= 4.9996715 (M=   5) - s=0.0100\n",
      "   5 - L=-703.2166119 - Gamma= 5.9995381 (M=   6) - s=0.0100\n",
      "   6 - L=-643.4802538 - Gamma= 6.9993940 (M=   7) - s=0.0100\n",
      "   7 - L=-585.5971820 - Gamma= 7.9992452 (M=   8) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 - L=-530.9726501 - Gamma= 8.9990876 (M=   9) - s=0.0100\n",
      "   9 - L=-486.0386069 - Gamma= 9.9988961 (M=  10) - s=0.0100\n",
      "  10 - L=-445.6917592 - Gamma=10.9986829 (M=  11) - s=0.0100\n",
      "  11 - L=-412.1017408 - Gamma=11.9984269 (M=  12) - s=0.0100\n",
      "  12 - L=-379.2561724 - Gamma=12.9981650 (M=  13) - s=0.0100\n",
      "  13 - L=-347.0245103 - Gamma=13.9978982 (M=  14) - s=0.0100\n",
      "  14 - L=-323.6087969 - Gamma=14.9975312 (M=  15) - s=0.0100\n",
      "  15 - L=-301.5164215 - Gamma=15.9971424 (M=  16) - s=0.0100\n",
      "  16 - L=-282.5615964 - Gamma=16.9966893 (M=  17) - s=0.0100\n",
      "  17 - L=-266.6649005 - Gamma=17.9961495 (M=  18) - s=0.0100\n",
      "  18 - L=-251.2366213 - Gamma=18.9955934 (M=  19) - s=0.0100\n",
      "  19 - L=-235.8983170 - Gamma=19.9950340 (M=  20) - s=0.0100\n",
      "  20 - L=-221.2205831 - Gamma=20.9944496 (M=  21) - s=0.0100\n",
      "  21 - L=-206.6817200 - Gamma=21.9938596 (M=  22) - s=0.0100\n",
      "  22 - L=-192.1887789 - Gamma=22.9932677 (M=  23) - s=0.0100\n",
      "  23 - L=-179.0214344 - Gamma=23.9926166 (M=  24) - s=0.0100\n",
      "  24 - L=-167.1872191 - Gamma=24.9918924 (M=  25) - s=0.0100\n",
      "  25 - L=-155.6062313 - Gamma=25.9911526 (M=  26) - s=0.0100\n",
      "  26 - L=-144.3014260 - Gamma=26.9903947 (M=  27) - s=0.0100\n",
      "  27 - L=-133.4048082 - Gamma=27.9896086 (M=  28) - s=0.0100\n",
      "  28 - L=-123.2643723 - Gamma=28.9887643 (M=  29) - s=0.0100\n",
      "  29 - L=-113.1653208 - Gamma=29.9879165 (M=  30) - s=0.0100\n",
      "  30 - L=-103.3592440 - Gamma=30.9870435 (M=  31) - s=0.0100\n",
      "  31 - L=-93.6459855 - Gamma=31.9861622 (M=  32) - s=0.0100\n",
      "  32 - L=-84.1460606 - Gamma=32.9852613 (M=  33) - s=0.0100\n",
      "  33 - L=-74.9199581 - Gamma=33.9843338 (M=  34) - s=0.0100\n",
      "  34 - L=-66.6950949 - Gamma=34.9832942 (M=  35) - s=0.0100\n",
      "  35 - L=-58.5070956 - Gamma=35.9822500 (M=  36) - s=0.0100\n",
      "  36 - L=-50.3304659 - Gamma=36.9812043 (M=  37) - s=0.0100\n",
      "  37 - L=-42.5834823 - Gamma=37.9801011 (M=  38) - s=0.0100\n",
      "  38 - L=-35.4315944 - Gamma=38.9789068 (M=  39) - s=0.0100\n",
      "  39 - L=-29.4428602 - Gamma=39.9774812 (M=  40) - s=0.0100\n",
      "  40 - L=-23.6818421 - Gamma=40.9760011 (M=  41) - s=0.0100\n",
      "  41 - L=-18.2004788 - Gamma=41.9744466 (M=  42) - s=0.0100\n",
      "  42 - L=-13.4817654 - Gamma=42.9726435 (M=  43) - s=0.0100\n",
      "  43 - L=-9.2574358 - Gamma=43.9706322 (M=  44) - s=0.0100\n",
      "  44 - L=-7.5322959 - Gamma=44.9657883 (M=  45) - s=0.0100\n",
      "  45 - L=-6.1556338 - Gamma=45.9597571 (M=  46) - s=0.0100\n",
      "  46 - L=-4.8618495 - Gamma=46.9533521 (M=  47) - s=0.0100\n",
      "  47 - L=-3.8617487 - Gamma=47.9451429 (M=  48) - s=0.0100\n",
      "  48 - L=-2.8719169 - Gamma=48.9368519 (M=  49) - s=0.0100\n",
      "  49 - L=-2.0120452 - Gamma=49.9273644 (M=  50) - s=0.0100\n",
      "  50 - L=-1.1810156 - Gamma=50.9175603 (M=  51) - s=0.0100\n",
      "  51 - L=-0.4091422 - Gamma=51.9070442 (M=  52) - s=0.0100\n",
      "  52 - L= 0.0284996 - Gamma=52.8891187 (M=  53) - s=0.0100\n",
      "  53 - L= 0.3340526 - Gamma=53.8642037 (M=  54) - s=0.0100\n",
      "  54 - L= 0.5507668 - Gamma=54.8303327 (M=  55) - s=0.0100\n",
      "  55 - L= 0.7181121 - Gamma=55.7879095 (M=  56) - s=0.0100\n",
      "  56 - L= 0.7191822 - Gamma=56.1566879 (M=  57) - s=0.0100\n",
      "  57 - L= 0.7192994 - Gamma=56.1566895 (M=  57) - s=0.0100\n",
      "  58 - L= 0.7193081 - Gamma=56.1566954 (M=  57) - s=0.0100\n",
      "  59 - L= 0.7193115 - Gamma=56.1566655 (M=  57) - s=0.0100\n",
      "  60 - L= 0.7193148 - Gamma=56.1567039 (M=  57) - s=0.0100\n",
      "  61 - L= 0.7193167 - Gamma=56.1566852 (M=  57) - s=0.0100\n",
      "  62 - L= 0.7193183 - Gamma=56.1566704 (M=  57) - s=0.0100\n",
      "  63 - L= 0.7193198 - Gamma=56.1566688 (M=  57) - s=0.0100\n",
      "  64 - L= 0.7193210 - Gamma=56.1566642 (M=  57) - s=0.0100\n",
      "  65 - L= 0.7193221 - Gamma=56.1566473 (M=  57) - s=0.0100\n",
      "  66 - L= 0.7193230 - Gamma=56.1566641 (M=  57) - s=0.0100\n",
      "  67 - L= 0.7193238 - Gamma=56.1566430 (M=  57) - s=0.0100\n",
      "  68 - L= 0.7193247 - Gamma=56.1566444 (M=  57) - s=0.0100\n",
      "  69 - L= 0.7193255 - Gamma=56.1566497 (M=  57) - s=0.0100\n",
      "  70 - L= 0.7193263 - Gamma=56.1565202 (M=  57) - s=0.0100\n",
      "  71 - L= 0.7193270 - Gamma=56.1565222 (M=  57) - s=0.0100\n",
      "  72 - L= 0.7193278 - Gamma=56.1566513 (M=  57) - s=0.0100\n",
      "  73 - L= 0.7193285 - Gamma=56.1566404 (M=  57) - s=0.0100\n",
      "  74 - L= 0.7193291 - Gamma=56.1566515 (M=  57) - s=0.0100\n",
      "  75 - L= 0.7193298 - Gamma=56.1566532 (M=  57) - s=0.0100\n",
      "  76 - L= 0.7193304 - Gamma=56.1566639 (M=  57) - s=0.0100\n",
      "  77 - L= 0.7193310 - Gamma=56.1566703 (M=  57) - s=0.0100\n",
      "  78 - L= 0.7193315 - Gamma=56.1566643 (M=  57) - s=0.0100\n",
      "  79 - L= 0.7193319 - Gamma=56.1566813 (M=  57) - s=0.0100\n",
      "  80 - L= 0.7193322 - Gamma=56.1566870 (M=  57) - s=0.0100\n",
      "  81 - L= 0.7193325 - Gamma=56.1566779 (M=  57) - s=0.0100\n",
      "  82 - L= 0.7193327 - Gamma=56.1566789 (M=  57) - s=0.0100\n",
      "  83 - L= 0.7193329 - Gamma=56.1566772 (M=  57) - s=0.0100\n",
      "  84 - L= 0.7193330 - Gamma=56.1566806 (M=  57) - s=0.0100\n",
      "  85 - L= 0.7193332 - Gamma=56.1566826 (M=  57) - s=0.0100\n",
      "  86 - L= 0.7193333 - Gamma=56.1566875 (M=  57) - s=0.0100\n",
      "  87 - L= 0.7193334 - Gamma=56.1566912 (M=  57) - s=0.0100\n",
      "  88 - L= 0.7193335 - Gamma=56.1566953 (M=  57) - s=0.0100\n",
      "  89 - L= 0.7193336 - Gamma=56.1567253 (M=  57) - s=0.0100\n",
      "  90 - L= 0.7193337 - Gamma=56.1567263 (M=  57) - s=0.0100\n",
      "  91 - L= 0.7193338 - Gamma=56.1567289 (M=  57) - s=0.0100\n",
      "  92 - L= 0.7193339 - Gamma=56.1567297 (M=  57) - s=0.0100\n",
      "  93 - L= 0.7193339 - Gamma=56.1567112 (M=  57) - s=0.0100\n",
      "  94 - L= 0.7193340 - Gamma=56.1567131 (M=  57) - s=0.0100\n",
      "  95 - L= 0.7193340 - Gamma=56.1567701 (M=  57) - s=0.0100\n",
      "  96 - L= 0.7193340 - Gamma=56.1567763 (M=  57) - s=0.0100\n",
      "  97 - L= 0.7193341 - Gamma=56.1567778 (M=  57) - s=0.0100\n",
      "  98 - L= 0.7193341 - Gamma=56.1567758 (M=  57) - s=0.0100\n",
      "  99 - L= 0.7193341 - Gamma=56.1567756 (M=  57) - s=0.0100\n",
      " 100 - L= 0.7193342 - Gamma=56.1567628 (M=  57) - s=0.0100\n",
      " 101 - L= 0.7193342 - Gamma=56.1567629 (M=  57) - s=0.0100\n",
      " 102 - L= 0.7193342 - Gamma=56.1567602 (M=  57) - s=0.0100\n",
      " 103 - L= 0.7193342 - Gamma=56.1567606 (M=  57) - s=0.0100\n",
      " 104 - L= 0.7193342 - Gamma=56.1567741 (M=  57) - s=0.0100\n",
      " 105 - L= 0.7193342 - Gamma=56.1568196 (M=  57) - s=0.0100\n",
      " 106 - L= 0.7193342 - Gamma=56.1568070 (M=  57) - s=0.0100\n",
      " 107 - L= 0.7193342 - Gamma=56.1568070 (M=  57) - s=0.0100\n",
      "Stopping at iteration 107 - max_delta_ml=1.4947142115993633e-07\n",
      "L=0.7193342259807076 - Gamma=56.15680702511456 (M=57) - s=0.01\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7050 - acc: 0.4565 - val_loss: 0.6875 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6839 - acc: 0.5435 - val_loss: 0.6867 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6684 - acc: 0.6087 - val_loss: 0.6716 - val_acc: 0.6667\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6118 - acc: 0.8043 - val_loss: 0.6341 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.5057 - acc: 0.8478 - val_loss: 0.5849 - val_acc: 0.9167\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.4318 - acc: 0.8913 - val_loss: 0.4525 - val_acc: 0.9167\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.3236 - acc: 0.9565 - val_loss: 0.3690 - val_acc: 0.9167\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.1886 - acc: 0.9783 - val_loss: 0.2653 - val_acc: 0.9167\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.0768 - acc: 0.9783 - val_loss: 0.2711 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.0163 - acc: 1.0000 - val_loss: 0.2844 - val_acc: 0.9167\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.0792 - acc: 0.9783 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 6.2476e-04 - val_acc: 1.0000\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 3.8433e-04 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 3.0285e-04 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 9.1604e-04 - acc: 1.0000 - val_loss: 2.4998e-04 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 5.2220e-04 - acc: 1.0000 - val_loss: 1.7869e-04 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 1.4452e-04 - acc: 1.0000 - val_loss: 1.5350e-04 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 2.5910e-04 - acc: 1.0000 - val_loss: 1.1325e-04 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 1.6529e-04 - acc: 1.0000 - val_loss: 8.4722e-05 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 1.5680e-04 - acc: 1.0000 - val_loss: 4.9995e-05 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 1.0998e-04 - acc: 1.0000 - val_loss: 4.1240e-05 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 8.4236e-05 - acc: 1.0000 - val_loss: 3.0095e-05 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 1.4749e-05 - acc: 1.0000 - val_loss: 2.6221e-05 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - ETA: 0s - loss: 6.6012e-05 - acc: 1.000 - 0s - loss: 3.1433e-05 - acc: 1.0000 - val_loss: 2.1074e-05 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  0.9655172413793104 +/-: 0.0016845025762980597\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality using PCA\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 58, 1) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7399 - acc: 0.5870 - val_loss: 0.6951 - val_acc: 0.4167\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6660 - acc: 0.5870 - val_loss: 0.6864 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6298 - acc: 0.6522 - val_loss: 0.7085 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.5416 - acc: 0.6957 - val_loss: 0.6430 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.4960 - acc: 0.7826 - val_loss: 0.6274 - val_acc: 0.6667\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.5211 - acc: 0.7609 - val_loss: 0.5206 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.5438 - acc: 0.7609 - val_loss: 0.5107 - val_acc: 0.6667\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.4179 - acc: 0.8696 - val_loss: 0.4579 - val_acc: 0.9167\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.3908 - acc: 0.8043 - val_loss: 0.4609 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.3798 - acc: 0.9130 - val_loss: 0.4239 - val_acc: 0.8333\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.3264 - acc: 0.8696 - val_loss: 0.2439 - val_acc: 0.9167\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.2521 - acc: 0.9565 - val_loss: 0.2338 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.2005 - acc: 1.0000 - val_loss: 0.2848 - val_acc: 0.9167\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.1814 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.1489 - acc: 0.9348 - val_loss: 0.2519 - val_acc: 0.9167\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.2517 - acc: 0.9362 - val_loss: 0.2160 - val_acc: 0.9091\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1926 - acc: 0.9149 - val_loss: 0.1449 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1257 - acc: 1.0000 - val_loss: 0.0693 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.0461 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.0534 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.0488 - acc: 1.0000 - val_loss: 0.0723 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.0331 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.0713 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.0486 - acc: 1.0000 - val_loss: 0.1250 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 0.0449 - val_acc: 1.0000\n",
      "MODEL: CNN accuracy:  0.8793103448275862 +/-: 0.015606420927467304\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "Rocket.X_GENOME = None\n",
    "Rocket.Y_CLASS = None\n",
    "Rocket.PREP_HASH = None\n",
    "\n",
    "RUNS, MODELS, ACC = Rocket.run_classification(method_list = METHOD_LIST, \n",
    "                          num_run = nruns,\n",
    "                          pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                      \"pre_processing\": PREPROC_DICT,\n",
    "                                      \"feature_selection\": FSELECT_DICT, # mannwhitney         \n",
    "                                      \"dim_reduction\": DIMRED_DICT},\n",
    "                          parameters = {}, \n",
    "                          features = 'genomic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on average: 0.6293103448275862 +- 0.017141390119987034, median: 0.5689655172413794+-0.017507116347782223\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "        acc model       var\n",
      "0  0.534483    RF  0.002468\n",
      "1  0.500000   XGB  0.017372\n",
      "2  0.534483  LGBM  0.032249\n",
      "3  0.551724    ET  0.004117\n",
      "4  0.724138   SVM  0.024997\n",
      "5  0.603448    LR  0.017642\n",
      "6  0.586207  MLNN  0.032276\n",
      "7  0.413793   RVM  0.023002\n",
      "8  0.965517   DNN  0.001685\n",
      "9  0.879310   CNN  0.015606\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on average: {} +- {}, median: {}+-{}\".format(ACC.mean()[0], ACC.mean()[1], ACC.median()[0], ACC.median()[1]))\n",
    "print(\"+\"*40)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "###########\n",
    "##Runs.append(AllResults)\n",
    "final_df = pandas.DataFrame()\n",
    "\n",
    "for idx, df in enumerate(RUNS):    \n",
    "    df['run'] = idx\n",
    "    final_df = final_df.append(df, ignore_index = True)\n",
    "final_df[Rocket.MODEL_PARAMETERS['ID']] = final_df[Rocket.MODEL_PARAMETERS['ID']].astype(str)\n",
    "final_df = final_df.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "final_df['pred']= pandas.to_numeric(final_df['pred'])\n",
    "final_df_agg = final_df.groupby([Rocket.MODEL_PARAMETERS['ID'], 'method']).agg({'pred': [numpy.mean, numpy.median, numpy.std]})\n",
    "final_df_agg = final_df_agg['pred'].groupby(by=Rocket.MODEL_PARAMETERS['ID']).agg({'mean': [numpy.mean, numpy.median, numpy.std]})['mean']\n",
    "final_df.to_csv(\"out/patient_results_\"+Rocket.SET_NAME+\"_FDR0025.csv\")\n",
    "final_df_agg.to_csv(\"out/patient_results_agg_\"+Rocket.SET_NAME+\"_FDR0025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_weights, top_coeffs = _helpers.get_top_genes(MODELS=MODELS, n_max=5000, RexR=Rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_RF</th>\n",
       "      <th>1_XGB</th>\n",
       "      <th>2_LGBM</th>\n",
       "      <th>3_ExtraTrees</th>\n",
       "      <th>9_RF</th>\n",
       "      <th>10_XGB</th>\n",
       "      <th>11_LGBM</th>\n",
       "      <th>12_ExtraTrees</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>MEDIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1569566_at</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.705969</td>\n",
       "      <td>0.646447</td>\n",
       "      <td>0.705969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223748_at</th>\n",
       "      <td>0.239810</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.213193</td>\n",
       "      <td>0.354560</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292892</td>\n",
       "      <td>0.461962</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233185_at</th>\n",
       "      <td>0.289019</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.390557</td>\n",
       "      <td>0.466654</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.365009</td>\n",
       "      <td>0.422040</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209661_at</th>\n",
       "      <td>0.144378</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.394731</td>\n",
       "      <td>0.219756</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.796495</td>\n",
       "      <td>0.360095</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223017_at</th>\n",
       "      <td>0.411182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.162605</td>\n",
       "      <td>0.324446</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.390542</td>\n",
       "      <td>0.325449</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209626_s_at</th>\n",
       "      <td>0.146771</td>\n",
       "      <td>0.476191</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372427</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.212822</td>\n",
       "      <td>0.315378</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552726_at</th>\n",
       "      <td>0.114999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312695</td>\n",
       "      <td>0.307323</td>\n",
       "      <td>0.114999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201015_s_at</th>\n",
       "      <td>0.665232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718868</td>\n",
       "      <td>0.664016</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.194040</td>\n",
       "      <td>0.305732</td>\n",
       "      <td>0.194040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228667_at</th>\n",
       "      <td>0.777687</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.204861</td>\n",
       "      <td>0.294335</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.145589</td>\n",
       "      <td>0.303470</td>\n",
       "      <td>0.294335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563473_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.296040</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.585204</td>\n",
       "      <td>0.303040</td>\n",
       "      <td>0.296040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221933_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.419465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.432588</td>\n",
       "      <td>0.294868</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554689_a_at</th>\n",
       "      <td>0.183221</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.274078</td>\n",
       "      <td>0.273133</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244472</td>\n",
       "      <td>0.274971</td>\n",
       "      <td>0.273133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45297_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.361025</td>\n",
       "      <td>0.204941</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.528291</td>\n",
       "      <td>0.257153</td>\n",
       "      <td>0.257153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225791_at</th>\n",
       "      <td>0.682172</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.246589</td>\n",
       "      <td>0.119646</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315221</td>\n",
       "      <td>0.243536</td>\n",
       "      <td>0.243536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239886_at</th>\n",
       "      <td>0.365663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.618255</td>\n",
       "      <td>0.227033</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.277684</td>\n",
       "      <td>0.237005</td>\n",
       "      <td>0.227033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229380_at</th>\n",
       "      <td>0.407494</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218567</td>\n",
       "      <td>0.137130</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.369293</td>\n",
       "      <td>0.224233</td>\n",
       "      <td>0.218567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227134_at</th>\n",
       "      <td>0.296832</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.340159</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.263165</td>\n",
       "      <td>0.220182</td>\n",
       "      <td>0.263165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552742_at</th>\n",
       "      <td>0.298797</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099290</td>\n",
       "      <td>0.217299</td>\n",
       "      <td>0.099290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224252_s_at</th>\n",
       "      <td>0.232003</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339085</td>\n",
       "      <td>0.432266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564064</td>\n",
       "      <td>0.216761</td>\n",
       "      <td>0.216761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230689_at</th>\n",
       "      <td>0.185628</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200150</td>\n",
       "      <td>0.177877</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144692</td>\n",
       "      <td>0.203953</td>\n",
       "      <td>0.185628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225611_at</th>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146048</td>\n",
       "      <td>0.196601</td>\n",
       "      <td>0.119048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218638_s_at</th>\n",
       "      <td>0.503735</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>0.174359</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776229</td>\n",
       "      <td>0.195798</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215784_at</th>\n",
       "      <td>0.109584</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148497</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193783</td>\n",
       "      <td>0.109584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230932_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512280</td>\n",
       "      <td>0.465298</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374511</td>\n",
       "      <td>0.191498</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556945_a_at</th>\n",
       "      <td>0.032344</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.330533</td>\n",
       "      <td>0.175226</td>\n",
       "      <td>0.175226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206907_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.363048</td>\n",
       "      <td>0.200825</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.287776</td>\n",
       "      <td>0.174578</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202552_s_at</th>\n",
       "      <td>0.289373</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110134</td>\n",
       "      <td>0.333203</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377373</td>\n",
       "      <td>0.172821</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238622_at</th>\n",
       "      <td>0.172830</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537788</td>\n",
       "      <td>0.281261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.169658</td>\n",
       "      <td>0.169658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203522_at</th>\n",
       "      <td>0.248877</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.686273</td>\n",
       "      <td>0.029284</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081920</td>\n",
       "      <td>0.168493</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207375_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.295477</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.161665</td>\n",
       "      <td>0.157124</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225613_at</th>\n",
       "      <td>0.066714</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249273</td>\n",
       "      <td>0.035447</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356734</td>\n",
       "      <td>0.096788</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203325_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.010448</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024655</td>\n",
       "      <td>0.091998</td>\n",
       "      <td>0.024655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237403_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483750</td>\n",
       "      <td>0.086558</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206752_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147126</td>\n",
       "      <td>0.056069</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205714</td>\n",
       "      <td>0.084844</td>\n",
       "      <td>0.056069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213093_at</th>\n",
       "      <td>0.013223</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216555</td>\n",
       "      <td>0.083138</td>\n",
       "      <td>0.030562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215933_s_at</th>\n",
       "      <td>0.254518</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.081603</td>\n",
       "      <td>0.012974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228097_at</th>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341788</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>0.002159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218337_at</th>\n",
       "      <td>0.132429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.074868</td>\n",
       "      <td>0.051670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205910_s_at</th>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323177</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208501_at</th>\n",
       "      <td>0.017848</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080279</td>\n",
       "      <td>0.055483</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073765</td>\n",
       "      <td>0.055483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222978_at</th>\n",
       "      <td>0.061169</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210312</td>\n",
       "      <td>0.046922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086762</td>\n",
       "      <td>0.071479</td>\n",
       "      <td>0.061169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204689_at</th>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107290</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207908_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>0.047428</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379852</td>\n",
       "      <td>0.068993</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212489_at</th>\n",
       "      <td>0.013943</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296336</td>\n",
       "      <td>0.051742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108841</td>\n",
       "      <td>0.064810</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219693_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123878</td>\n",
       "      <td>0.079211</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073104</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229678_at</th>\n",
       "      <td>0.115424</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089184</td>\n",
       "      <td>0.122377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049706</td>\n",
       "      <td>0.058991</td>\n",
       "      <td>0.058991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218376_s_at</th>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220681</td>\n",
       "      <td>0.053841</td>\n",
       "      <td>0.002130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223600_s_at</th>\n",
       "      <td>0.176172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160806</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212124_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>0.262009</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027539</td>\n",
       "      <td>0.052874</td>\n",
       "      <td>0.027539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211549_s_at</th>\n",
       "      <td>0.051922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>0.052420</td>\n",
       "      <td>0.015786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219282_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.036086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209804</td>\n",
       "      <td>0.051570</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215195_at</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125975</td>\n",
       "      <td>0.045269</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208592_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029046</td>\n",
       "      <td>0.044532</td>\n",
       "      <td>0.029046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206310_at</th>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048259</td>\n",
       "      <td>0.042343</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223575_at</th>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026476</td>\n",
       "      <td>0.033316</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210676_x_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019680</td>\n",
       "      <td>0.031577</td>\n",
       "      <td>0.019680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228098_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094754</td>\n",
       "      <td>0.023635</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214958_s_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>0.057429</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007504</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.007504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215117_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017896</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052289</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217865_at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.010222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0_RF     1_XGB    2_LGBM  3_ExtraTrees      9_RF    10_XGB  \\\n",
       "1569566_at    1.000000  0.761905  0.166667      0.000000  1.000000  0.703704   \n",
       "223748_at     0.239810  0.261905  1.000000      0.213193  0.354560  0.333333   \n",
       "233185_at     0.289019  0.142857  0.833333      0.390557  0.466654  0.388889   \n",
       "209661_at     0.144378  0.214286  0.500000      0.394731  0.219756  0.277778   \n",
       "223017_at     0.411182  0.333333  0.333333      0.162605  0.324446  0.148148   \n",
       "209626_s_at   0.146771  0.476191  0.333333      0.000000  0.372427  0.481481   \n",
       "1552726_at    0.114999  1.000000  0.000000      0.016747  0.014141  1.000000   \n",
       "201015_s_at   0.665232  0.000000  0.000000      0.718868  0.664016  0.037037   \n",
       "228667_at     0.777687  0.190476  0.166667      0.204861  0.294335  0.314815   \n",
       "1563473_at    0.000000  0.285714  0.500000      0.294400  0.296040  0.296296   \n",
       "221933_at     0.000000  0.166667  0.333333      0.506893  0.419465  0.000000   \n",
       "1554689_a_at  0.183221  0.761905  0.166667      0.274078  0.273133  0.296296   \n",
       "45297_at      0.000000  0.000000  0.500000      0.361025  0.204941  0.129630   \n",
       "225791_at     0.682172  0.047619  0.166667      0.246589  0.119646  0.037037   \n",
       "239886_at     0.365663  0.000000  0.166667      0.618255  0.227033  0.074074   \n",
       "229380_at     0.407494  0.309524  0.000000      0.218567  0.137130  0.185185   \n",
       "227134_at     0.296832  0.119048  0.333333      0.001509  0.340159  0.074074   \n",
       "1552742_at    0.298797  0.833333  0.000000      0.025492  0.000000  0.481481   \n",
       "224252_s_at   0.232003  0.166667  0.000000      0.339085  0.432266  0.000000   \n",
       "230689_at     0.185628  0.404762  0.500000      0.200150  0.177877  0.018519   \n",
       "225611_at     0.052721  0.119048  0.000000      1.000000  0.254994  0.000000   \n",
       "218638_s_at   0.503735  0.071429  0.000000      0.022114  0.174359  0.018519   \n",
       "215784_at     0.109584  0.238095  0.000000      0.148497  0.017047  0.037037   \n",
       "230932_at     0.000000  0.142857  0.000000      0.512280  0.465298  0.037037   \n",
       "1556945_a_at  0.032344  0.214286  0.000000      0.269091  0.000000  0.388889   \n",
       "206907_at     0.000000  0.119048  0.166667      0.363048  0.200825  0.092593   \n",
       "202552_s_at   0.289373  0.142857  0.000000      0.110134  0.333203  0.129630   \n",
       "238622_at     0.172830  0.023810  0.000000      0.537788  0.281261  0.000000   \n",
       "203522_at     0.248877  0.190476  0.000000      0.686273  0.029284  0.111111   \n",
       "207375_s_at   0.000000  0.166667  0.166667      0.295477  0.003553  0.296296   \n",
       "...                ...       ...       ...           ...       ...       ...   \n",
       "225613_at     0.066714  0.047619  0.000000      0.249273  0.035447  0.018519   \n",
       "203325_s_at   0.000000  0.309524  0.000000      0.039506  0.010448  0.351852   \n",
       "237403_at     0.000000  0.095238  0.000000      0.094958  0.000000  0.018519   \n",
       "206752_s_at   0.000000  0.214286  0.000000      0.147126  0.056069  0.055556   \n",
       "213093_at     0.013223  0.238095  0.000000      0.030562  0.000000  0.166667   \n",
       "215933_s_at   0.254518  0.071429  0.000000      0.308700  0.012974  0.000000   \n",
       "228097_at     0.002159  0.238095  0.000000      0.034581  0.000000  0.000000   \n",
       "218337_at     0.132429  0.000000  0.000000      0.000000  0.051670  0.314815   \n",
       "205910_s_at   0.008458  0.095238  0.000000      0.145088  0.000000  0.018519   \n",
       "208501_at     0.017848  0.047619  0.000000      0.080279  0.055483  0.055556   \n",
       "222978_at     0.061169  0.166667  0.000000      0.210312  0.046922  0.000000   \n",
       "204689_at     0.004828  0.071429  0.000000      0.363966  0.000000  0.018519   \n",
       "207908_at     0.000000  0.023810  0.000000      0.063816  0.047428  0.037037   \n",
       "212489_at     0.013943  0.047619  0.000000      0.296336  0.051742  0.000000   \n",
       "219693_at     0.000000  0.071429  0.000000      0.123878  0.079211  0.148148   \n",
       "229678_at     0.115424  0.095238  0.000000      0.089184  0.122377  0.000000   \n",
       "218376_s_at   0.001569  0.095238  0.000000      0.000000  0.002130  0.111111   \n",
       "223600_s_at   0.176172  0.000000  0.000000      0.051518  0.000000  0.037037   \n",
       "212124_at     0.000000  0.071429  0.000000      0.024980  0.262009  0.037037   \n",
       "211549_s_at   0.051922  0.000000  0.000000      0.277577  0.000000  0.074074   \n",
       "219282_s_at   0.000000  0.000000  0.166667      0.036086  0.000000  0.000000   \n",
       "215195_at     0.001826  0.071429  0.000000      0.125885  0.000000  0.037037   \n",
       "208592_s_at   0.000000  0.119048  0.000000      0.078533  0.000000  0.129630   \n",
       "206310_at     0.002236  0.023810  0.000000      0.245923  0.000000  0.018519   \n",
       "223575_at     0.185185  0.000000  0.000000      0.054870  0.000000  0.000000   \n",
       "210676_x_at   0.000000  0.095238  0.000000      0.026585  0.000000  0.111111   \n",
       "228098_s_at   0.000000  0.000000  0.000000      0.038772  0.000000  0.055556   \n",
       "214958_s_at   0.000000  0.000000  0.000000      0.024102  0.057429  0.074074   \n",
       "215117_at     0.000000  0.023810  0.000000      0.017896  0.038398  0.018519   \n",
       "217865_at     0.000000  0.047619  0.000000      0.000000  0.019599  0.000000   \n",
       "\n",
       "               11_LGBM  12_ExtraTrees      MEAN    MEDIAN  \n",
       "1569566_at    0.833333       0.705969  0.646447  0.705969  \n",
       "223748_at     1.000000       0.292892  0.461962  0.333333  \n",
       "233185_at     0.500000       0.365009  0.422040  0.390557  \n",
       "209661_at     0.333333       0.796495  0.360095  0.333333  \n",
       "223017_at     0.500000       0.390542  0.325449  0.333333  \n",
       "209626_s_at   0.500000       0.212822  0.315378  0.333333  \n",
       "1552726_at    0.000000       0.312695  0.307323  0.114999  \n",
       "201015_s_at   0.166667       0.194040  0.305732  0.194040  \n",
       "228667_at     0.333333       0.145589  0.303470  0.294335  \n",
       "1563473_at    0.166667       0.585204  0.303040  0.296040  \n",
       "221933_at     0.500000       0.432588  0.294868  0.333333  \n",
       "1554689_a_at  0.000000       0.244472  0.274971  0.273133  \n",
       "45297_at      0.333333       0.528291  0.257153  0.257153  \n",
       "225791_at     0.333333       0.315221  0.243536  0.243536  \n",
       "239886_at     0.166667       0.277684  0.237005  0.227033  \n",
       "229380_at     0.166667       0.369293  0.224233  0.218567  \n",
       "227134_at     0.333333       0.263165  0.220182  0.263165  \n",
       "1552742_at    0.000000       0.099290  0.217299  0.099290  \n",
       "224252_s_at   0.000000       0.564064  0.216761  0.216761  \n",
       "230689_at     0.000000       0.144692  0.203953  0.185628  \n",
       "225611_at     0.000000       0.146048  0.196601  0.119048  \n",
       "218638_s_at   0.000000       0.776229  0.195798  0.071429  \n",
       "215784_at     0.000000       1.000000  0.193783  0.109584  \n",
       "230932_at     0.000000       0.374511  0.191498  0.142857  \n",
       "1556945_a_at  0.166667       0.330533  0.175226  0.175226  \n",
       "206907_at     0.166667       0.287776  0.174578  0.166667  \n",
       "202552_s_at   0.000000       0.377373  0.172821  0.142857  \n",
       "238622_at     0.000000       0.341578  0.169658  0.169658  \n",
       "203522_at     0.000000       0.081920  0.168493  0.111111  \n",
       "207375_s_at   0.166667       0.161665  0.157124  0.166667  \n",
       "...                ...            ...       ...       ...  \n",
       "225613_at     0.000000       0.356734  0.096788  0.047619  \n",
       "203325_s_at   0.000000       0.024655  0.091998  0.024655  \n",
       "237403_at     0.000000       0.483750  0.086558  0.018519  \n",
       "206752_s_at   0.000000       0.205714  0.084844  0.056069  \n",
       "213093_at     0.000000       0.216555  0.083138  0.030562  \n",
       "215933_s_at   0.000000       0.005204  0.081603  0.012974  \n",
       "228097_at     0.000000       0.341788  0.077078  0.002159  \n",
       "218337_at     0.000000       0.100030  0.074868  0.051670  \n",
       "205910_s_at   0.000000       0.323177  0.073810  0.018519  \n",
       "208501_at     0.333333       0.000000  0.073765  0.055483  \n",
       "222978_at     0.000000       0.086762  0.071479  0.061169  \n",
       "204689_at     0.000000       0.107290  0.070754  0.018519  \n",
       "207908_at     0.000000       0.379852  0.068993  0.037037  \n",
       "212489_at     0.000000       0.108841  0.064810  0.047619  \n",
       "219693_at     0.000000       0.073104  0.061971  0.071429  \n",
       "229678_at     0.000000       0.049706  0.058991  0.058991  \n",
       "218376_s_at   0.000000       0.220681  0.053841  0.002130  \n",
       "223600_s_at   0.000000       0.160806  0.053192  0.037037  \n",
       "212124_at     0.000000       0.027539  0.052874  0.027539  \n",
       "211549_s_at   0.000000       0.015786  0.052420  0.015786  \n",
       "219282_s_at   0.000000       0.209804  0.051570  0.000000  \n",
       "215195_at     0.000000       0.125975  0.045269  0.037037  \n",
       "208592_s_at   0.000000       0.029046  0.044532  0.029046  \n",
       "206310_at     0.000000       0.048259  0.042343  0.018519  \n",
       "223575_at     0.000000       0.026476  0.033316  0.000000  \n",
       "210676_x_at   0.000000       0.019680  0.031577  0.019680  \n",
       "228098_s_at   0.000000       0.094754  0.023635  0.000000  \n",
       "214958_s_at   0.000000       0.007504  0.020389  0.007504  \n",
       "215117_at     0.000000       0.052289  0.018864  0.018519  \n",
       "217865_at     0.000000       0.014556  0.010222  0.000000  \n",
       "\n",
       "[78 rows x 10 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17_SVM</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>MEDIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207375_s_at</th>\n",
       "      <td>-0.778240</td>\n",
       "      <td>-0.778240</td>\n",
       "      <td>-0.778240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207375_s_at</th>\n",
       "      <td>-0.778240</td>\n",
       "      <td>-0.778240</td>\n",
       "      <td>-0.778240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569566_at</th>\n",
       "      <td>-0.738152</td>\n",
       "      <td>-0.738152</td>\n",
       "      <td>-0.738152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569566_at</th>\n",
       "      <td>-0.738152</td>\n",
       "      <td>-0.738152</td>\n",
       "      <td>-0.738152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223017_at</th>\n",
       "      <td>-0.711523</td>\n",
       "      <td>-0.711523</td>\n",
       "      <td>-0.711523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223017_at</th>\n",
       "      <td>-0.711523</td>\n",
       "      <td>-0.711523</td>\n",
       "      <td>-0.711523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217865_at</th>\n",
       "      <td>-0.580438</td>\n",
       "      <td>-0.580438</td>\n",
       "      <td>-0.580438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217865_at</th>\n",
       "      <td>-0.580438</td>\n",
       "      <td>-0.580438</td>\n",
       "      <td>-0.580438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221933_at</th>\n",
       "      <td>-0.572213</td>\n",
       "      <td>-0.572213</td>\n",
       "      <td>-0.572213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221933_at</th>\n",
       "      <td>-0.572213</td>\n",
       "      <td>-0.572213</td>\n",
       "      <td>-0.572213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230689_at</th>\n",
       "      <td>-0.570741</td>\n",
       "      <td>-0.570741</td>\n",
       "      <td>-0.570741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230689_at</th>\n",
       "      <td>-0.570741</td>\n",
       "      <td>-0.570741</td>\n",
       "      <td>-0.570741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218862_at</th>\n",
       "      <td>-0.551026</td>\n",
       "      <td>-0.551026</td>\n",
       "      <td>-0.551026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218862_at</th>\n",
       "      <td>-0.551026</td>\n",
       "      <td>-0.551026</td>\n",
       "      <td>-0.551026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206589_at</th>\n",
       "      <td>-0.509342</td>\n",
       "      <td>-0.509342</td>\n",
       "      <td>-0.509342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206589_at</th>\n",
       "      <td>-0.509342</td>\n",
       "      <td>-0.509342</td>\n",
       "      <td>-0.509342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213093_at</th>\n",
       "      <td>-0.489782</td>\n",
       "      <td>-0.489782</td>\n",
       "      <td>-0.489782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213093_at</th>\n",
       "      <td>-0.489782</td>\n",
       "      <td>-0.489782</td>\n",
       "      <td>-0.489782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552726_at</th>\n",
       "      <td>-0.477115</td>\n",
       "      <td>-0.477115</td>\n",
       "      <td>-0.477115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552726_at</th>\n",
       "      <td>-0.477115</td>\n",
       "      <td>-0.477115</td>\n",
       "      <td>-0.477115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568713_a_at</th>\n",
       "      <td>-0.436315</td>\n",
       "      <td>-0.436315</td>\n",
       "      <td>-0.436315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568713_a_at</th>\n",
       "      <td>-0.436315</td>\n",
       "      <td>-0.436315</td>\n",
       "      <td>-0.436315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233185_at</th>\n",
       "      <td>-0.432951</td>\n",
       "      <td>-0.432951</td>\n",
       "      <td>-0.432951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233185_at</th>\n",
       "      <td>-0.432951</td>\n",
       "      <td>-0.432951</td>\n",
       "      <td>-0.432951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554689_a_at</th>\n",
       "      <td>-0.362968</td>\n",
       "      <td>-0.362968</td>\n",
       "      <td>-0.362968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554689_a_at</th>\n",
       "      <td>-0.362968</td>\n",
       "      <td>-0.362968</td>\n",
       "      <td>-0.362968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239886_at</th>\n",
       "      <td>-0.356627</td>\n",
       "      <td>-0.356627</td>\n",
       "      <td>-0.356627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239886_at</th>\n",
       "      <td>-0.356627</td>\n",
       "      <td>-0.356627</td>\n",
       "      <td>-0.356627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206923_at</th>\n",
       "      <td>-0.285478</td>\n",
       "      <td>-0.285478</td>\n",
       "      <td>-0.285478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206923_at</th>\n",
       "      <td>-0.285478</td>\n",
       "      <td>-0.285478</td>\n",
       "      <td>-0.285478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225611_at</th>\n",
       "      <td>0.500255</td>\n",
       "      <td>0.500255</td>\n",
       "      <td>0.500255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225611_at</th>\n",
       "      <td>0.500255</td>\n",
       "      <td>0.500255</td>\n",
       "      <td>0.500255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45297_at</th>\n",
       "      <td>0.504332</td>\n",
       "      <td>0.504332</td>\n",
       "      <td>0.504332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45297_at</th>\n",
       "      <td>0.504332</td>\n",
       "      <td>0.504332</td>\n",
       "      <td>0.504332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212488_at</th>\n",
       "      <td>0.539668</td>\n",
       "      <td>0.539668</td>\n",
       "      <td>0.539668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212488_at</th>\n",
       "      <td>0.539668</td>\n",
       "      <td>0.539668</td>\n",
       "      <td>0.539668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213367_at</th>\n",
       "      <td>0.545995</td>\n",
       "      <td>0.545995</td>\n",
       "      <td>0.545995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213367_at</th>\n",
       "      <td>0.545995</td>\n",
       "      <td>0.545995</td>\n",
       "      <td>0.545995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225791_at</th>\n",
       "      <td>0.582158</td>\n",
       "      <td>0.582158</td>\n",
       "      <td>0.582158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225791_at</th>\n",
       "      <td>0.582158</td>\n",
       "      <td>0.582158</td>\n",
       "      <td>0.582158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203522_at</th>\n",
       "      <td>0.585908</td>\n",
       "      <td>0.585908</td>\n",
       "      <td>0.585908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203522_at</th>\n",
       "      <td>0.585908</td>\n",
       "      <td>0.585908</td>\n",
       "      <td>0.585908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552742_at</th>\n",
       "      <td>0.602966</td>\n",
       "      <td>0.602966</td>\n",
       "      <td>0.602966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552742_at</th>\n",
       "      <td>0.602966</td>\n",
       "      <td>0.602966</td>\n",
       "      <td>0.602966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204692_at</th>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.643246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204692_at</th>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.643246</td>\n",
       "      <td>0.643246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223748_at</th>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.665019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223748_at</th>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.665019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556945_a_at</th>\n",
       "      <td>0.699921</td>\n",
       "      <td>0.699921</td>\n",
       "      <td>0.699921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556945_a_at</th>\n",
       "      <td>0.699921</td>\n",
       "      <td>0.699921</td>\n",
       "      <td>0.699921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223575_at</th>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.702153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223575_at</th>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.702153</td>\n",
       "      <td>0.702153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215784_at</th>\n",
       "      <td>0.722935</td>\n",
       "      <td>0.722935</td>\n",
       "      <td>0.722935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215784_at</th>\n",
       "      <td>0.722935</td>\n",
       "      <td>0.722935</td>\n",
       "      <td>0.722935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215807_s_at</th>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215807_s_at</th>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.730463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209661_at</th>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.932969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209661_at</th>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.932969</td>\n",
       "      <td>0.932969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209626_s_at</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209626_s_at</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                17_SVM      MEAN    MEDIAN\n",
       "207375_s_at  -0.778240 -0.778240 -0.778240\n",
       "207375_s_at  -0.778240 -0.778240 -0.778240\n",
       "1569566_at   -0.738152 -0.738152 -0.738152\n",
       "1569566_at   -0.738152 -0.738152 -0.738152\n",
       "223017_at    -0.711523 -0.711523 -0.711523\n",
       "223017_at    -0.711523 -0.711523 -0.711523\n",
       "217865_at    -0.580438 -0.580438 -0.580438\n",
       "217865_at    -0.580438 -0.580438 -0.580438\n",
       "221933_at    -0.572213 -0.572213 -0.572213\n",
       "221933_at    -0.572213 -0.572213 -0.572213\n",
       "230689_at    -0.570741 -0.570741 -0.570741\n",
       "230689_at    -0.570741 -0.570741 -0.570741\n",
       "218862_at    -0.551026 -0.551026 -0.551026\n",
       "218862_at    -0.551026 -0.551026 -0.551026\n",
       "206589_at    -0.509342 -0.509342 -0.509342\n",
       "206589_at    -0.509342 -0.509342 -0.509342\n",
       "213093_at    -0.489782 -0.489782 -0.489782\n",
       "213093_at    -0.489782 -0.489782 -0.489782\n",
       "1552726_at   -0.477115 -0.477115 -0.477115\n",
       "1552726_at   -0.477115 -0.477115 -0.477115\n",
       "1568713_a_at -0.436315 -0.436315 -0.436315\n",
       "1568713_a_at -0.436315 -0.436315 -0.436315\n",
       "233185_at    -0.432951 -0.432951 -0.432951\n",
       "233185_at    -0.432951 -0.432951 -0.432951\n",
       "1554689_a_at -0.362968 -0.362968 -0.362968\n",
       "1554689_a_at -0.362968 -0.362968 -0.362968\n",
       "239886_at    -0.356627 -0.356627 -0.356627\n",
       "239886_at    -0.356627 -0.356627 -0.356627\n",
       "206923_at    -0.285478 -0.285478 -0.285478\n",
       "206923_at    -0.285478 -0.285478 -0.285478\n",
       "...                ...       ...       ...\n",
       "225611_at     0.500255  0.500255  0.500255\n",
       "225611_at     0.500255  0.500255  0.500255\n",
       "45297_at      0.504332  0.504332  0.504332\n",
       "45297_at      0.504332  0.504332  0.504332\n",
       "212488_at     0.539668  0.539668  0.539668\n",
       "212488_at     0.539668  0.539668  0.539668\n",
       "213367_at     0.545995  0.545995  0.545995\n",
       "213367_at     0.545995  0.545995  0.545995\n",
       "225791_at     0.582158  0.582158  0.582158\n",
       "225791_at     0.582158  0.582158  0.582158\n",
       "203522_at     0.585908  0.585908  0.585908\n",
       "203522_at     0.585908  0.585908  0.585908\n",
       "1552742_at    0.602966  0.602966  0.602966\n",
       "1552742_at    0.602966  0.602966  0.602966\n",
       "204692_at     0.643246  0.643246  0.643246\n",
       "204692_at     0.643246  0.643246  0.643246\n",
       "223748_at     0.665019  0.665019  0.665019\n",
       "223748_at     0.665019  0.665019  0.665019\n",
       "1556945_a_at  0.699921  0.699921  0.699921\n",
       "1556945_a_at  0.699921  0.699921  0.699921\n",
       "223575_at     0.702153  0.702153  0.702153\n",
       "223575_at     0.702153  0.702153  0.702153\n",
       "215784_at     0.722935  0.722935  0.722935\n",
       "215784_at     0.722935  0.722935  0.722935\n",
       "215807_s_at   0.730463  0.730463  0.730463\n",
       "215807_s_at   0.730463  0.730463  0.730463\n",
       "209661_at     0.932969  0.932969  0.932969\n",
       "209661_at     0.932969  0.932969  0.932969\n",
       "209626_s_at   1.000000  1.000000  1.000000\n",
       "209626_s_at   1.000000  1.000000  1.000000\n",
       "\n",
       "[156 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualise the top weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_coeffs.to_csv(\"out/coeffs_\"+Rocket.SET_NAME+\"_FDR_alpha0025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_weights.to_csv(\"out/weights_\"+Rocket.SET_NAME+\"_FDR_alpha0025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF9 = top_genomes_weights['RandomForest'].quantile(q=0.9)\n",
    "GBM9 = top_genomes_weights['GBM'].quantile(q=0.9)\n",
    "ADA9 = top_genomes_weights['AdaBoost'].quantile(q=0.9)\n",
    "ET9 = top_genomes_weights['ExtraTrees'].quantile(q=0.9)\n",
    "Overlapping_genomes = set(top_genomes_weights.loc[top_genomes_weights['RandomForest']>RF9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['GBM']>GBM9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['AdaBoost']>ADA9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['ExtraTrees']>ET9].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "#from scipy.dspatial.distance import cosine\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cdist\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TransPosed = Rocket.DATA_all_samples.T # all microarrays, may be multiple per patient versus all probesets, may be multiple per genome\n",
    "Normal = Rocket.DATA_merged_processed.loc[:, (Rocket.DATA_merged_processed.columns !='target') & \n",
    "                                             (Rocket.DATA_merged_processed.columns !='ID')]\n",
    "#AllNormal = Rocket.DATA_merged\n",
    "#probeset_weights = Rocket.get_probeset_weights(method = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 9827_corr2.CEL, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'cosine', normalised = True, inflation = 2, minkowski_dim=1)\n",
    "##### apply Markov clustering\n",
    "#######################\n",
    "# non-distributed, non-sparse version, only for small-sized problems (N is order 1000)\n",
    "e = 2\n",
    "r = 2 \n",
    "epsilon = 1e-7\n",
    "convergence = 0.001\n",
    "num_iter = 10\n",
    "Orientation = 'col' # columnwise or rowwise\n",
    "\n",
    "# add loop\n",
    "def add_loop(df_matrix, value=0): \n",
    "    for i in df_matrix.index:\n",
    "        df_matrix.loc[i, i] = value\n",
    "    return df_matrix\n",
    "patient_sim = add_loop(patient_sim, 1)\n",
    "patient_sim = patient_sim - epsilon\n",
    "\n",
    "def normalise(sim, type = 'col'):\n",
    "    if(type == 'col'):\n",
    "        # column normalisation\n",
    "        for variable in sim.keys():\n",
    "            col_vec = sim[variable]\n",
    "            sum_val = sum([p for p in col_vec])\n",
    "            sim[variable] = sim[variable]/sum_val\n",
    "    elif (type == 'row'):\n",
    "        # row normalisation\n",
    "        for variable in sim.keys():\n",
    "            row_vec = sim.loc[variable, :]\n",
    "            sum_val = sum([p for p in row_vec])\n",
    "            sim.loc[variable,:] = sim.loc[variable,:]/sum_val\n",
    "    return sim\n",
    "\n",
    "# step E: expansion, get the nth power of the matrix\n",
    "def expansion(sim):\n",
    "    X = numpy.array(sim)\n",
    "    VarList = sim.keys()\n",
    "    if e == 1:\n",
    "        return sim\n",
    "    elif e > 1:        \n",
    "        return pandas.DataFrame(numpy.linalg.matrix_power(X, e), index = VarList, columns = VarList)\n",
    "     \n",
    "# step I: inflation, per column raise by rth power and column normalise\n",
    "def inflation(sim, type = 'col'):    \n",
    "    if type == 'col':\n",
    "        Axis = 0\n",
    "    elif type == 'row':\n",
    "        Axis = 1\n",
    "    return sim.apply(lambda x: x**r/sum(x**r), axis = Axis)\n",
    "\n",
    "# remove weak connections, values < epsilon\n",
    "def clean(sim):\n",
    "    return sim.applymap(lambda x:0 if x<epsilon else x)\n",
    "    \n",
    "def difference(old, new):\n",
    "    # relative zeroes over entire array\n",
    "    #return (new.apply(lambda x: numpy.ceil(x-epsilon)) - old.apply(lambda x: numpy.ceil(x-epsilon))).sum().sum()/len(old)**2    \n",
    "    return abs(new - old).sum().sum()/len(old)**2    \n",
    "\n",
    "#patient_sim = normalise(patient_sim, type = Orientation)\n",
    "_sim_a = patient_sim\n",
    "for i in range(0,num_iter):\n",
    "    # repeat E and I until convergence, the row-wise elements form the clusters.\n",
    "    _sim_b = clean(inflation(expansion(_sim_a), type = Orientation))\n",
    "    _sim_a = normalise(_sim_a, type = Orientation)\n",
    "    #if ((difference(_sim_a, _sim_b)) < convergence) & (i>0):\n",
    "    #    print(difference(_sim_a, _sim_b))\n",
    "    #    print(\"CONVERGED after \", i, \" iterations\")\n",
    "    #    break;\n",
    "    _sim_a = _sim_b\n",
    "\n",
    "result_mcl = clean(_sim_b)\n",
    "result_mcl.loc[result_mcl.loc['9827_corr2.CEL',:]>epsilon, '9827_corr2.CEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 patient clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'pearson', normalised = False, inflation=1, minkowski_dim=1)\n",
    "##### apply Affinity Propagation\n",
    "#######################\n",
    "X = numpy.array(patient_sim)\n",
    "af = AffinityPropagation(preference=-10).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "patient_clusters = patient_sim.keys()[cluster_centers_indices].values\n",
    "patient_cluster_members = af.labels_\n",
    "print(\"There are {} patient clusters\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AggResults = Rocket.DATA_merged\n",
    "AggResults = _helpers._preprocess(AggResults, Rclass = Rocket)\n",
    "#AggResults = _helpers._group_patients(AggResults, method = 'mean')\n",
    "AggResults['cluster_ap'] = patient_cluster_members\n",
    "\n",
    "#AggResults.groupby(['Treatment risk group in ALL10', 'cluster_ap']).agg({'Microarray file': pandas.Series.nunique})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "AggResults['FU_RFS'] = pandas.to_numeric(AggResults['FU_RFS'])\n",
    "AggResults['FU_EFS'] = pandas.to_numeric(AggResults['FU_EFS'])\n",
    "AggResults['FU_OS'] = pandas.to_numeric(AggResults['FU_OS'])\n",
    "AggResults['WhiteBloodCellcount'] = pandas.to_numeric(AggResults['WhiteBloodCellcount'])\n",
    "AggResults['Age'] = pandas.to_numeric(AggResults['Age'])\n",
    "AggResults['Gender'] = pandas.to_numeric(AggResults['Gender'])\n",
    "AggResults['code_RFS']= pandas.to_numeric(AggResults['code_RFS'])\n",
    "AggResults['code_EFS']= pandas.to_numeric(AggResults['code_EFS'])\n",
    "AggResults['code_OS']= pandas.to_numeric(AggResults['code_OS'])\n",
    "\n",
    "AggResults['mutations_NOTCH_pathway'] = pandas.to_numeric(AggResults['mutations_NOTCH_pathway'])\n",
    "AggResults['mutations_PTEN_AKT_pathway'] = pandas.to_numeric(AggResults['mutations_PTEN_AKT_pathway'])\n",
    "AggResults['mutations_IL7R_pathway'] = pandas.to_numeric(AggResults['mutations_IL7R_pathway'])\n",
    "#AggResults.replace(to_replace=9999, value=0.5, inplace=True)\n",
    "AggResults[['mutations_NOTCH_pathway', \n",
    "            'mutations_PTEN_AKT_pathway', \n",
    "            'mutations_IL7R_pathway']] = AggResults[['mutations_NOTCH_pathway', \n",
    "                                                    'mutations_PTEN_AKT_pathway', \n",
    "                                                    'mutations_IL7R_pathway']].replace([9999],[numpy.nan],\n",
    "                                                                                       inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "AggResults['comb_mutations_NOTCH_IL7R'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_NOTCH_PTEN'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_PTEN_AKT_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN'] =  AggResults['mutations_PTEN_AKT_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN_NOTCH'] =  AggResults['mutations_PTEN_AKT_pathway']\\\n",
    "                                                + AggResults['mutations_IL7R_pathway']\\\n",
    "                                                + AggResults['mutations_NOTCH_pathway']\n",
    "\n",
    "\n",
    "patient_count = AggResults.groupby(['cluster_ap']).agg({'labnr_patient': pandas.Series.nunique})\n",
    "Clustered_by_patients_whitebloodcells = AggResults[AggResults['WhiteBloodCellcount'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'WhiteBloodCellcount': numpy.mean,\n",
    "    'Age': numpy.mean, \n",
    "    'Gender': numpy.mean})\n",
    "\n",
    "# Cancer_gene\n",
    "# Treatment_protocol\n",
    "# Treatment_risk_group_in_ALL_10\n",
    "\n",
    "Clustered_by_patients_CODE = AggResults.groupby(['cluster_ap']).agg(\n",
    "    {'code_RFS': numpy.mean, \n",
    "     'code_EFS': numpy.mean,\n",
    "     'code_OS': numpy.mean})\n",
    "\n",
    "Clustered_by_patients_FU_RFS = AggResults[AggResults['FU_RFS'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'FU_RFS': numpy.median, \n",
    "     'FU_EFS': numpy.median,\n",
    "     'FU_OS': numpy.median})\n",
    "Clustered_by_patients_NotchPath = AggResults[AggResults['mutations_NOTCH_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_NOTCH_pathway': numpy.mean})\n",
    "Clustered_by_patients_IL7RPath = AggResults[AggResults['mutations_IL7R_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_IL7R_pathway': numpy.mean})\n",
    "Clustered_by_patients_PTENAKTPath = AggResults[AggResults['mutations_PTEN_AKT_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_PTEN_AKT_pathway': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_IL7R = AggResults[AggResults['comb_mutations_NOTCH_IL7R'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_IL7R': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_PTEN = AggResults[AggResults['comb_mutations_NOTCH_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN = AggResults[AggResults['comb_mutations_IL7R_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN_NOTCH = AggResults[AggResults['comb_mutations_IL7R_PTEN_NOTCH'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN_NOTCH': numpy.mean})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_agg = pandas.merge(Clustered_by_patients_whitebloodcells, Clustered_by_patients_CODE, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN_NOTCH, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_IL7R, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_PTEN, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_FU_RFS, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_IL7RPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_NotchPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_PTENAKTPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, patient_count, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Cluster centers:\",patient_sim.keys()[cluster_centers_indices].values)\n",
    "print(patient_cluster_members)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    class_members = patient_cluster_members == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.', \n",
    "             label = patient_sim.keys()[cluster_centers_indices[k]])\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.legend()\n",
    "        \n",
    "plt.title('Estimated number of clusters from Affinity Propagation: %d' % n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATE graph from similarity matrix\n",
    "##################\n",
    "# nodes\n",
    "VarList = TransPosed.keys()\n",
    "nodes = []\n",
    "node_index = 0\n",
    "for patient_name in VarList:\n",
    "    nodes.append((node_index, {'name': patient_name}))\n",
    "    node_index = node_index + 1\n",
    "\n",
    "edges = []\n",
    "# edges\n",
    "patient_sim = patient_similarity(Normal, sim_type = 'pearson', normalised = True, inflation=2)\n",
    "node_index_x = 0\n",
    "node_index_y = 0\n",
    "for patient_name_x in VarList:\n",
    "    for patient_name_y in VarList:        \n",
    "        edges.append((node_index_x, node_index_y, patient_sim.iloc[node_index_x, node_index_y]))\n",
    "        node_index_y = node_index_y + 1\n",
    "    node_index_x = node_index_x + 1\n",
    "    node_index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_weighted_edges_from(edges, weight = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:126: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  b = plt.ishold()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:138: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold(b)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFCCAYAAABSJMy8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f31beac128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### apply Spring-force\n",
    "#######################\n",
    "pos = nx.spring_layout(G, k = None, dim = 3, scale = 1.0)\n",
    "nx.draw_spring(G, k = 30, dim = 2, scale = 1.0, iterations =1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### APPLY community detector\n",
    "# maximize betweenness and modularity\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### LOAD IN DATA\n",
    "###################\n",
    "# https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
