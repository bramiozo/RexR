{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ Firing up RexR! ++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from RexR import *\n",
    "import _helpers\n",
    "Rocket = RexR(datalocation = None, #'_data/genomic_data/data.pkl', \n",
    "              seed = 3123, \n",
    "              debug = False, \n",
    "              write_out=True,\n",
    "              set_name = 'ALL_10') # data to read in ALL_10, or MELA\n",
    "Rocket.load_probeset_data();\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: ET accuracy:  0.568965517241 +/-: 0.000337801318776\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: CART accuracy:  0.603448275862 +/-: 0.0121563434584\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: RF accuracy:  0.551724137931 +/-: 0.0250153136598\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: GBM accuracy:  0.448275862069 +/-: 0.0221417504414\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: ADA accuracy:  0.5 +/-: 0.0240334378265\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: LR accuracy:  0.603448275862 +/-: 0.0176422368753\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: SVM accuracy:  0.724137931034 +/-: 0.0249972975894\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: GNB accuracy:  0.51724137931 +/-: 0.00335999711743\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "MODEL: MLNN accuracy:  0.551724137931 +/-: 0.0233172990307\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Initial alpha = [[ 0.1288511]]\n",
      "   1 - L=-1112.6490852 - Gamma= 1.9999356 (M=   2) - s=0.0100\n",
      "   2 - L=-984.5816943 - Gamma= 2.9998507 (M=   3) - s=0.0100\n",
      "   3 - L=-866.0203176 - Gamma= 3.9997591 (M=   4) - s=0.0100\n",
      "   4 - L=-789.8218358 - Gamma= 4.9996150 (M=   5) - s=0.0100\n",
      "   5 - L=-709.1747209 - Gamma= 5.9994783 (M=   6) - s=0.0100\n",
      "   6 - L=-617.8017005 - Gamma= 6.9993514 (M=   7) - s=0.0100\n",
      "   7 - L=-563.1133998 - Gamma= 7.9991523 (M=   8) - s=0.0100\n",
      "   8 - L=-506.2890324 - Gamma= 8.9989582 (M=   9) - s=0.0100\n",
      "   9 - L=-434.0518215 - Gamma= 9.9987983 (M=  10) - s=0.0100\n",
      "  10 - L=-384.1082850 - Gamma=10.9985724 (M=  11) - s=0.0100\n",
      "  11 - L=-344.6418587 - Gamma=11.9982805 (M=  12) - s=0.0100\n",
      "  12 - L=-306.8114267 - Gamma=12.9979862 (M=  13) - s=0.0100\n",
      "  13 - L=-255.7731987 - Gamma=13.9977511 (M=  14) - s=0.0100\n",
      "  14 - L=-223.9540605 - Gamma=14.9973902 (M=  15) - s=0.0100\n",
      "  15 - L=-199.1961804 - Gamma=15.9969385 (M=  16) - s=0.0100\n",
      "  16 - L=-177.2285226 - Gamma=16.9964333 (M=  17) - s=0.0100\n",
      "  17 - L=-156.1836824 - Gamma=17.9958937 (M=  18) - s=0.0100\n",
      "  18 - L=-132.9440981 - Gamma=18.9952801 (M=  19) - s=0.0100\n",
      "  19 - L=-113.9206279 - Gamma=19.9946950 (M=  20) - s=0.0100\n",
      "  20 - L=-93.0674030 - Gamma=20.9941302 (M=  21) - s=0.0100\n",
      "  21 - L=-64.7734470 - Gamma=21.9937090 (M=  22) - s=0.0100\n",
      "  22 - L=-49.5399535 - Gamma=22.9929628 (M=  23) - s=0.0100\n",
      "  23 - L=-42.1533237 - Gamma=23.9914814 (M=  24) - s=0.0100\n",
      "  24 - L=-34.4640893 - Gamma=24.9900025 (M=  25) - s=0.0100\n",
      "  25 - L=-28.5850061 - Gamma=25.9881412 (M=  26) - s=0.0100\n",
      "  26 - L=-23.7820536 - Gamma=26.9857645 (M=  27) - s=0.0100\n",
      "  27 - L=-20.3722414 - Gamma=27.9825617 (M=  28) - s=0.0100\n",
      "  28 - L=-17.3649500 - Gamma=28.9785638 (M=  29) - s=0.0100\n",
      "  29 - L=-14.2918346 - Gamma=29.9750104 (M=  30) - s=0.0100\n",
      "  30 - L=-11.4346433 - Gamma=30.9709728 (M=  31) - s=0.0100\n",
      "  31 - L=-7.9478784 - Gamma=31.9673369 (M=  32) - s=0.0100\n",
      "  32 - L=-3.7802575 - Gamma=32.9642179 (M=  33) - s=0.0100\n",
      "  33 - L=-2.4449722 - Gamma=33.9558979 (M=  34) - s=0.0100\n",
      "  34 - L=-1.1758253 - Gamma=34.9474475 (M=  35) - s=0.0100\n",
      "  35 - L=-0.5757109 - Gamma=35.9300628 (M=  36) - s=0.0100\n",
      "  36 - L=-0.0640593 - Gamma=36.9062764 (M=  37) - s=0.0100\n",
      "  37 - L= 0.4879279 - Gamma=37.8816773 (M=  38) - s=0.0100\n",
      "  38 - L= 0.7450655 - Gamma=38.8420514 (M=  39) - s=0.0100\n",
      "  39 - L= 0.8365223 - Gamma=39.7512078 (M=  40) - s=0.0100\n",
      "  40 - L= 0.8717803 - Gamma=40.5811616 (M=  41) - s=0.0100\n",
      "  41 - L= 0.8883293 - Gamma=40.5563707 (M=  41) - s=0.0100\n",
      "  42 - L= 0.9013788 - Gamma=40.5568097 (M=  41) - s=0.0100\n",
      "  43 - L= 0.9122874 - Gamma=40.5591549 (M=  41) - s=0.0100\n",
      "  44 - L= 0.9198219 - Gamma=40.5584746 (M=  41) - s=0.0100\n",
      "  45 - L= 0.9267611 - Gamma=40.5705731 (M=  41) - s=0.0100\n",
      "  46 - L= 0.9334457 - Gamma=40.5733568 (M=  41) - s=0.0100\n",
      "  47 - L= 0.9400337 - Gamma=40.5749121 (M=  41) - s=0.0100\n",
      "  48 - L= 0.9451370 - Gamma=40.5750960 (M=  41) - s=0.0100\n",
      "  49 - L= 0.9492921 - Gamma=40.5752287 (M=  41) - s=0.0100\n",
      "  50 - L= 0.9531345 - Gamma=40.5748052 (M=  41) - s=0.0100\n",
      "  51 - L= 0.9569501 - Gamma=40.5750558 (M=  41) - s=0.0100\n",
      "  52 - L= 0.9600551 - Gamma=40.5751137 (M=  41) - s=0.0100\n",
      "  53 - L= 0.9630461 - Gamma=40.5744843 (M=  41) - s=0.0100\n",
      "  54 - L= 0.9659334 - Gamma=41.0394103 (M=  42) - s=0.0100\n",
      "  55 - L= 0.9689755 - Gamma=41.0414016 (M=  42) - s=0.0100\n",
      "  56 - L= 0.9714396 - Gamma=41.0417487 (M=  42) - s=0.0100\n",
      "  57 - L= 0.9736710 - Gamma=41.0421302 (M=  42) - s=0.0100\n",
      "  58 - L= 0.9750497 - Gamma=41.0409718 (M=  42) - s=0.0100\n",
      "  59 - L= 0.9762356 - Gamma=41.0403731 (M=  42) - s=0.0100\n",
      "  60 - L= 0.9771783 - Gamma=41.0415835 (M=  42) - s=0.0100\n",
      "  61 - L= 0.9780367 - Gamma=41.0431017 (M=  42) - s=0.0100\n",
      "  62 - L= 0.9784957 - Gamma=41.0521408 (M=  42) - s=0.0100\n",
      "  63 - L= 0.9789224 - Gamma=41.0534594 (M=  42) - s=0.0100\n",
      "  64 - L= 0.9791534 - Gamma=41.0512027 (M=  42) - s=0.0100\n",
      "  65 - L= 0.9793431 - Gamma=41.0512829 (M=  42) - s=0.0100\n",
      "  66 - L= 0.9794619 - Gamma=41.0513480 (M=  42) - s=0.0100\n",
      "  67 - L= 0.9795773 - Gamma=41.1676816 (M=  43) - s=0.0100\n",
      "  68 - L= 0.9796878 - Gamma=41.1676918 (M=  43) - s=0.0100\n",
      "  69 - L= 0.9797851 - Gamma=41.1677769 (M=  43) - s=0.0100\n",
      "  70 - L= 0.9798647 - Gamma=41.1640936 (M=  43) - s=0.0100\n",
      "  71 - L= 0.9799465 - Gamma=41.1661713 (M=  43) - s=0.0100\n",
      "  72 - L= 0.9800232 - Gamma=41.1688226 (M=  43) - s=0.0100\n",
      "  73 - L= 0.9800952 - Gamma=41.1692097 (M=  43) - s=0.0100\n",
      "  74 - L= 0.9801496 - Gamma=41.1690082 (M=  43) - s=0.0100\n",
      "  75 - L= 0.9801937 - Gamma=41.1690326 (M=  43) - s=0.0100\n",
      "  76 - L= 0.9802072 - Gamma=41.1604787 (M=  43) - s=0.0100\n",
      "  77 - L= 0.9802217 - Gamma=41.2077847 (M=  44) - s=0.0100\n",
      "  78 - L= 0.9802413 - Gamma=41.2126395 (M=  44) - s=0.0100\n",
      "  79 - L= 0.9802526 - Gamma=41.2126523 (M=  44) - s=0.0100\n",
      "  80 - L= 0.9802622 - Gamma=41.2126477 (M=  44) - s=0.0100\n",
      "  81 - L= 0.9802710 - Gamma=41.1813197 (M=  44) - s=0.0100\n",
      "  82 - L= 0.9802800 - Gamma=41.1740041 (M=  44) - s=0.0100\n",
      "  83 - L= 0.9802893 - Gamma=41.1941637 (M=  44) - s=0.0100\n",
      "  84 - L= 0.9802992 - Gamma=41.1594886 (M=  44) - s=0.0100\n",
      "  85 - L= 0.9803057 - Gamma=41.1594688 (M=  44) - s=0.0100\n",
      "  86 - L= 0.9803119 - Gamma=41.1597219 (M=  44) - s=0.0100\n",
      "  87 - L= 0.9803180 - Gamma=41.1893034 (M=  44) - s=0.0100\n",
      "  88 - L= 0.9803281 - Gamma=41.1812172 (M=  44) - s=0.0100\n",
      "  89 - L= 0.9803412 - Gamma=41.2252316 (M=  45) - s=0.0100\n",
      "  90 - L= 0.9803698 - Gamma=41.1779790 (M=  44) - s=0.0100\n",
      "  91 - L= 0.9803942 - Gamma=41.2334477 (M=  44) - s=0.0100\n",
      "  92 - L= 0.9804547 - Gamma=41.3224391 (M=  44) - s=0.0100\n",
      "  93 - L= 0.9804999 - Gamma=41.3038916 (M=  44) - s=0.0100\n",
      "  94 - L= 0.9805735 - Gamma=41.3897868 (M=  44) - s=0.0100\n",
      "  95 - L= 0.9806334 - Gamma=41.4688017 (M=  44) - s=0.0100\n",
      "  96 - L= 0.9806629 - Gamma=41.5182186 (M=  44) - s=0.0100\n",
      "  97 - L= 0.9807121 - Gamma=41.4968269 (M=  44) - s=0.0100\n",
      "  98 - L= 0.9807394 - Gamma=41.5457436 (M=  44) - s=0.0100\n",
      "  99 - L= 0.9807736 - Gamma=41.5946797 (M=  44) - s=0.0100\n",
      " 100 - L= 0.9807933 - Gamma=41.5933386 (M=  44) - s=0.0100\n",
      " 101 - L= 0.9808090 - Gamma=41.6282639 (M=  44) - s=0.0100\n",
      " 102 - L= 0.9808299 - Gamma=41.6132237 (M=  44) - s=0.0100\n",
      " 103 - L= 0.9808495 - Gamma=41.6477465 (M=  44) - s=0.0100\n",
      " 104 - L= 0.9808658 - Gamma=41.6470874 (M=  44) - s=0.0100\n",
      " 105 - L= 0.9808790 - Gamma=41.6774362 (M=  44) - s=0.0100\n",
      " 106 - L= 0.9808874 - Gamma=41.6778569 (M=  44) - s=0.0100\n",
      " 107 - L= 0.9808954 - Gamma=41.6681011 (M=  44) - s=0.0100\n",
      " 108 - L= 0.9809070 - Gamma=41.6933193 (M=  44) - s=0.0100\n",
      " 109 - L= 0.9809150 - Gamma=41.7159634 (M=  44) - s=0.0100\n",
      " 110 - L= 0.9809241 - Gamma=41.6957132 (M=  44) - s=0.0100\n",
      " 111 - L= 0.9809324 - Gamma=41.6954092 (M=  44) - s=0.0100\n",
      " 112 - L= 0.9809401 - Gamma=41.6947752 (M=  44) - s=0.0100\n",
      " 113 - L= 0.9809477 - Gamma=41.6946769 (M=  44) - s=0.0100\n",
      " 114 - L= 0.9809533 - Gamma=41.6947638 (M=  44) - s=0.0100\n",
      " 115 - L= 0.9809586 - Gamma=41.6946589 (M=  44) - s=0.0100\n",
      " 116 - L= 0.9809639 - Gamma=41.6939376 (M=  44) - s=0.0100\n",
      " 117 - L= 0.9809696 - Gamma=41.7109596 (M=  44) - s=0.0100\n",
      " 118 - L= 0.9809762 - Gamma=41.7308561 (M=  44) - s=0.0100\n",
      " 119 - L= 0.9809863 - Gamma=41.7193414 (M=  44) - s=0.0100\n",
      " 120 - L= 0.9809925 - Gamma=41.7363712 (M=  44) - s=0.0100\n",
      " 121 - L= 0.9809989 - Gamma=41.7390711 (M=  44) - s=0.0100\n",
      " 122 - L= 0.9810033 - Gamma=41.7399589 (M=  44) - s=0.0100\n",
      " 123 - L= 0.9810070 - Gamma=41.7545186 (M=  44) - s=0.0100\n",
      " 124 - L= 0.9810102 - Gamma=41.7551937 (M=  44) - s=0.0100\n",
      " 125 - L= 0.9810132 - Gamma=41.7551436 (M=  44) - s=0.0100\n",
      " 126 - L= 0.9810157 - Gamma=41.7492108 (M=  44) - s=0.0100\n",
      " 127 - L= 0.9810194 - Gamma=41.7619866 (M=  44) - s=0.0100\n",
      " 128 - L= 0.9810218 - Gamma=41.7622097 (M=  44) - s=0.0100\n",
      " 129 - L= 0.9810242 - Gamma=41.7621889 (M=  44) - s=0.0100\n",
      " 130 - L= 0.9810265 - Gamma=41.7733069 (M=  44) - s=0.0100\n",
      " 131 - L= 0.9810294 - Gamma=41.7614607 (M=  44) - s=0.0100\n",
      " 132 - L= 0.9810320 - Gamma=41.7609416 (M=  44) - s=0.0100\n",
      " 133 - L= 0.9810341 - Gamma=41.7609548 (M=  44) - s=0.0100\n",
      " 134 - L= 0.9810355 - Gamma=41.7609397 (M=  44) - s=0.0100\n",
      " 135 - L= 0.9810370 - Gamma=41.7609461 (M=  44) - s=0.0100\n",
      " 136 - L= 0.9810384 - Gamma=41.7687798 (M=  44) - s=0.0100\n",
      " 137 - L= 0.9810403 - Gamma=41.7634965 (M=  44) - s=0.0100\n",
      " 138 - L= 0.9810422 - Gamma=41.7735053 (M=  44) - s=0.0100\n",
      " 139 - L= 0.9810436 - Gamma=41.7734357 (M=  44) - s=0.0100\n",
      " 140 - L= 0.9810450 - Gamma=41.7809839 (M=  44) - s=0.0100\n",
      " 141 - L= 0.9810465 - Gamma=41.7807823 (M=  44) - s=0.0100\n",
      " 142 - L= 0.9810477 - Gamma=41.7807784 (M=  44) - s=0.0100\n",
      " 143 - L= 0.9810490 - Gamma=41.7805127 (M=  44) - s=0.0100\n",
      " 144 - L= 0.9810503 - Gamma=41.7805171 (M=  44) - s=0.0100\n",
      " 145 - L= 0.9810511 - Gamma=41.7771055 (M=  44) - s=0.0100\n",
      " 146 - L= 0.9810518 - Gamma=41.7833643 (M=  44) - s=0.0100\n",
      " 147 - L= 0.9810526 - Gamma=41.7888417 (M=  44) - s=0.0100\n",
      " 148 - L= 0.9810533 - Gamma=41.7887489 (M=  44) - s=0.0100\n",
      " 149 - L= 0.9810539 - Gamma=41.7887127 (M=  44) - s=0.0100\n",
      " 150 - L= 0.9810545 - Gamma=41.7887389 (M=  44) - s=0.0100\n",
      " 151 - L= 0.9810550 - Gamma=41.7887131 (M=  44) - s=0.0100\n",
      " 152 - L= 0.9810554 - Gamma=41.7887193 (M=  44) - s=0.0100\n",
      " 153 - L= 0.9810559 - Gamma=41.7887191 (M=  44) - s=0.0100\n",
      " 154 - L= 0.9810563 - Gamma=41.7885012 (M=  44) - s=0.0100\n",
      " 155 - L= 0.9810568 - Gamma=41.7837104 (M=  44) - s=0.0100\n",
      " 156 - L= 0.9810572 - Gamma=41.7838083 (M=  44) - s=0.0100\n",
      " 157 - L= 0.9810578 - Gamma=41.7889375 (M=  44) - s=0.0100\n",
      " 158 - L= 0.9810582 - Gamma=41.7862568 (M=  44) - s=0.0100\n",
      " 159 - L= 0.9810589 - Gamma=41.7914354 (M=  44) - s=0.0100\n",
      " 160 - L= 0.9810593 - Gamma=41.7914052 (M=  44) - s=0.0100\n",
      " 161 - L= 0.9810597 - Gamma=41.7914062 (M=  44) - s=0.0100\n",
      " 162 - L= 0.9810600 - Gamma=41.7953266 (M=  44) - s=0.0100\n",
      " 163 - L= 0.9810603 - Gamma=41.7955669 (M=  44) - s=0.0100\n",
      " 164 - L= 0.9810607 - Gamma=41.7955714 (M=  44) - s=0.0100\n",
      " 165 - L= 0.9810609 - Gamma=41.7955632 (M=  44) - s=0.0100\n",
      " 166 - L= 0.9810612 - Gamma=41.7955753 (M=  44) - s=0.0100\n",
      " 167 - L= 0.9810614 - Gamma=41.7937141 (M=  44) - s=0.0100\n",
      " 168 - L= 0.9810616 - Gamma=41.7942081 (M=  44) - s=0.0100\n",
      " 169 - L= 0.9810619 - Gamma=41.7972279 (M=  44) - s=0.0100\n",
      " 170 - L= 0.9810620 - Gamma=41.7972256 (M=  44) - s=0.0100\n",
      " 171 - L= 0.9810622 - Gamma=41.7972271 (M=  44) - s=0.0100\n",
      " 172 - L= 0.9810623 - Gamma=41.7971065 (M=  44) - s=0.0100\n",
      " 173 - L= 0.9810625 - Gamma=41.7996824 (M=  44) - s=0.0100\n",
      " 174 - L= 0.9810626 - Gamma=41.7995965 (M=  44) - s=0.0100\n",
      " 175 - L= 0.9810627 - Gamma=41.7970158 (M=  44) - s=0.0100\n",
      " 176 - L= 0.9810628 - Gamma=41.7970155 (M=  44) - s=0.0100\n",
      " 177 - L= 0.9810629 - Gamma=41.7970071 (M=  44) - s=0.0100\n",
      " 178 - L= 0.9810630 - Gamma=41.7970462 (M=  44) - s=0.0100\n",
      " 179 - L= 0.9810631 - Gamma=41.7987717 (M=  44) - s=0.0100\n",
      " 180 - L= 0.9810632 - Gamma=41.7973365 (M=  44) - s=0.0100\n",
      " 181 - L= 0.9810633 - Gamma=41.7996665 (M=  44) - s=0.0100\n",
      " 182 - L= 0.9810634 - Gamma=41.8014041 (M=  44) - s=0.0100\n",
      " 183 - L= 0.9810635 - Gamma=41.8013552 (M=  44) - s=0.0100\n",
      " 184 - L= 0.9810635 - Gamma=41.8014550 (M=  44) - s=0.0100\n",
      " 185 - L= 0.9810636 - Gamma=41.8014535 (M=  44) - s=0.0100\n",
      " 186 - L= 0.9810636 - Gamma=41.8005841 (M=  44) - s=0.0100\n",
      " 187 - L= 0.9810637 - Gamma=41.8005600 (M=  44) - s=0.0100\n",
      " 188 - L= 0.9810637 - Gamma=41.8019249 (M=  44) - s=0.0100\n",
      " 189 - L= 0.9810638 - Gamma=41.8018586 (M=  44) - s=0.0100\n",
      " 190 - L= 0.9810638 - Gamma=41.8031085 (M=  44) - s=0.0100\n",
      " 191 - L= 0.9810638 - Gamma=41.8031154 (M=  44) - s=0.0100\n",
      " 192 - L= 0.9810639 - Gamma=41.8031207 (M=  44) - s=0.0100\n",
      " 193 - L= 0.9810639 - Gamma=41.8031170 (M=  44) - s=0.0100\n",
      " 194 - L= 0.9810639 - Gamma=41.8031181 (M=  44) - s=0.0100\n",
      " 195 - L= 0.9810640 - Gamma=41.8031158 (M=  44) - s=0.0100\n",
      " 196 - L= 0.9810640 - Gamma=41.8031098 (M=  44) - s=0.0100\n",
      " 197 - L= 0.9810640 - Gamma=41.8031002 (M=  44) - s=0.0100\n",
      " 198 - L= 0.9810640 - Gamma=41.8024808 (M=  44) - s=0.0100\n",
      " 199 - L= 0.9810641 - Gamma=41.8034967 (M=  44) - s=0.0100\n",
      " 200 - L= 0.9810641 - Gamma=41.8034944 (M=  44) - s=0.0100\n",
      "Initial alpha = [[ 0.12628492]]\n",
      "   1 - L=-1004.0899991 - Gamma= 1.9999517 (M=   2) - s=0.0100\n",
      "   2 - L=-891.7387620 - Gamma= 2.9998547 (M=   3) - s=0.0100\n",
      "   3 - L=-773.2075170 - Gamma= 3.9997627 (M=   4) - s=0.0100\n",
      "   4 - L=-663.6518193 - Gamma= 4.9996620 (M=   5) - s=0.0100\n",
      "   5 - L=-546.1527994 - Gamma= 5.9995615 (M=   6) - s=0.0100\n",
      "   6 - L=-476.5165066 - Gamma= 6.9994022 (M=   7) - s=0.0100\n",
      "   7 - L=-415.9919779 - Gamma= 7.9992219 (M=   8) - s=0.0100\n",
      "   8 - L=-360.3793940 - Gamma= 8.9990236 (M=   9) - s=0.0100\n",
      "   9 - L=-317.2745824 - Gamma= 9.9987685 (M=  10) - s=0.0100\n",
      "  10 - L=-276.6713562 - Gamma=10.9984923 (M=  11) - s=0.0100\n",
      "  11 - L=-223.1630710 - Gamma=11.9982601 (M=  12) - s=0.0100\n",
      "  12 - L=-189.0751897 - Gamma=12.9979377 (M=  13) - s=0.0100\n",
      "  13 - L=-158.9459957 - Gamma=13.9975664 (M=  14) - s=0.0100\n",
      "  14 - L=-137.3731297 - Gamma=14.9970555 (M=  15) - s=0.0100\n",
      "  15 - L=-112.9903288 - Gamma=15.9965754 (M=  16) - s=0.0100\n",
      "  16 - L=-98.6557175 - Gamma=16.9957987 (M=  17) - s=0.0100\n",
      "  17 - L=-88.5247303 - Gamma=17.9946419 (M=  18) - s=0.0100\n",
      "  18 - L=-78.7207664 - Gamma=18.9934969 (M=  19) - s=0.0100\n",
      "  19 - L=-68.4262134 - Gamma=19.9924052 (M=  20) - s=0.0100\n",
      "  20 - L=-57.0096520 - Gamma=20.9914271 (M=  21) - s=0.0100\n",
      "  21 - L=-46.2352331 - Gamma=21.9903538 (M=  22) - s=0.0100\n",
      "  22 - L=-39.3269769 - Gamma=22.9887402 (M=  23) - s=0.0100\n",
      "  23 - L=-32.8641621 - Gamma=23.9868951 (M=  24) - s=0.0100\n",
      "  24 - L=-25.7825503 - Gamma=24.9852903 (M=  25) - s=0.0100\n",
      "  25 - L=-21.0444929 - Gamma=25.9829948 (M=  26) - s=0.0100\n",
      "  26 - L=-16.5670133 - Gamma=26.9802739 (M=  27) - s=0.0100\n",
      "  27 - L=-13.0086526 - Gamma=27.9768844 (M=  28) - s=0.0100\n",
      "  28 - L=-10.3506999 - Gamma=28.9726655 (M=  29) - s=0.0100\n",
      "  29 - L=-8.0001919 - Gamma=29.9678970 (M=  30) - s=0.0100\n",
      "  30 - L=-6.1497541 - Gamma=30.9619288 (M=  31) - s=0.0100\n",
      "  31 - L=-4.4671498 - Gamma=31.9543069 (M=  32) - s=0.0100\n",
      "  32 - L=-2.8511993 - Gamma=32.9473500 (M=  33) - s=0.0100\n",
      "  33 - L=-1.8739914 - Gamma=33.9328130 (M=  34) - s=0.0100\n",
      "  34 - L=-1.0657254 - Gamma=34.9180152 (M=  35) - s=0.0100\n",
      "  35 - L=-0.3183469 - Gamma=35.9023635 (M=  36) - s=0.0100\n",
      "  36 - L= 0.2161765 - Gamma=36.8802817 (M=  37) - s=0.0100\n",
      "  37 - L= 0.7100892 - Gamma=37.8570451 (M=  38) - s=0.0100\n",
      "  38 - L= 0.9932392 - Gamma=38.8217120 (M=  39) - s=0.0100\n",
      "  39 - L= 1.0379967 - Gamma=39.6734334 (M=  40) - s=0.0100\n",
      "  40 - L= 1.0661656 - Gamma=40.4730263 (M=  41) - s=0.0100\n",
      "  41 - L= 1.0757697 - Gamma=40.4745316 (M=  41) - s=0.0100\n",
      "  42 - L= 1.0840003 - Gamma=40.4747935 (M=  41) - s=0.0100\n",
      "  43 - L= 1.0893744 - Gamma=40.4782323 (M=  41) - s=0.0100\n",
      "  44 - L= 1.0943695 - Gamma=40.4739713 (M=  41) - s=0.0100\n",
      "  45 - L= 1.0993286 - Gamma=40.4704993 (M=  41) - s=0.0100\n",
      "  46 - L= 1.1038699 - Gamma=40.4699517 (M=  41) - s=0.0100\n",
      "  47 - L= 1.1080900 - Gamma=40.4724344 (M=  41) - s=0.0100\n",
      "  48 - L= 1.1118825 - Gamma=40.4726902 (M=  41) - s=0.0100\n",
      "  49 - L= 1.1155356 - Gamma=40.4709251 (M=  41) - s=0.0100\n",
      "  50 - L= 1.1188346 - Gamma=40.4492790 (M=  41) - s=0.0100\n",
      "  51 - L= 1.1221330 - Gamma=40.4502954 (M=  41) - s=0.0100\n",
      "  52 - L= 1.1253854 - Gamma=40.4505524 (M=  41) - s=0.0100\n",
      "  53 - L= 1.1283933 - Gamma=40.4549512 (M=  41) - s=0.0100\n",
      "  54 - L= 1.1306175 - Gamma=40.4551509 (M=  41) - s=0.0100\n",
      "  55 - L= 1.1326858 - Gamma=40.4556933 (M=  41) - s=0.0100\n",
      "  56 - L= 1.1346474 - Gamma=40.4556378 (M=  41) - s=0.0100\n",
      "  57 - L= 1.1360811 - Gamma=40.4561819 (M=  41) - s=0.0100\n",
      "  58 - L= 1.1374313 - Gamma=40.4562324 (M=  41) - s=0.0100\n",
      "  59 - L= 1.1386394 - Gamma=40.4502048 (M=  41) - s=0.0100\n",
      "  60 - L= 1.1398474 - Gamma=40.4501869 (M=  41) - s=0.0100\n",
      "  61 - L= 1.1409216 - Gamma=40.4500869 (M=  41) - s=0.0100\n",
      "  62 - L= 1.1419436 - Gamma=40.4501665 (M=  41) - s=0.0100\n",
      "  63 - L= 1.1429372 - Gamma=40.4500077 (M=  41) - s=0.0100\n",
      "  64 - L= 1.1436779 - Gamma=40.4498656 (M=  41) - s=0.0100\n",
      "  65 - L= 1.1442122 - Gamma=40.4507728 (M=  41) - s=0.0100\n",
      "  66 - L= 1.1446513 - Gamma=40.4509723 (M=  41) - s=0.0100\n",
      "  67 - L= 1.1449322 - Gamma=40.4576361 (M=  41) - s=0.0100\n",
      "  68 - L= 1.1451899 - Gamma=40.4576612 (M=  41) - s=0.0100\n",
      "  69 - L= 1.1453922 - Gamma=40.4264246 (M=  41) - s=0.0100\n",
      "  70 - L= 1.1455421 - Gamma=40.4268426 (M=  41) - s=0.0100\n",
      "  71 - L= 1.1456767 - Gamma=40.4265677 (M=  41) - s=0.0100\n",
      "  72 - L= 1.1457883 - Gamma=40.4280780 (M=  41) - s=0.0100\n",
      "  73 - L= 1.1458941 - Gamma=40.4282221 (M=  41) - s=0.0100\n",
      "  74 - L= 1.1459572 - Gamma=40.4280020 (M=  41) - s=0.0100\n",
      "  75 - L= 1.1460003 - Gamma=40.4263287 (M=  41) - s=0.0100\n",
      "  76 - L= 1.1460361 - Gamma=40.4278560 (M=  41) - s=0.0100\n",
      "  77 - L= 1.1460696 - Gamma=40.4285544 (M=  41) - s=0.0100\n",
      "  78 - L= 1.1461021 - Gamma=40.4300177 (M=  41) - s=0.0100\n",
      "  79 - L= 1.1461311 - Gamma=40.4300436 (M=  41) - s=0.0100\n",
      "  80 - L= 1.1461323 - Gamma=40.4301298 (M=  41) - s=0.0100\n",
      "  81 - L= 1.1461333 - Gamma=40.4300458 (M=  41) - s=0.0100\n",
      "  82 - L= 1.1461336 - Gamma=40.4297854 (M=  41) - s=0.0100\n",
      "  83 - L= 1.1461337 - Gamma=40.4288711 (M=  41) - s=0.0100\n",
      "  84 - L= 1.1461338 - Gamma=40.4288595 (M=  41) - s=0.0100\n",
      "  85 - L= 1.1461339 - Gamma=40.4288626 (M=  41) - s=0.0100\n",
      "  86 - L= 1.1461340 - Gamma=40.4288595 (M=  41) - s=0.0100\n",
      "  87 - L= 1.1461341 - Gamma=40.4287625 (M=  41) - s=0.0100\n",
      "  88 - L= 1.1461341 - Gamma=40.4287716 (M=  41) - s=0.0100\n",
      "  89 - L= 1.1461342 - Gamma=40.4287555 (M=  41) - s=0.0100\n",
      "  90 - L= 1.1461342 - Gamma=40.4287586 (M=  41) - s=0.0100\n",
      "  91 - L= 1.1461342 - Gamma=40.4287589 (M=  41) - s=0.0100\n",
      "  92 - L= 1.1461343 - Gamma=40.4287544 (M=  41) - s=0.0100\n",
      "  93 - L= 1.1461343 - Gamma=40.4287557 (M=  41) - s=0.0100\n",
      "  94 - L= 1.1461343 - Gamma=40.4287866 (M=  41) - s=0.0100\n",
      "  95 - L= 1.1461343 - Gamma=40.4288102 (M=  41) - s=0.0100\n",
      "  96 - L= 1.1461343 - Gamma=40.4288295 (M=  41) - s=0.0100\n",
      "  97 - L= 1.1461343 - Gamma=40.4288191 (M=  41) - s=0.0100\n",
      "  98 - L= 1.1461343 - Gamma=40.4288191 (M=  41) - s=0.0100\n",
      "Stopping at iteration 98 - max_delta_ml=2.39821369855237e-07\n",
      "L=1.1461343187180026 - Gamma=40.42881911649431 (M=41) - s=0.01\n",
      "Initial alpha = [[ 0.130648]]\n",
      "   1 - L=-1156.5311980 - Gamma= 1.9999269 (M=   2) - s=0.0100\n",
      "   2 - L=-1016.2779214 - Gamma= 2.9998493 (M=   3) - s=0.0100\n",
      "   3 - L=-905.3302483 - Gamma= 3.9997514 (M=   4) - s=0.0100\n",
      "   4 - L=-806.8541152 - Gamma= 4.9996400 (M=   5) - s=0.0100\n",
      "   5 - L=-722.4892930 - Gamma= 5.9995068 (M=   6) - s=0.0100\n",
      "   6 - L=-625.6404886 - Gamma= 6.9993924 (M=   7) - s=0.0100\n",
      "   7 - L=-553.3519270 - Gamma= 7.9992416 (M=   8) - s=0.0100\n",
      "   8 - L=-490.9578680 - Gamma= 8.9990669 (M=   9) - s=0.0100\n",
      "   9 - L=-440.7336268 - Gamma= 9.9988495 (M=  10) - s=0.0100\n",
      "  10 - L=-387.7409722 - Gamma=10.9986439 (M=  11) - s=0.0100\n",
      "  11 - L=-339.0442213 - Gamma=11.9984148 (M=  12) - s=0.0100\n",
      "  12 - L=-297.7099281 - Gamma=12.9981432 (M=  13) - s=0.0100\n",
      "  13 - L=-254.2886952 - Gamma=13.9978783 (M=  14) - s=0.0100\n",
      "  14 - L=-222.6584891 - Gamma=14.9975289 (M=  15) - s=0.0100\n",
      "  15 - L=-187.5368294 - Gamma=15.9972112 (M=  16) - s=0.0100\n",
      "  16 - L=-161.9098264 - Gamma=16.9967791 (M=  17) - s=0.0100\n",
      "  17 - L=-135.8420356 - Gamma=17.9963347 (M=  18) - s=0.0100\n",
      "  18 - L=-111.2682895 - Gamma=18.9958809 (M=  19) - s=0.0100\n",
      "  19 - L=-92.4890557 - Gamma=19.9952809 (M=  20) - s=0.0100\n",
      "  20 - L=-78.3365443 - Gamma=20.9944936 (M=  21) - s=0.0100\n",
      "  21 - L=-67.8782676 - Gamma=21.9933793 (M=  22) - s=0.0100\n",
      "  22 - L=-54.1323568 - Gamma=22.9925287 (M=  23) - s=0.0100\n",
      "  23 - L=-45.0463130 - Gamma=23.9913244 (M=  24) - s=0.0100\n",
      "  24 - L=-37.2345945 - Gamma=24.9898788 (M=  25) - s=0.0100\n",
      "  25 - L=-30.6534030 - Gamma=25.9880489 (M=  26) - s=0.0100\n",
      "  26 - L=-26.7403847 - Gamma=26.9851614 (M=  27) - s=0.0100\n",
      "  27 - L=-22.0359143 - Gamma=27.9823709 (M=  28) - s=0.0100\n",
      "  28 - L=-15.8558550 - Gamma=28.9802951 (M=  29) - s=0.0100\n",
      "  29 - L=-10.6969400 - Gamma=29.9780064 (M=  30) - s=0.0100\n",
      "  30 - L=-5.7302028 - Gamma=30.9756917 (M=  31) - s=0.0100\n",
      "  31 - L=-3.2997373 - Gamma=31.9711163 (M=  32) - s=0.0100\n",
      "  32 - L=-1.9426719 - Gamma=32.9630341 (M=  33) - s=0.0100\n",
      "  33 - L=-0.7533824 - Gamma=33.9542695 (M=  34) - s=0.0100\n",
      "  34 - L= 0.2170212 - Gamma=34.9430363 (M=  35) - s=0.0100\n",
      "  35 - L= 0.9332379 - Gamma=35.9286854 (M=  36) - s=0.0100\n",
      "  36 - L= 1.0022479 - Gamma=36.8221520 (M=  37) - s=0.0100\n",
      "  37 - L= 1.0491469 - Gamma=37.6821180 (M=  38) - s=0.0100\n",
      "  38 - L= 1.0635255 - Gamma=37.6847505 (M=  38) - s=0.0100\n",
      "  39 - L= 1.0736561 - Gamma=37.6857484 (M=  38) - s=0.0100\n",
      "  40 - L= 1.0823699 - Gamma=38.3139937 (M=  39) - s=0.0100\n",
      "  41 - L= 1.0880075 - Gamma=38.3099823 (M=  39) - s=0.0100\n",
      "  42 - L= 1.0926367 - Gamma=38.3109927 (M=  39) - s=0.0100\n",
      "  43 - L= 1.0964140 - Gamma=38.3097627 (M=  39) - s=0.0100\n",
      "  44 - L= 1.1001536 - Gamma=38.3100553 (M=  39) - s=0.0100\n",
      "  45 - L= 1.1038051 - Gamma=38.3101253 (M=  39) - s=0.0100\n",
      "  46 - L= 1.1069978 - Gamma=38.3093306 (M=  39) - s=0.0100\n",
      "  47 - L= 1.1098417 - Gamma=38.3089828 (M=  39) - s=0.0100\n",
      "  48 - L= 1.1124929 - Gamma=38.3091331 (M=  39) - s=0.0100\n",
      "  49 - L= 1.1149828 - Gamma=38.3092107 (M=  39) - s=0.0100\n",
      "  50 - L= 1.1169443 - Gamma=38.3090329 (M=  39) - s=0.0100\n",
      "  51 - L= 1.1187170 - Gamma=38.7092956 (M=  40) - s=0.0100\n",
      "  52 - L= 1.1203610 - Gamma=38.7094440 (M=  40) - s=0.0100\n",
      "  53 - L= 1.1218406 - Gamma=38.7104308 (M=  40) - s=0.0100\n",
      "  54 - L= 1.1232384 - Gamma=38.7114393 (M=  40) - s=0.0100\n",
      "  55 - L= 1.1245133 - Gamma=38.7122174 (M=  40) - s=0.0100\n",
      "  56 - L= 1.1257324 - Gamma=38.7125298 (M=  40) - s=0.0100\n",
      "  57 - L= 1.1267245 - Gamma=38.7125932 (M=  40) - s=0.0100\n",
      "  58 - L= 1.1277122 - Gamma=38.6280283 (M=  40) - s=0.0100\n",
      "  59 - L= 1.1285640 - Gamma=38.6280602 (M=  40) - s=0.0100\n",
      "  60 - L= 1.1293801 - Gamma=38.6280059 (M=  40) - s=0.0100\n",
      "  61 - L= 1.1300600 - Gamma=38.6280328 (M=  40) - s=0.0100\n",
      "  62 - L= 1.1305405 - Gamma=38.6281455 (M=  40) - s=0.0100\n",
      "  63 - L= 1.1309118 - Gamma=38.6280076 (M=  40) - s=0.0100\n",
      "  64 - L= 1.1311639 - Gamma=38.6280118 (M=  40) - s=0.0100\n",
      "  65 - L= 1.1314114 - Gamma=38.6260528 (M=  40) - s=0.0100\n",
      "  66 - L= 1.1316573 - Gamma=38.6281399 (M=  40) - s=0.0100\n",
      "  67 - L= 1.1318704 - Gamma=38.6285579 (M=  40) - s=0.0100\n",
      "  68 - L= 1.1320773 - Gamma=38.6283469 (M=  40) - s=0.0100\n",
      "  69 - L= 1.1322728 - Gamma=38.6280322 (M=  40) - s=0.0100\n",
      "  70 - L= 1.1324395 - Gamma=38.6280664 (M=  40) - s=0.0100\n",
      "  71 - L= 1.1325594 - Gamma=38.6280117 (M=  40) - s=0.0100\n",
      "  72 - L= 1.1326757 - Gamma=38.6299382 (M=  40) - s=0.0100\n",
      "  73 - L= 1.1327572 - Gamma=38.6309289 (M=  40) - s=0.0100\n",
      "  74 - L= 1.1328255 - Gamma=38.6308501 (M=  40) - s=0.0100\n",
      "  75 - L= 1.1328799 - Gamma=38.6308668 (M=  40) - s=0.0100\n",
      "  76 - L= 1.1329210 - Gamma=38.6582605 (M=  40) - s=0.0100\n",
      "  77 - L= 1.1329417 - Gamma=38.6646901 (M=  40) - s=0.0100\n",
      "  78 - L= 1.1329531 - Gamma=38.6644560 (M=  40) - s=0.0100\n",
      "  79 - L= 1.1329579 - Gamma=38.6814116 (M=  40) - s=0.0100\n",
      "  80 - L= 1.1329619 - Gamma=38.6815654 (M=  40) - s=0.0100\n",
      "  81 - L= 1.1329656 - Gamma=38.6815372 (M=  40) - s=0.0100\n",
      "  82 - L= 1.1329673 - Gamma=38.6774640 (M=  40) - s=0.0100\n",
      "  83 - L= 1.1329680 - Gamma=38.6809533 (M=  40) - s=0.0100\n",
      "  84 - L= 1.1329687 - Gamma=38.6809379 (M=  40) - s=0.0100\n",
      "  85 - L= 1.1329692 - Gamma=38.6809328 (M=  40) - s=0.0100\n",
      "  86 - L= 1.1329695 - Gamma=38.6809225 (M=  40) - s=0.0100\n",
      "  87 - L= 1.1329698 - Gamma=38.6809269 (M=  40) - s=0.0100\n",
      "  88 - L= 1.1329700 - Gamma=38.6809158 (M=  40) - s=0.0100\n",
      "  89 - L= 1.1329701 - Gamma=38.6809148 (M=  40) - s=0.0100\n",
      "  90 - L= 1.1329703 - Gamma=38.6839309 (M=  40) - s=0.0100\n",
      "  91 - L= 1.1329704 - Gamma=38.6839295 (M=  40) - s=0.0100\n",
      "  92 - L= 1.1329705 - Gamma=38.6830116 (M=  40) - s=0.0100\n",
      "  93 - L= 1.1329705 - Gamma=38.6830118 (M=  40) - s=0.0100\n",
      "  94 - L= 1.1329706 - Gamma=38.6830163 (M=  40) - s=0.0100\n",
      "  95 - L= 1.1329706 - Gamma=38.6830166 (M=  40) - s=0.0100\n",
      "  96 - L= 1.1329707 - Gamma=38.6830120 (M=  40) - s=0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bram van Es\\DEV\\RexR\\rvm.py:289: RuntimeWarning: invalid value encountered in log\n",
      "  change = np.abs(np.log(newAlpha) - np.log(self.Alpha[j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  97 - L= 1.1329707 - Gamma=38.6830164 (M=  40) - s=0.0100\n",
      "  98 - L= 1.1329707 - Gamma=38.6830129 (M=  40) - s=0.0100\n",
      "  99 - L= 1.1329707 - Gamma=38.6834101 (M=  40) - s=0.0100\n",
      " 100 - L= 1.1329707 - Gamma=38.6834108 (M=  40) - s=0.0100\n",
      " 101 - L= 1.1329707 - Gamma=38.6834099 (M=  40) - s=0.0100\n",
      " 102 - L= 1.1329707 - Gamma=38.6834104 (M=  40) - s=0.0100\n",
      " 103 - L= 1.1329708 - Gamma=38.6834110 (M=  40) - s=0.0100\n",
      " 104 - L= 1.1329708 - Gamma=38.6834108 (M=  40) - s=0.0100\n",
      " 105 - L= 1.1329708 - Gamma=38.6834116 (M=  40) - s=0.0100\n",
      " 106 - L= 1.1329708 - Gamma=38.6834116 (M=  40) - s=0.0100\n",
      "Stopping at iteration 106 - max_delta_ml=2.469800984540799e-07\n",
      "L=1.1329707635770077 - Gamma=38.68341159127109 (M=40) - s=0.01\n",
      "Initial alpha = [[ 0.11807905]]\n",
      "   1 - L=-1077.0220858 - Gamma= 1.9999150 (M=   2) - s=0.0100\n",
      "   2 - L=-910.2413932 - Gamma= 2.9998509 (M=   3) - s=0.0100\n",
      "   3 - L=-806.7905169 - Gamma= 3.9997479 (M=   4) - s=0.0100\n",
      "   4 - L=-699.6665609 - Gamma= 4.9996485 (M=   5) - s=0.0100\n",
      "   5 - L=-592.7438864 - Gamma= 5.9995485 (M=   6) - s=0.0100\n",
      "   6 - L=-511.0782177 - Gamma= 6.9994182 (M=   7) - s=0.0100\n",
      "   7 - L=-437.1573074 - Gamma= 7.9992716 (M=   8) - s=0.0100\n",
      "   8 - L=-365.7199072 - Gamma= 8.9991217 (M=   9) - s=0.0100\n",
      "   9 - L=-320.8582138 - Gamma= 9.9988824 (M=  10) - s=0.0100\n",
      "  10 - L=-278.7632728 - Gamma=10.9986253 (M=  11) - s=0.0100\n",
      "  11 - L=-249.7585559 - Gamma=11.9982559 (M=  12) - s=0.0100\n",
      "  12 - L=-221.3479389 - Gamma=12.9978726 (M=  13) - s=0.0100\n",
      "  13 - L=-198.1009877 - Gamma=13.9974110 (M=  14) - s=0.0100\n",
      "  14 - L=-177.5090353 - Gamma=14.9968848 (M=  15) - s=0.0100\n",
      "  15 - L=-151.0728272 - Gamma=15.9964379 (M=  16) - s=0.0100\n",
      "  16 - L=-128.8297879 - Gamma=16.9959406 (M=  17) - s=0.0100\n",
      "  17 - L=-112.2334561 - Gamma=17.9952625 (M=  18) - s=0.0100\n",
      "  18 - L=-98.8976825 - Gamma=18.9944405 (M=  19) - s=0.0100\n",
      "  19 - L=-86.2066113 - Gamma=19.9935668 (M=  20) - s=0.0100\n",
      "  20 - L=-71.0002384 - Gamma=20.9928227 (M=  21) - s=0.0100\n",
      "  21 - L=-58.9242506 - Gamma=21.9919192 (M=  22) - s=0.0100\n",
      "  22 - L=-47.7525258 - Gamma=22.9909428 (M=  23) - s=0.0100\n",
      "  23 - L=-38.7514431 - Gamma=23.9896640 (M=  24) - s=0.0100\n",
      "  24 - L=-30.0714987 - Gamma=24.9883188 (M=  25) - s=0.0100\n",
      "  25 - L=-22.4742217 - Gamma=25.9864212 (M=  26) - s=0.0100\n",
      "  26 - L=-17.8067535 - Gamma=26.9840274 (M=  27) - s=0.0100\n",
      "  27 - L=-13.4384610 - Gamma=27.9814346 (M=  28) - s=0.0100\n",
      "  28 - L=-10.6282899 - Gamma=28.9774104 (M=  29) - s=0.0100\n",
      "  29 - L=-8.7063868 - Gamma=29.9716428 (M=  30) - s=0.0100\n",
      "  30 - L=-6.8683867 - Gamma=30.9655499 (M=  31) - s=0.0100\n",
      "  31 - L=-5.1334071 - Gamma=31.9591431 (M=  32) - s=0.0100\n",
      "  32 - L=-3.7211080 - Gamma=32.9508864 (M=  33) - s=0.0100\n",
      "  33 - L=-2.1207168 - Gamma=33.9422269 (M=  34) - s=0.0100\n",
      "  34 - L=-0.9692552 - Gamma=34.9324051 (M=  35) - s=0.0100\n",
      "  35 - L=-0.2238011 - Gamma=35.9168821 (M=  36) - s=0.0100\n",
      "  36 - L= 0.3384857 - Gamma=36.8983423 (M=  37) - s=0.0100\n",
      "  37 - L= 0.7131716 - Gamma=37.8708157 (M=  38) - s=0.0100\n",
      "  38 - L= 0.8186149 - Gamma=38.7923563 (M=  39) - s=0.0100\n",
      "  39 - L= 0.8612385 - Gamma=38.6952765 (M=  39) - s=0.0100\n",
      "  40 - L= 0.8917399 - Gamma=39.4925919 (M=  40) - s=0.0100\n",
      "  41 - L= 0.9139293 - Gamma=39.4937461 (M=  40) - s=0.0100\n",
      "  42 - L= 0.9359904 - Gamma=39.4943270 (M=  40) - s=0.0100\n",
      "  43 - L= 0.9563675 - Gamma=40.2228031 (M=  41) - s=0.0100\n",
      "  44 - L= 0.9724363 - Gamma=40.2288890 (M=  41) - s=0.0100\n",
      "  45 - L= 0.9852071 - Gamma=40.2297846 (M=  41) - s=0.0100\n",
      "  46 - L= 0.9950893 - Gamma=40.2352310 (M=  41) - s=0.0100\n",
      "  47 - L= 1.0048356 - Gamma=40.2360869 (M=  41) - s=0.0100\n",
      "  48 - L= 1.0144377 - Gamma=40.2376333 (M=  41) - s=0.0100\n",
      "  49 - L= 1.0233274 - Gamma=40.2391835 (M=  41) - s=0.0100\n",
      "  50 - L= 1.0307938 - Gamma=40.2199584 (M=  41) - s=0.0100\n",
      "  51 - L= 1.0382399 - Gamma=40.2200252 (M=  41) - s=0.0100\n",
      "  52 - L= 1.0447953 - Gamma=40.8126044 (M=  42) - s=0.0100\n",
      "  53 - L= 1.0508894 - Gamma=40.8128245 (M=  42) - s=0.0100\n",
      "  54 - L= 1.0565947 - Gamma=40.8109626 (M=  42) - s=0.0100\n",
      "  55 - L= 1.0620411 - Gamma=40.8163756 (M=  42) - s=0.0100\n",
      "  56 - L= 1.0663171 - Gamma=40.8160608 (M=  42) - s=0.0100\n",
      "  57 - L= 1.0705427 - Gamma=40.8162514 (M=  42) - s=0.0100\n",
      "  58 - L= 1.0746052 - Gamma=40.8152277 (M=  42) - s=0.0100\n",
      "  59 - L= 1.0785179 - Gamma=40.8158129 (M=  42) - s=0.0100\n",
      "  60 - L= 1.0819185 - Gamma=40.8160910 (M=  42) - s=0.0100\n",
      "  61 - L= 1.0849040 - Gamma=40.8163930 (M=  42) - s=0.0100\n",
      "  62 - L= 1.0875853 - Gamma=40.8701210 (M=  42) - s=0.0100\n",
      "  63 - L= 1.0899831 - Gamma=40.8702056 (M=  42) - s=0.0100\n",
      "  64 - L= 1.0921676 - Gamma=40.7774160 (M=  42) - s=0.0100\n",
      "  65 - L= 1.0941711 - Gamma=40.7771487 (M=  42) - s=0.0100\n",
      "  66 - L= 1.0957662 - Gamma=40.7776313 (M=  42) - s=0.0100\n",
      "  67 - L= 1.0970205 - Gamma=40.7833455 (M=  42) - s=0.0100\n",
      "  68 - L= 1.0980025 - Gamma=40.7832322 (M=  42) - s=0.0100\n",
      "  69 - L= 1.0988978 - Gamma=40.7831193 (M=  42) - s=0.0100\n",
      "  70 - L= 1.0997167 - Gamma=40.7831251 (M=  42) - s=0.0100\n",
      "  71 - L= 1.1005258 - Gamma=40.7839352 (M=  42) - s=0.0100\n",
      "  72 - L= 1.1011710 - Gamma=40.7839566 (M=  42) - s=0.0100\n",
      "  73 - L= 1.1018066 - Gamma=40.7868146 (M=  42) - s=0.0100\n",
      "  74 - L= 1.1023895 - Gamma=40.7885212 (M=  42) - s=0.0100\n",
      "  75 - L= 1.1029214 - Gamma=40.7888002 (M=  42) - s=0.0100\n",
      "  76 - L= 1.1034357 - Gamma=41.0410206 (M=  43) - s=0.0100\n",
      "  77 - L= 1.1038356 - Gamma=41.0409594 (M=  43) - s=0.0100\n",
      "  78 - L= 1.1041077 - Gamma=41.0403062 (M=  43) - s=0.0100\n",
      "  79 - L= 1.1042691 - Gamma=41.0407196 (M=  43) - s=0.0100\n",
      "  80 - L= 1.1043159 - Gamma=41.1234740 (M=  44) - s=0.0100\n",
      "  81 - L= 1.1043581 - Gamma=41.1250073 (M=  44) - s=0.0100\n",
      "  82 - L= 1.1043924 - Gamma=41.1228719 (M=  44) - s=0.0100\n",
      "  83 - L= 1.1044255 - Gamma=41.1364151 (M=  44) - s=0.0100\n",
      "  84 - L= 1.1044622 - Gamma=41.1178414 (M=  44) - s=0.0100\n",
      "  85 - L= 1.1044843 - Gamma=41.1059073 (M=  44) - s=0.0100\n",
      "  86 - L= 1.1045046 - Gamma=41.1064885 (M=  44) - s=0.0100\n",
      "  87 - L= 1.1045203 - Gamma=41.1512452 (M=  44) - s=0.0100\n",
      "  88 - L= 1.1045304 - Gamma=41.1524566 (M=  44) - s=0.0100\n",
      "  89 - L= 1.1045384 - Gamma=41.1233753 (M=  44) - s=0.0100\n",
      "  90 - L= 1.1045448 - Gamma=41.1234687 (M=  44) - s=0.0100\n",
      "  91 - L= 1.1045513 - Gamma=41.1291566 (M=  44) - s=0.0100\n",
      "  92 - L= 1.1045563 - Gamma=41.1219281 (M=  44) - s=0.0100\n",
      "  93 - L= 1.1045612 - Gamma=41.1219924 (M=  44) - s=0.0100\n",
      "  94 - L= 1.1045647 - Gamma=41.1423794 (M=  44) - s=0.0100\n",
      "  95 - L= 1.1045674 - Gamma=41.1380587 (M=  44) - s=0.0100\n",
      "  96 - L= 1.1045687 - Gamma=41.1380098 (M=  44) - s=0.0100\n",
      "  97 - L= 1.1045699 - Gamma=41.1381209 (M=  44) - s=0.0100\n",
      "  98 - L= 1.1045712 - Gamma=41.1405884 (M=  44) - s=0.0100\n",
      "  99 - L= 1.1045723 - Gamma=41.1371951 (M=  44) - s=0.0100\n",
      " 100 - L= 1.1045733 - Gamma=41.1370543 (M=  44) - s=0.0100\n",
      " 101 - L= 1.1045743 - Gamma=41.1366797 (M=  44) - s=0.0100\n",
      " 102 - L= 1.1045753 - Gamma=41.1472185 (M=  44) - s=0.0100\n",
      " 103 - L= 1.1045763 - Gamma=41.1463865 (M=  44) - s=0.0100\n",
      " 104 - L= 1.1045772 - Gamma=41.1463819 (M=  44) - s=0.0100\n",
      " 105 - L= 1.1045781 - Gamma=41.1464998 (M=  44) - s=0.0100\n",
      " 106 - L= 1.1045789 - Gamma=41.1465095 (M=  44) - s=0.0100\n",
      " 107 - L= 1.1045795 - Gamma=41.1465136 (M=  44) - s=0.0100\n",
      " 108 - L= 1.1045802 - Gamma=41.1465193 (M=  44) - s=0.0100\n",
      " 109 - L= 1.1045807 - Gamma=41.1465270 (M=  44) - s=0.0100\n",
      " 110 - L= 1.1045812 - Gamma=41.1391137 (M=  44) - s=0.0100\n",
      " 111 - L= 1.1045816 - Gamma=41.1389684 (M=  44) - s=0.0100\n",
      " 112 - L= 1.1045820 - Gamma=41.1368840 (M=  44) - s=0.0100\n",
      " 113 - L= 1.1045824 - Gamma=41.1351055 (M=  44) - s=0.0100\n",
      " 114 - L= 1.1045830 - Gamma=41.1367035 (M=  44) - s=0.0100\n",
      " 115 - L= 1.1045836 - Gamma=41.1450414 (M=  44) - s=0.0100\n",
      " 116 - L= 1.1045840 - Gamma=41.1450827 (M=  44) - s=0.0100\n",
      " 117 - L= 1.1045843 - Gamma=41.1450510 (M=  44) - s=0.0100\n",
      " 118 - L= 1.1045846 - Gamma=41.1450564 (M=  44) - s=0.0100\n",
      " 119 - L= 1.1045849 - Gamma=41.1434769 (M=  44) - s=0.0100\n",
      " 120 - L= 1.1045851 - Gamma=41.1434884 (M=  44) - s=0.0100\n",
      " 121 - L= 1.1045853 - Gamma=41.1434886 (M=  44) - s=0.0100\n",
      " 122 - L= 1.1045854 - Gamma=41.1434841 (M=  44) - s=0.0100\n",
      " 123 - L= 1.1045856 - Gamma=41.1431482 (M=  44) - s=0.0100\n",
      " 124 - L= 1.1045858 - Gamma=41.1410589 (M=  44) - s=0.0100\n",
      " 125 - L= 1.1045859 - Gamma=41.1410573 (M=  44) - s=0.0100\n",
      " 126 - L= 1.1045860 - Gamma=41.1410597 (M=  44) - s=0.0100\n",
      " 127 - L= 1.1045862 - Gamma=41.1447886 (M=  44) - s=0.0100\n",
      " 128 - L= 1.1045863 - Gamma=41.1454890 (M=  44) - s=0.0100\n",
      " 129 - L= 1.1045864 - Gamma=41.1453731 (M=  44) - s=0.0100\n",
      " 130 - L= 1.1045865 - Gamma=41.1445553 (M=  44) - s=0.0100\n",
      " 131 - L= 1.1045865 - Gamma=41.1435613 (M=  44) - s=0.0100\n",
      " 132 - L= 1.1045866 - Gamma=41.1435946 (M=  44) - s=0.0100\n",
      " 133 - L= 1.1045867 - Gamma=41.1436210 (M=  44) - s=0.0100\n",
      " 134 - L= 1.1045867 - Gamma=41.1436216 (M=  44) - s=0.0100\n",
      " 135 - L= 1.1045868 - Gamma=41.1436113 (M=  44) - s=0.0100\n",
      " 136 - L= 1.1045869 - Gamma=41.1461208 (M=  44) - s=0.0100\n",
      " 137 - L= 1.1045869 - Gamma=41.1433259 (M=  44) - s=0.0100\n",
      " 138 - L= 1.1045870 - Gamma=41.1433326 (M=  44) - s=0.0100\n",
      " 139 - L= 1.1045870 - Gamma=41.1433015 (M=  44) - s=0.0100\n",
      " 140 - L= 1.1045871 - Gamma=41.1433084 (M=  44) - s=0.0100\n",
      " 141 - L= 1.1045871 - Gamma=41.1436826 (M=  44) - s=0.0100\n",
      " 142 - L= 1.1045871 - Gamma=41.1435346 (M=  44) - s=0.0100\n",
      " 143 - L= 1.1045872 - Gamma=41.1429482 (M=  44) - s=0.0100\n",
      " 144 - L= 1.1045872 - Gamma=41.1448389 (M=  44) - s=0.0100\n",
      " 145 - L= 1.1045872 - Gamma=41.1448324 (M=  44) - s=0.0100\n",
      " 146 - L= 1.1045873 - Gamma=41.1447669 (M=  44) - s=0.0100\n",
      " 147 - L= 1.1045873 - Gamma=41.1443085 (M=  44) - s=0.0100\n",
      " 148 - L= 1.1045873 - Gamma=41.1443079 (M=  44) - s=0.0100\n",
      " 149 - L= 1.1045873 - Gamma=41.1443071 (M=  44) - s=0.0100\n",
      " 150 - L= 1.1045873 - Gamma=41.1442792 (M=  44) - s=0.0100\n",
      " 151 - L= 1.1045874 - Gamma=41.1442796 (M=  44) - s=0.0100\n",
      " 152 - L= 1.1045874 - Gamma=41.1445253 (M=  44) - s=0.0100\n",
      " 153 - L= 1.1045874 - Gamma=41.1441372 (M=  44) - s=0.0100\n",
      " 154 - L= 1.1045874 - Gamma=41.1441440 (M=  44) - s=0.0100\n",
      " 155 - L= 1.1045874 - Gamma=41.1452362 (M=  44) - s=0.0100\n",
      " 156 - L= 1.1045874 - Gamma=41.1452469 (M=  44) - s=0.0100\n",
      " 157 - L= 1.1045874 - Gamma=41.1451745 (M=  44) - s=0.0100\n",
      " 158 - L= 1.1045874 - Gamma=41.1451752 (M=  44) - s=0.0100\n",
      " 159 - L= 1.1045874 - Gamma=41.1451761 (M=  44) - s=0.0100\n",
      " 160 - L= 1.1045874 - Gamma=41.1442636 (M=  44) - s=0.0100\n",
      " 161 - L= 1.1045874 - Gamma=41.1442645 (M=  44) - s=0.0100\n",
      " 162 - L= 1.1045874 - Gamma=41.1440653 (M=  44) - s=0.0100\n",
      " 163 - L= 1.1045875 - Gamma=41.1438965 (M=  44) - s=0.0100\n",
      " 164 - L= 1.1045875 - Gamma=41.1440439 (M=  44) - s=0.0100\n",
      " 165 - L= 1.1045875 - Gamma=41.1448015 (M=  44) - s=0.0100\n",
      " 166 - L= 1.1045875 - Gamma=41.1448015 (M=  44) - s=0.0100\n",
      "Stopping at iteration 166 - max_delta_ml=1.742636418848825e-07\n",
      "L=1.1045874634820714 - Gamma=41.14480152759865 (M=44) - s=0.01\n",
      "Initial alpha = [[ 0.13207969]]\n",
      "   1 - L=-1092.2855061 - Gamma= 1.9999396 (M=   2) - s=0.0100\n",
      "   2 - L=-970.0603407 - Gamma= 2.9998524 (M=   3) - s=0.0100\n",
      "   3 - L=-885.8374308 - Gamma= 3.9997257 (M=   4) - s=0.0100\n",
      "   4 - L=-809.4315384 - Gamma= 4.9995863 (M=   5) - s=0.0100\n",
      "   5 - L=-724.6363324 - Gamma= 5.9994580 (M=   6) - s=0.0100\n",
      "   6 - L=-660.0701678 - Gamma= 6.9992862 (M=   7) - s=0.0100\n",
      "   7 - L=-603.1171158 - Gamma= 7.9990975 (M=   8) - s=0.0100\n",
      "   8 - L=-560.6657410 - Gamma= 8.9988387 (M=   9) - s=0.0100\n",
      "   9 - L=-512.8926867 - Gamma= 9.9986068 (M=  10) - s=0.0100\n",
      "  10 - L=-454.7018020 - Gamma=10.9984142 (M=  11) - s=0.0100\n",
      "  11 - L=-394.1152074 - Gamma=11.9982314 (M=  12) - s=0.0100\n",
      "  12 - L=-345.4565679 - Gamma=12.9980083 (M=  13) - s=0.0100\n",
      "  13 - L=-304.0043532 - Gamma=13.9977508 (M=  14) - s=0.0100\n",
      "  14 - L=-261.0949233 - Gamma=14.9974983 (M=  15) - s=0.0100\n",
      "  15 - L=-223.5092384 - Gamma=15.9972125 (M=  16) - s=0.0100\n",
      "  16 - L=-191.9926756 - Gamma=16.9968717 (M=  17) - s=0.0100\n",
      "  17 - L=-157.1414716 - Gamma=17.9965578 (M=  18) - s=0.0100\n",
      "  18 - L=-124.8903266 - Gamma=18.9962213 (M=  19) - s=0.0100\n",
      "  19 - L=-101.8756333 - Gamma=19.9957277 (M=  20) - s=0.0100\n",
      "  20 - L=-88.8084957 - Gamma=20.9948967 (M=  21) - s=0.0100\n",
      "  21 - L=-78.4306693 - Gamma=21.9938413 (M=  22) - s=0.0100\n",
      "  22 - L=-69.0175406 - Gamma=22.9926213 (M=  23) - s=0.0100\n",
      "  23 - L=-59.4090042 - Gamma=23.9914909 (M=  24) - s=0.0100\n",
      "  24 - L=-50.4648616 - Gamma=24.9903016 (M=  25) - s=0.0100\n",
      "  25 - L=-40.8962048 - Gamma=25.9891577 (M=  26) - s=0.0100\n",
      "  26 - L=-34.4251235 - Gamma=26.9875030 (M=  27) - s=0.0100\n",
      "  27 - L=-28.6165305 - Gamma=27.9855309 (M=  28) - s=0.0100\n",
      "  28 - L=-23.3753553 - Gamma=28.9828270 (M=  29) - s=0.0100\n",
      "  29 - L=-16.6162728 - Gamma=29.9810430 (M=  30) - s=0.0100\n",
      "  30 - L=-9.5281862 - Gamma=30.9778612 (M=  31) - s=0.0100\n",
      "  31 - L=-7.4100676 - Gamma=31.9711260 (M=  32) - s=0.0100\n",
      "  32 - L=-5.1661831 - Gamma=32.9660311 (M=  33) - s=0.0100\n",
      "  33 - L=-2.6863282 - Gamma=33.9608496 (M=  34) - s=0.0100\n",
      "  34 - L=-1.4741994 - Gamma=34.9466257 (M=  35) - s=0.0100\n",
      "  35 - L=-0.6334233 - Gamma=35.9337520 (M=  36) - s=0.0100\n",
      "  36 - L=-0.1442000 - Gamma=36.9139212 (M=  37) - s=0.0100\n",
      "  37 - L= 0.1638658 - Gamma=37.8830575 (M=  38) - s=0.0100\n",
      "  38 - L= 0.4532355 - Gamma=38.8501638 (M=  39) - s=0.0100\n",
      "  39 - L= 0.5919651 - Gamma=39.7849130 (M=  40) - s=0.0100\n",
      "  40 - L= 0.6858831 - Gamma=40.6992924 (M=  41) - s=0.0100\n",
      "  41 - L= 0.7490419 - Gamma=41.5153549 (M=  42) - s=0.0100\n",
      "  42 - L= 0.7793238 - Gamma=42.3190846 (M=  43) - s=0.0100\n",
      "  43 - L= 0.8037978 - Gamma=42.3234990 (M=  43) - s=0.0100\n",
      "  44 - L= 0.8213522 - Gamma=42.3265246 (M=  43) - s=0.0100\n",
      "  45 - L= 0.8348123 - Gamma=43.0297090 (M=  44) - s=0.0100\n",
      "  46 - L= 0.8477417 - Gamma=43.0325576 (M=  44) - s=0.0100\n",
      "  47 - L= 0.8595250 - Gamma=43.0353348 (M=  44) - s=0.0100\n",
      "  48 - L= 0.8685015 - Gamma=43.0379583 (M=  44) - s=0.0100\n",
      "  49 - L= 0.8762933 - Gamma=43.0332269 (M=  44) - s=0.0100\n",
      "  50 - L= 0.8841234 - Gamma=43.0361246 (M=  44) - s=0.0100\n",
      "  51 - L= 0.8905925 - Gamma=43.0367297 (M=  44) - s=0.0100\n",
      "  52 - L= 0.8957257 - Gamma=43.0340358 (M=  44) - s=0.0100\n",
      "  53 - L= 0.9005035 - Gamma=43.0329835 (M=  44) - s=0.0100\n",
      "  54 - L= 0.9047147 - Gamma=43.0049120 (M=  44) - s=0.0100\n",
      "  55 - L= 0.9087001 - Gamma=43.0058890 (M=  44) - s=0.0100\n",
      "  56 - L= 0.9126785 - Gamma=43.0063920 (M=  44) - s=0.0100\n",
      "  57 - L= 0.9165487 - Gamma=43.0118029 (M=  44) - s=0.0100\n",
      "  58 - L= 0.9202098 - Gamma=43.0108884 (M=  44) - s=0.0100\n",
      "  59 - L= 0.9232158 - Gamma=43.0328526 (M=  44) - s=0.0100\n",
      "  60 - L= 0.9261807 - Gamma=43.0316249 (M=  44) - s=0.0100\n",
      "  61 - L= 0.9290254 - Gamma=43.0323598 (M=  44) - s=0.0100\n",
      "  62 - L= 0.9318020 - Gamma=43.0316160 (M=  44) - s=0.0100\n",
      "  63 - L= 0.9338936 - Gamma=43.0313237 (M=  44) - s=0.0100\n",
      "  64 - L= 0.9356608 - Gamma=43.0310101 (M=  44) - s=0.0100\n",
      "  65 - L= 0.9374119 - Gamma=43.0322599 (M=  44) - s=0.0100\n",
      "  66 - L= 0.9386631 - Gamma=43.0578719 (M=  44) - s=0.0100\n",
      "  67 - L= 0.9399547 - Gamma=42.9986364 (M=  44) - s=0.0100\n",
      "  68 - L= 0.9412072 - Gamma=43.2796329 (M=  45) - s=0.0100\n",
      "  69 - L= 0.9423177 - Gamma=43.1631165 (M=  45) - s=0.0100\n",
      "  70 - L= 0.9432080 - Gamma=43.1632307 (M=  45) - s=0.0100\n",
      "  71 - L= 0.9440204 - Gamma=43.1683315 (M=  45) - s=0.0100\n",
      "  72 - L= 0.9446680 - Gamma=43.1681166 (M=  45) - s=0.0100\n",
      "  73 - L= 0.9452517 - Gamma=43.1681360 (M=  45) - s=0.0100\n",
      "  74 - L= 0.9456827 - Gamma=43.1682175 (M=  45) - s=0.0100\n",
      "  75 - L= 0.9460698 - Gamma=43.1201849 (M=  45) - s=0.0100\n",
      "  76 - L= 0.9466250 - Gamma=43.2426348 (M=  45) - s=0.0100\n",
      "  77 - L= 0.9471555 - Gamma=43.2398141 (M=  45) - s=0.0100\n",
      "  78 - L= 0.9476253 - Gamma=43.2515544 (M=  45) - s=0.0100\n",
      "  79 - L= 0.9480758 - Gamma=43.2607142 (M=  45) - s=0.0100\n",
      "  80 - L= 0.9483808 - Gamma=43.2608952 (M=  45) - s=0.0100\n",
      "  81 - L= 0.9486463 - Gamma=43.2820978 (M=  45) - s=0.0100\n",
      "  82 - L= 0.9489502 - Gamma=43.2236201 (M=  45) - s=0.0100\n",
      "  83 - L= 0.9493028 - Gamma=43.2964444 (M=  45) - s=0.0100\n",
      "  84 - L= 0.9496691 - Gamma=43.1963136 (M=  45) - s=0.0100\n",
      "  85 - L= 0.9499119 - Gamma=43.1963653 (M=  45) - s=0.0100\n",
      "  86 - L= 0.9501438 - Gamma=43.1966076 (M=  45) - s=0.0100\n",
      "  87 - L= 0.9502684 - Gamma=43.1888804 (M=  45) - s=0.0100\n",
      "  88 - L= 0.9503918 - Gamma=43.1420386 (M=  45) - s=0.0100\n",
      "  89 - L= 0.9505425 - Gamma=43.1790706 (M=  45) - s=0.0100\n",
      "  90 - L= 0.9507193 - Gamma=43.2882918 (M=  46) - s=0.0100\n",
      "  91 - L= 0.9511500 - Gamma=43.1939782 (M=  46) - s=0.0100\n",
      "  92 - L= 0.9514620 - Gamma=43.2116451 (M=  46) - s=0.0100\n",
      "  93 - L= 0.9517746 - Gamma=43.3288562 (M=  46) - s=0.0100\n",
      "  94 - L= 0.9523145 - Gamma=43.1662100 (M=  46) - s=0.0100\n",
      "  95 - L= 0.9530780 - Gamma=43.2236954 (M=  46) - s=0.0100\n",
      "  96 - L= 0.9536796 - Gamma=43.2339407 (M=  46) - s=0.0100\n",
      "  97 - L= 0.9543652 - Gamma=43.0710227 (M=  46) - s=0.0100\n",
      "  98 - L= 0.9551183 - Gamma=42.8929399 (M=  46) - s=0.0100\n",
      "  99 - L= 0.9564892 - Gamma=43.0660465 (M=  46) - s=0.0100\n",
      " 100 - L= 0.9581269 - Gamma=42.8034541 (M=  45) - s=0.0100\n",
      " 101 - L= 0.9593358 - Gamma=42.5654812 (M=  44) - s=0.0100\n",
      " 102 - L= 0.9612578 - Gamma=42.5086861 (M=  44) - s=0.0100\n",
      " 103 - L= 0.9626963 - Gamma=42.5581966 (M=  44) - s=0.0100\n",
      " 104 - L= 0.9642318 - Gamma=42.5837372 (M=  44) - s=0.0100\n",
      " 105 - L= 0.9654191 - Gamma=42.2188616 (M=  44) - s=0.0100\n",
      " 106 - L= 0.9668004 - Gamma=42.2106628 (M=  44) - s=0.0100\n",
      " 107 - L= 0.9668055 - Gamma=42.2032275 (M=  43) - s=0.0100\n",
      " 108 - L= 0.9682045 - Gamma=42.2079604 (M=  43) - s=0.0100\n",
      " 109 - L= 0.9694929 - Gamma=42.1999341 (M=  43) - s=0.0100\n",
      " 110 - L= 0.9706538 - Gamma=42.3089064 (M=  43) - s=0.0100\n",
      " 111 - L= 0.9714339 - Gamma=42.3174888 (M=  43) - s=0.0100\n",
      " 112 - L= 0.9722560 - Gamma=42.2517741 (M=  43) - s=0.0100\n",
      " 113 - L= 0.9727386 - Gamma=42.2500177 (M=  43) - s=0.0100\n",
      " 114 - L= 0.9732301 - Gamma=42.2691421 (M=  43) - s=0.0100\n",
      " 115 - L= 0.9737195 - Gamma=42.2683521 (M=  43) - s=0.0100\n",
      " 116 - L= 0.9741849 - Gamma=42.2678616 (M=  43) - s=0.0100\n",
      " 117 - L= 0.9744920 - Gamma=42.2755252 (M=  43) - s=0.0100\n",
      " 118 - L= 0.9747755 - Gamma=42.2751321 (M=  43) - s=0.0100\n",
      " 119 - L= 0.9750334 - Gamma=42.2754514 (M=  43) - s=0.0100\n",
      " 120 - L= 0.9752881 - Gamma=42.2753654 (M=  43) - s=0.0100\n",
      " 121 - L= 0.9755365 - Gamma=42.2749494 (M=  43) - s=0.0100\n",
      " 122 - L= 0.9757406 - Gamma=42.2796334 (M=  43) - s=0.0100\n",
      " 123 - L= 0.9759305 - Gamma=42.2792700 (M=  43) - s=0.0100\n",
      " 124 - L= 0.9761132 - Gamma=42.2795048 (M=  43) - s=0.0100\n",
      " 125 - L= 0.9762901 - Gamma=42.2801125 (M=  43) - s=0.0100\n",
      " 126 - L= 0.9764455 - Gamma=42.2804146 (M=  43) - s=0.0100\n",
      " 127 - L= 0.9766005 - Gamma=42.2804981 (M=  43) - s=0.0100\n",
      " 128 - L= 0.9767499 - Gamma=42.2813047 (M=  43) - s=0.0100\n",
      " 129 - L= 0.9768999 - Gamma=42.2448705 (M=  43) - s=0.0100\n",
      " 130 - L= 0.9770660 - Gamma=42.2753819 (M=  43) - s=0.0100\n",
      " 131 - L= 0.9772131 - Gamma=42.2720957 (M=  43) - s=0.0100\n",
      " 132 - L= 0.9773476 - Gamma=42.2720122 (M=  43) - s=0.0100\n",
      " 133 - L= 0.9774788 - Gamma=42.2722247 (M=  43) - s=0.0100\n",
      " 134 - L= 0.9775566 - Gamma=42.2721051 (M=  43) - s=0.0100\n",
      " 135 - L= 0.9776271 - Gamma=42.2720262 (M=  43) - s=0.0100\n",
      " 136 - L= 0.9776938 - Gamma=42.2719805 (M=  43) - s=0.0100\n",
      " 137 - L= 0.9777559 - Gamma=42.2721204 (M=  43) - s=0.0100\n",
      " 138 - L= 0.9778079 - Gamma=42.2721054 (M=  43) - s=0.0100\n",
      " 139 - L= 0.9778591 - Gamma=42.2701204 (M=  43) - s=0.0100\n",
      " 140 - L= 0.9779088 - Gamma=42.2718170 (M=  43) - s=0.0100\n",
      " 141 - L= 0.9779549 - Gamma=42.2725056 (M=  43) - s=0.0100\n",
      " 142 - L= 0.9780033 - Gamma=42.2489356 (M=  43) - s=0.0100\n",
      " 143 - L= 0.9780485 - Gamma=42.2509107 (M=  43) - s=0.0100\n",
      " 144 - L= 0.9780916 - Gamma=42.2532611 (M=  43) - s=0.0100\n",
      " 145 - L= 0.9781330 - Gamma=42.2532832 (M=  43) - s=0.0100\n",
      " 146 - L= 0.9781696 - Gamma=42.2550684 (M=  43) - s=0.0100\n",
      " 147 - L= 0.9781986 - Gamma=42.2549831 (M=  43) - s=0.0100\n",
      " 148 - L= 0.9782250 - Gamma=42.2587965 (M=  43) - s=0.0100\n",
      " 149 - L= 0.9782458 - Gamma=42.2587786 (M=  43) - s=0.0100\n",
      " 150 - L= 0.9782661 - Gamma=42.2583868 (M=  43) - s=0.0100\n",
      " 151 - L= 0.9782838 - Gamma=42.2583980 (M=  43) - s=0.0100\n",
      " 152 - L= 0.9783014 - Gamma=42.2584322 (M=  43) - s=0.0100\n",
      " 153 - L= 0.9783126 - Gamma=42.2584617 (M=  43) - s=0.0100\n",
      " 154 - L= 0.9783223 - Gamma=42.2651770 (M=  43) - s=0.0100\n",
      " 155 - L= 0.9783312 - Gamma=42.2544261 (M=  43) - s=0.0100\n",
      " 156 - L= 0.9783394 - Gamma=42.2536075 (M=  43) - s=0.0100\n",
      " 157 - L= 0.9783476 - Gamma=42.2536413 (M=  43) - s=0.0100\n",
      " 158 - L= 0.9783551 - Gamma=42.2535355 (M=  43) - s=0.0100\n",
      " 159 - L= 0.9783624 - Gamma=42.2534713 (M=  43) - s=0.0100\n",
      " 160 - L= 0.9783673 - Gamma=42.2541282 (M=  43) - s=0.0100\n",
      " 161 - L= 0.9783718 - Gamma=42.2542493 (M=  43) - s=0.0100\n",
      " 162 - L= 0.9783756 - Gamma=42.2542020 (M=  43) - s=0.0100\n",
      " 163 - L= 0.9783794 - Gamma=42.2541478 (M=  43) - s=0.0100\n",
      " 164 - L= 0.9783827 - Gamma=42.2540986 (M=  43) - s=0.0100\n",
      " 165 - L= 0.9783860 - Gamma=42.2535757 (M=  43) - s=0.0100\n",
      " 166 - L= 0.9783893 - Gamma=42.2539864 (M=  43) - s=0.0100\n",
      " 167 - L= 0.9783925 - Gamma=42.2539791 (M=  43) - s=0.0100\n",
      " 168 - L= 0.9783955 - Gamma=42.2540491 (M=  43) - s=0.0100\n",
      " 169 - L= 0.9783985 - Gamma=42.2540364 (M=  43) - s=0.0100\n",
      " 170 - L= 0.9784015 - Gamma=42.2540657 (M=  43) - s=0.0100\n",
      " 171 - L= 0.9784044 - Gamma=42.2541008 (M=  43) - s=0.0100\n",
      " 172 - L= 0.9784073 - Gamma=42.2477352 (M=  43) - s=0.0100\n",
      " 173 - L= 0.9784098 - Gamma=42.2482608 (M=  43) - s=0.0100\n",
      " 174 - L= 0.9784119 - Gamma=42.2482827 (M=  43) - s=0.0100\n",
      " 175 - L= 0.9784137 - Gamma=42.2511171 (M=  43) - s=0.0100\n",
      " 176 - L= 0.9784152 - Gamma=42.2512354 (M=  43) - s=0.0100\n",
      " 177 - L= 0.9784163 - Gamma=42.2512360 (M=  43) - s=0.0100\n",
      " 178 - L= 0.9784172 - Gamma=42.2519091 (M=  43) - s=0.0100\n",
      " 179 - L= 0.9784179 - Gamma=42.2518344 (M=  43) - s=0.0100\n",
      " 180 - L= 0.9784185 - Gamma=42.2488588 (M=  43) - s=0.0100\n",
      " 181 - L= 0.9784193 - Gamma=42.2486095 (M=  43) - s=0.0100\n",
      " 182 - L= 0.9784198 - Gamma=42.2486219 (M=  43) - s=0.0100\n",
      " 183 - L= 0.9784203 - Gamma=42.2486260 (M=  43) - s=0.0100\n",
      " 184 - L= 0.9784207 - Gamma=42.2488129 (M=  43) - s=0.0100\n",
      " 185 - L= 0.9784212 - Gamma=42.2488092 (M=  43) - s=0.0100\n",
      " 186 - L= 0.9784216 - Gamma=42.2488031 (M=  43) - s=0.0100\n",
      " 187 - L= 0.9784219 - Gamma=42.2487949 (M=  43) - s=0.0100\n",
      " 188 - L= 0.9784223 - Gamma=42.2487905 (M=  43) - s=0.0100\n",
      " 189 - L= 0.9784226 - Gamma=42.2487876 (M=  43) - s=0.0100\n",
      " 190 - L= 0.9784228 - Gamma=42.2488869 (M=  43) - s=0.0100\n",
      " 191 - L= 0.9784230 - Gamma=42.2487557 (M=  43) - s=0.0100\n",
      " 192 - L= 0.9784231 - Gamma=42.2474768 (M=  43) - s=0.0100\n",
      " 193 - L= 0.9784232 - Gamma=42.2476034 (M=  43) - s=0.0100\n",
      " 194 - L= 0.9784234 - Gamma=42.2484012 (M=  43) - s=0.0100\n",
      " 195 - L= 0.9784235 - Gamma=42.2484202 (M=  43) - s=0.0100\n",
      " 196 - L= 0.9784236 - Gamma=42.2484068 (M=  43) - s=0.0100\n",
      " 197 - L= 0.9784237 - Gamma=42.2483988 (M=  43) - s=0.0100\n",
      " 198 - L= 0.9784238 - Gamma=42.2484961 (M=  43) - s=0.0100\n",
      " 199 - L= 0.9784239 - Gamma=42.2484914 (M=  43) - s=0.0100\n",
      " 200 - L= 0.9784240 - Gamma=42.2484920 (M=  43) - s=0.0100\n",
      "MODEL: RVM accuracy:  0.413793103448 +/-: 0.00798111915829\n",
      "Initial alpha = [[ 0.10163165]]\n",
      "   1 - L=-1136.8890395 - Gamma= 1.9999378 (M=   2) - s=0.0100\n",
      "   2 - L=-1009.8394536 - Gamma= 2.9998700 (M=   3) - s=0.0100\n",
      "   3 - L=-909.4168414 - Gamma= 3.9997842 (M=   4) - s=0.0100\n",
      "   4 - L=-834.9231802 - Gamma= 4.9996685 (M=   5) - s=0.0100\n",
      "   5 - L=-772.3044180 - Gamma= 5.9995310 (M=   6) - s=0.0100\n",
      "   6 - L=-710.7170110 - Gamma= 6.9993912 (M=   7) - s=0.0100\n",
      "   7 - L=-654.4182934 - Gamma= 7.9992383 (M=   8) - s=0.0100\n",
      "   8 - L=-600.1505312 - Gamma= 8.9990797 (M=   9) - s=0.0100\n",
      "   9 - L=-553.6502978 - Gamma= 9.9988946 (M=  10) - s=0.0100\n",
      "  10 - L=-507.6565119 - Gamma=10.9987076 (M=  11) - s=0.0100\n",
      "  11 - L=-462.2328930 - Gamma=11.9985181 (M=  12) - s=0.0100\n",
      "  12 - L=-421.0083214 - Gamma=12.9983094 (M=  13) - s=0.0100\n",
      "  13 - L=-381.7025165 - Gamma=13.9980905 (M=  14) - s=0.0100\n",
      "  14 - L=-351.6280165 - Gamma=14.9978046 (M=  15) - s=0.0100\n",
      "  15 - L=-322.0795589 - Gamma=15.9975136 (M=  16) - s=0.0100\n",
      "  16 - L=-293.7869807 - Gamma=16.9972097 (M=  17) - s=0.0100\n",
      "  17 - L=-272.2107740 - Gamma=17.9968116 (M=  18) - s=0.0100\n",
      "  18 - L=-251.1315782 - Gamma=18.9964041 (M=  19) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19 - L=-230.0907874 - Gamma=19.9959958 (M=  20) - s=0.0100\n",
      "  20 - L=-210.1580247 - Gamma=20.9955650 (M=  21) - s=0.0100\n",
      "  21 - L=-193.6642092 - Gamma=21.9950446 (M=  22) - s=0.0100\n",
      "  22 - L=-177.5898659 - Gamma=22.9945108 (M=  23) - s=0.0100\n",
      "  23 - L=-162.1321658 - Gamma=23.9939557 (M=  24) - s=0.0100\n",
      "  24 - L=-147.0627703 - Gamma=24.9933863 (M=  25) - s=0.0100\n",
      "  25 - L=-132.7411961 - Gamma=25.9927874 (M=  26) - s=0.0100\n",
      "  26 - L=-119.3249074 - Gamma=26.9921483 (M=  27) - s=0.0100\n",
      "  27 - L=-106.1367819 - Gamma=27.9914982 (M=  28) - s=0.0100\n",
      "  28 - L=-93.5013765 - Gamma=28.9908197 (M=  29) - s=0.0100\n",
      "  29 - L=-82.3001924 - Gamma=29.9900548 (M=  30) - s=0.0100\n",
      "  30 - L=-71.7977654 - Gamma=30.9892394 (M=  31) - s=0.0100\n",
      "  31 - L=-62.0680099 - Gamma=31.9883596 (M=  32) - s=0.0100\n",
      "  32 - L=-54.0415301 - Gamma=32.9872945 (M=  33) - s=0.0100\n",
      "  33 - L=-47.5974883 - Gamma=33.9859702 (M=  34) - s=0.0100\n",
      "  34 - L=-41.9324920 - Gamma=34.9844657 (M=  35) - s=0.0100\n",
      "  35 - L=-36.6959491 - Gamma=35.9828393 (M=  36) - s=0.0100\n",
      "  36 - L=-31.4735525 - Gamma=36.9812085 (M=  37) - s=0.0100\n",
      "  37 - L=-26.3235935 - Gamma=37.9795550 (M=  38) - s=0.0100\n",
      "  38 - L=-21.3964087 - Gamma=38.9778276 (M=  39) - s=0.0100\n",
      "  39 - L=-17.5154720 - Gamma=39.9756409 (M=  40) - s=0.0100\n",
      "  40 - L=-15.0910672 - Gamma=40.9721673 (M=  41) - s=0.0100\n",
      "  41 - L=-12.9196448 - Gamma=41.9682979 (M=  42) - s=0.0100\n",
      "  42 - L=-10.8480790 - Gamma=42.9642462 (M=  43) - s=0.0100\n",
      "  43 - L=-8.8765854 - Gamma=43.9599936 (M=  44) - s=0.0100\n",
      "  44 - L=-6.9406137 - Gamma=44.9556649 (M=  45) - s=0.0100\n",
      "  45 - L=-5.4213810 - Gamma=45.9501835 (M=  46) - s=0.0100\n",
      "  46 - L=-3.9818533 - Gamma=46.9444078 (M=  47) - s=0.0100\n",
      "  47 - L=-2.9321285 - Gamma=47.9365719 (M=  48) - s=0.0100\n",
      "  48 - L=-1.9012414 - Gamma=48.9285983 (M=  49) - s=0.0100\n",
      "  49 - L=-1.1719709 - Gamma=49.9174990 (M=  50) - s=0.0100\n",
      "  50 - L=-0.4913530 - Gamma=50.9056488 (M=  51) - s=0.0100\n",
      "  51 - L= 0.0474897 - Gamma=51.8908824 (M=  52) - s=0.0100\n",
      "  52 - L= 0.4730274 - Gamma=52.8724858 (M=  53) - s=0.0100\n",
      "  53 - L= 0.6277738 - Gamma=53.8271212 (M=  54) - s=0.0100\n",
      "  54 - L= 0.6767610 - Gamma=54.7143122 (M=  55) - s=0.0100\n",
      "  55 - L= 0.7008538 - Gamma=55.5324742 (M=  56) - s=0.0100\n",
      "  56 - L= 0.7248916 - Gamma=56.3503778 (M=  57) - s=0.0100\n",
      "  57 - L= 0.7383778 - Gamma=57.0961857 (M=  58) - s=0.0100\n",
      "  58 - L= 0.7384840 - Gamma=57.0961872 (M=  58) - s=0.0100\n",
      "  59 - L= 0.7384894 - Gamma=57.0961912 (M=  58) - s=0.0100\n",
      "  60 - L= 0.7384918 - Gamma=57.0961944 (M=  58) - s=0.0100\n",
      "  61 - L= 0.7384934 - Gamma=57.0961934 (M=  58) - s=0.0100\n",
      "  62 - L= 0.7384948 - Gamma=57.0961918 (M=  58) - s=0.0100\n",
      "  63 - L= 0.7384959 - Gamma=57.0961948 (M=  58) - s=0.0100\n",
      "  64 - L= 0.7384970 - Gamma=57.0961959 (M=  58) - s=0.0100\n",
      "  65 - L= 0.7384979 - Gamma=57.0961989 (M=  58) - s=0.0100\n",
      "  66 - L= 0.7384987 - Gamma=57.0961967 (M=  58) - s=0.0100\n",
      "  67 - L= 0.7384994 - Gamma=57.0961985 (M=  58) - s=0.0100\n",
      "  68 - L= 0.7384998 - Gamma=57.0961912 (M=  58) - s=0.0100\n",
      "  69 - L= 0.7385001 - Gamma=57.0961937 (M=  58) - s=0.0100\n",
      "  70 - L= 0.7385005 - Gamma=57.0952200 (M=  58) - s=0.0100\n",
      "  71 - L= 0.7385007 - Gamma=57.0952233 (M=  58) - s=0.0100\n",
      "  72 - L= 0.7385010 - Gamma=57.0952270 (M=  58) - s=0.0100\n",
      "  73 - L= 0.7385012 - Gamma=57.0952299 (M=  58) - s=0.0100\n",
      "  74 - L= 0.7385013 - Gamma=57.0952309 (M=  58) - s=0.0100\n",
      "  75 - L= 0.7385014 - Gamma=57.0952324 (M=  58) - s=0.0100\n",
      "  76 - L= 0.7385015 - Gamma=57.0952302 (M=  58) - s=0.0100\n",
      "  77 - L= 0.7385016 - Gamma=57.0952134 (M=  58) - s=0.0100\n",
      "  78 - L= 0.7385016 - Gamma=57.0952156 (M=  58) - s=0.0100\n",
      "  79 - L= 0.7385017 - Gamma=57.0952075 (M=  58) - s=0.0100\n",
      "  80 - L= 0.7385017 - Gamma=57.0952080 (M=  58) - s=0.0100\n",
      "  81 - L= 0.7385017 - Gamma=57.0952072 (M=  58) - s=0.0100\n",
      "  82 - L= 0.7385018 - Gamma=57.0952034 (M=  58) - s=0.0100\n",
      "  83 - L= 0.7385018 - Gamma=57.0956205 (M=  58) - s=0.0100\n",
      "  84 - L= 0.7385018 - Gamma=57.0956196 (M=  58) - s=0.0100\n",
      "  85 - L= 0.7385018 - Gamma=57.0956181 (M=  58) - s=0.0100\n",
      "  86 - L= 0.7385018 - Gamma=57.0959923 (M=  58) - s=0.0100\n",
      "  87 - L= 0.7385019 - Gamma=57.0959935 (M=  58) - s=0.0100\n",
      "  88 - L= 0.7385019 - Gamma=57.0959915 (M=  58) - s=0.0100\n",
      "  89 - L= 0.7385019 - Gamma=57.0959932 (M=  58) - s=0.0100\n",
      "  90 - L= 0.7385019 - Gamma=57.0959918 (M=  58) - s=0.0100\n",
      "  91 - L= 0.7385019 - Gamma=57.0959916 (M=  58) - s=0.0100\n",
      "  92 - L= 0.7385019 - Gamma=57.0959916 (M=  58) - s=0.0100\n",
      "Stopping at iteration 92 - max_delta_ml=2.3080795385528466e-07\n",
      "L=0.738501879879203 - Gamma=57.095991646175676 (M=58) - s=0.01\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7111 - acc: 0.5652 - val_loss: 0.6058 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.5080 - acc: 0.8696 - val_loss: 0.5706 - val_acc: 0.8333\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.4051 - acc: 0.9783 - val_loss: 0.5311 - val_acc: 0.7500\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.3184 - acc: 1.0000 - val_loss: 0.5004 - val_acc: 0.8333\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.2621 - acc: 1.0000 - val_loss: 0.5041 - val_acc: 0.8333\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.2959 - acc: 0.9565 - val_loss: 0.2056 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.2121 - acc: 1.0000 - val_loss: 0.1912 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.1877 - acc: 1.0000 - val_loss: 0.1805 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.1753 - acc: 1.0000 - val_loss: 0.1726 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.1673 - acc: 1.0000 - val_loss: 0.1669 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.1632 - acc: 1.0000 - val_loss: 0.1575 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.1585 - acc: 1.0000 - val_loss: 0.1541 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.1548 - acc: 1.0000 - val_loss: 0.1510 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.1516 - acc: 1.0000 - val_loss: 0.1482 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.1487 - acc: 1.0000 - val_loss: 0.1456 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.1452 - acc: 1.0000 - val_loss: 0.1465 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1426 - acc: 1.0000 - val_loss: 0.1440 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1399 - acc: 1.0000 - val_loss: 0.1413 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.1373 - acc: 1.0000 - val_loss: 0.1384 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.1342 - acc: 1.0000 - val_loss: 0.1351 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.1310 - acc: 1.0000 - val_loss: 0.1318 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1277 - acc: 1.0000 - val_loss: 0.1286 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1249 - acc: 1.0000 - val_loss: 0.1256 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.1222 - acc: 1.0000 - val_loss: 0.1229 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.1196 - acc: 1.0000 - val_loss: 0.1204 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  0.965517241379 +/-: 0.00455806579469\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "METHOD_LIST = ['ExtraTrees', 'CART', 'RandomForest', 'GBM', 'AdaBoost', 'LR', 'SVM', 'NaiveBayes', 'MLNN'] # XGB\n",
    "Runs = []\n",
    "nruns = 1\n",
    "SCALER = \"standard\" # minmax, standard, normaliser\n",
    "GROUPING = \"mean\"\n",
    "DIM_TYPE =  \"PCA\"   # \"LDA\", \"PCA\"\n",
    "DIM_NUM = 1000\n",
    "Results = None\n",
    "ACC = pd.DataFrame()\n",
    "Rocket.VIZ = False\n",
    "for i in range(0, nruns):\n",
    "    Rocket.SEED = np.random.randint(0,10000)\n",
    "    MODELS  = []\n",
    "    for idx, METHOD in enumerate(METHOD_LIST):\n",
    "        preds, class_model, accuracy = Rocket.classify_treatment(model_type = METHOD, \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "        MODELS.append({'method': METHOD, 'model': class_model})\n",
    "        ACC = ACC.append(accuracy, ignore_index= True)\n",
    "        preds = [pred_[1]for pred_ in preds]\n",
    "        #len(Rocket.DATA_merged[Rocket.DATA_merged[\"array-batch\"].isin([\"cohort 1\", \"cohort 2\", \"JB\", \"IA\", \"ALL-10\"])])\n",
    "        if Results is None:\n",
    "            Results = Rocket.DATA_merged_processed.copy()\n",
    "        Results['pred'] = preds\n",
    "        Results['method'] = METHOD\n",
    "        if idx == 0:\n",
    "            AllResults = Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]]\n",
    "        else:\n",
    "            AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], \n",
    "                                                    'pred', \n",
    "                                                    'method', \n",
    "                                                    Rocket.MODEL_PARAMETERS['target']]], \n",
    "                                      ignore_index = True)\n",
    "\n",
    "    ####\n",
    "    ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, nruns):\n",
    "    preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"RVM\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "    MODELS.append({'method': METHOD, 'model': class_model})\n",
    "    ACC = ACC.append(accuracy, ignore_index = True)\n",
    "    Results = Rocket.DATA_merged_processed.copy()\n",
    "    preds = [pred_ for pred_ in preds]\n",
    "    Results['pred'] = preds\n",
    "    Results['method'] = \"RVM\"\n",
    "    AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]], ignore_index = True)\n",
    "\n",
    "    ####\n",
    "    ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7484 - acc: 0.5652 - val_loss: 0.6462 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6239 - acc: 0.5652 - val_loss: 0.5924 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.5454 - acc: 0.5652 - val_loss: 0.6386 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.4549 - acc: 0.5870 - val_loss: 0.6513 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.3859 - acc: 0.7609 - val_loss: 0.6991 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.4295 - acc: 0.8913 - val_loss: 0.3263 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.3645 - acc: 0.9783 - val_loss: 0.3394 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.3080 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.2888 - acc: 1.0000 - val_loss: 0.2777 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.2760 - acc: 1.0000 - val_loss: 0.2645 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.2665 - acc: 1.0000 - val_loss: 0.2519 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.2571 - acc: 1.0000 - val_loss: 0.2438 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.2481 - acc: 1.0000 - val_loss: 0.2353 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.2390 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.2312 - acc: 1.0000 - val_loss: 0.2203 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.2218 - acc: 1.0000 - val_loss: 0.2288 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.2172 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.2126 - acc: 1.0000 - val_loss: 0.2202 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.2091 - acc: 1.0000 - val_loss: 0.2164 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.2056 - acc: 1.0000 - val_loss: 0.2128 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.2019 - acc: 1.0000 - val_loss: 0.2095 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1987 - acc: 1.0000 - val_loss: 0.2062 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1957 - acc: 1.0000 - val_loss: 0.2029 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.1927 - acc: 1.0000 - val_loss: 0.1997 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.1896 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  0.913793103448 +/-: 0.0284879112168\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6856 - acc: 0.5652 - val_loss: 0.6628 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.5693 - acc: 0.6304 - val_loss: 0.6413 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.4952 - acc: 0.8696 - val_loss: 0.6304 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.4492 - acc: 0.9783 - val_loss: 0.6307 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.4102 - acc: 1.0000 - val_loss: 0.6053 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.4360 - acc: 0.8913 - val_loss: 0.3659 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.3479 - acc: 1.000 - 0s - loss: 0.3688 - acc: 1.0000 - val_loss: 0.3504 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.3358 - acc: 1.0000 - val_loss: 0.3328 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.3095 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.2884 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.2787 - acc: 1.0000 - val_loss: 0.2595 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.2614 - acc: 1.0000 - val_loss: 0.2485 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.2493 - acc: 1.0000 - val_loss: 0.2398 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.2406 - acc: 1.0000 - val_loss: 0.2331 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.2335 - acc: 1.0000 - val_loss: 0.2277 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.2269 - acc: 1.0000 - val_loss: 0.2269 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.2220 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.2177 - acc: 1.0000 - val_loss: 0.2182 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.2136 - acc: 1.0000 - val_loss: 0.2143 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.2097 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.2062 - acc: 1.0000 - val_loss: 0.2065 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.2027 - acc: 1.0000 - val_loss: 0.2031 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1993 - acc: 1.0000 - val_loss: 0.1998 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.1960 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.1928 - acc: 1.0000 - val_loss: 0.1935 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  0.913793103448 +/-: 0.0284879112168\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Reducing dimensionality\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7573 - acc: 0.5652 - val_loss: 0.6938 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6609 - acc: 0.5652 - val_loss: 0.6683 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.5913 - acc: 0.5652 - val_loss: 0.6471 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.5386 - acc: 0.5652 - val_loss: 0.6305 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.4801 - acc: 0.5652 - val_loss: 0.6066 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s - loss: 0.4718 - acc: 0.6957 - val_loss: 0.4093 - val_acc: 0.9167\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.3900 - acc: 0.9130 - val_loss: 0.3832 - val_acc: 0.9167\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.3337 - acc: 0.9783 - val_loss: 0.3666 - val_acc: 0.9167\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.2943 - acc: 1.0000 - val_loss: 0.3562 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.2660 - acc: 1.0000 - val_loss: 0.3473 - val_acc: 0.9167\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.2735 - acc: 0.9783 - val_loss: 0.2242 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.2329 - acc: 1.0000 - val_loss: 0.2121 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.2173 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.2071 - acc: 1.0000 - val_loss: 0.1949 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.1982 - acc: 1.0000 - val_loss: 0.1888 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.1896 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1844 - acc: 1.0000 - val_loss: 0.1893 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1805 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.1772 - acc: 1.0000 - val_loss: 0.1831 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.1744 - acc: 1.0000 - val_loss: 0.1804 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.1718 - acc: 1.0000 - val_loss: 0.1777 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1693 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.1671 - acc: 1.0000 - val_loss: 0.1731 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.1648 - acc: 1.0000 - val_loss: 0.1709 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.1626 - acc: 1.0000 - val_loss: 0.1688 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  0.896551724138 +/-: 0.0266547760602\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "nruns = 3\n",
    "for i in range(0, nruns):\n",
    "    preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"DNN\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "    MODELS.append({'method': METHOD, 'model': class_model})\n",
    "    ACC = ACC.append(accuracy, ignore_index = True)\n",
    "    Results = Rocket.DATA_merged_processed.copy()\n",
    "    preds = [pred_ for pred_ in preds]\n",
    "    Results['pred'] = preds\n",
    "    Results['method'] = \"DNN\"\n",
    "    AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]],\n",
    "                                   ignore_index = True)\n",
    "\n",
    "    AllResults[Rocket.MODEL_PARAMETERS['ID']] = AllResults[Rocket.MODEL_PARAMETERS['ID']].astype('str')\n",
    "    AllResults = AllResults.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "    #AllResults[AllResults['Treatment_risk_group_in_ALL10'].notnull()]\n",
    "    ####\n",
    "    ####\n",
    "    Runs.append(AllResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on average: 0.6551724137931034 +- 0.017797947197471575, median: 0.5862068965517241+-0.02272952473606457\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "         acc model       var\n",
      "0   0.568966    ET  0.000338\n",
      "1   0.603448  CART  0.012156\n",
      "2   0.551724    RF  0.025015\n",
      "3   0.448276   GBM  0.022142\n",
      "4   0.500000   ADA  0.024033\n",
      "5   0.603448    LR  0.017642\n",
      "6   0.724138   SVM  0.024997\n",
      "7   0.517241   GNB  0.003360\n",
      "8   0.551724  MLNN  0.023317\n",
      "9   0.413793   RVM  0.007981\n",
      "10  0.965517   DNN  0.004558\n",
      "11  0.913793   DNN  0.028488\n",
      "12  0.913793   DNN  0.028488\n",
      "13  0.896552   DNN  0.026655\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on average: {} +- {}, median: {}+-{}\".format(ACC.mean()[0], ACC.mean()[1], ACC.median()[0], ACC.median()[1]))\n",
    "print(\"+\"*40)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "###########\n",
    "##Runs.append(AllResults)\n",
    "final_df = pandas.DataFrame()\n",
    "for idx, df in enumerate(Runs):\n",
    "    df['run'] = idx\n",
    "    final_df = final_df.append(df, ignore_index = True)\n",
    "final_df = final_df.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "final_df['pred']= pandas.to_numeric(final_df['pred'])\n",
    "final_df.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/patient_results_\"+Rocket.SET_NAME+\".csv\")\n",
    "final_df = final_df.groupby([Rocket.MODEL_PARAMETERS['ID'], 'method']).agg({'pred': [numpy.mean, numpy.median, numpy.std]})\n",
    "final_df.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/patient_results_agg_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through models..\n",
    "### Tree weights\n",
    "top_genomes_weights = pandas.DataFrame()\n",
    "#top_genomes.index = Rocket.DATA_merged_processed\n",
    "for mod in MODELS:\n",
    "    if(mod['method'] in ['RandomForest', 'GBM', 'AdaBoost', 'ExtraTrees']): # RF, ET, GBM, ADA\n",
    "        top_genomes_weights[mod['method']]=mod['model'].feature_importances_\n",
    "        # column normalise\n",
    "        top_genomes_weights[mod['method']] = top_genomes_weights[mod['method']]/top_genomes_weights[mod['method']].max()\n",
    "        \n",
    "top_genomes_weights.index = Rocket.DATA_merged_processed.drop(['target', 'ID'], axis=1).columns\n",
    "#top_genomes['ALL'] = top_genomes.sum(axis=1)\n",
    "top_genomes_weights['MED'] = top_genomes_weights.median(axis=1)\n",
    "top_genomes_weights = top_genomes_weights.sort_values(by='MED', ascending=False)\n",
    "       \n",
    "### Coefficients\n",
    "top_genomes_coeffs = pandas.DataFrame()\n",
    "for mod in MODELS:\n",
    "    if(mod['method'] in ['LR', 'SVM']):\n",
    "        top_genomes_coeffs[mod['method']] = mod['model'].coef_[0,:]\n",
    "        top_genomes_coeffs[mod['method']] = top_genomes_coeffs[mod['method']]/top_genomes_coeffs[mod['method']].max() #\\\n",
    "                                                               #  -top_genomes[mod['method']].min())\n",
    "                                                                 #+numpy.abs(top_genomes[mod['method']].min())\n",
    "top_genomes_coeffs.index = Rocket.DATA_merged_processed.drop(['target', 'ID'], axis=1).columns\n",
    "#top_genomes['ALL'] = top_genomes.sum(axis=1)\n",
    "top_genomes_coeffs['MEAN'] = top_genomes_coeffs.mean(axis=1)\n",
    "top_genomes_coeffs = top_genomes_coeffs.sort_values(by='MEAN', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_genomes_coeffs.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/coeffs_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_genomes_weights.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/weights_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF9 = top_genomes_weights['RandomForest'].quantile(q=0.9)\n",
    "GBM9 = top_genomes_weights['GBM'].quantile(q=0.9)\n",
    "ADA9 = top_genomes_weights['AdaBoost'].quantile(q=0.9)\n",
    "ET9 = top_genomes_weights['ExtraTrees'].quantile(q=0.9)\n",
    "Overlapping_genomes = set(top_genomes_weights.loc[top_genomes_weights['RandomForest']>RF9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['GBM']>GBM9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['AdaBoost']>ADA9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['ExtraTrees']>ET9].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "#from scipy.dspatial.distance import cosine\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cdist\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TransPosed = Rocket.DATA_all_samples.T # all microarrays, may be multiple per patient versus all probesets, may be multiple per genome\n",
    "Normal = Rocket.DATA_merged_processed.loc[:, (Rocket.DATA_merged_processed.columns !='target') & \n",
    "                                             (Rocket.DATA_merged_processed.columns !='ID')]\n",
    "#AllNormal = Rocket.DATA_merged\n",
    "#probeset_weights = Rocket.get_probeset_weights(method = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 9827_corr2.CEL, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'cosine', normalised = True, inflation = 2, minkowski_dim=1)\n",
    "##### apply Markov clustering\n",
    "#######################\n",
    "# non-distributed, non-sparse version, only for small-sized problems (N is order 1000)\n",
    "e = 2\n",
    "r = 2 \n",
    "epsilon = 1e-7\n",
    "convergence = 0.001\n",
    "num_iter = 10\n",
    "Orientation = 'col' # columnwise or rowwise\n",
    "\n",
    "# add loop\n",
    "def add_loop(df_matrix, value=0): \n",
    "    for i in df_matrix.index:\n",
    "        df_matrix.loc[i, i] = value\n",
    "    return df_matrix\n",
    "patient_sim = add_loop(patient_sim, 1)\n",
    "patient_sim = patient_sim - epsilon\n",
    "\n",
    "def normalise(sim, type = 'col'):\n",
    "    if(type == 'col'):\n",
    "        # column normalisation\n",
    "        for variable in sim.keys():\n",
    "            col_vec = sim[variable]\n",
    "            sum_val = sum([p for p in col_vec])\n",
    "            sim[variable] = sim[variable]/sum_val\n",
    "    elif (type == 'row'):\n",
    "        # row normalisation\n",
    "        for variable in sim.keys():\n",
    "            row_vec = sim.loc[variable, :]\n",
    "            sum_val = sum([p for p in row_vec])\n",
    "            sim.loc[variable,:] = sim.loc[variable,:]/sum_val\n",
    "    return sim\n",
    "\n",
    "# step E: expansion, get the nth power of the matrix\n",
    "def expansion(sim):\n",
    "    X = numpy.array(sim)\n",
    "    VarList = sim.keys()\n",
    "    if e == 1:\n",
    "        return sim\n",
    "    elif e > 1:        \n",
    "        return pandas.DataFrame(numpy.linalg.matrix_power(X, e), index = VarList, columns = VarList)\n",
    "     \n",
    "# step I: inflation, per column raise by rth power and column normalise\n",
    "def inflation(sim, type = 'col'):    \n",
    "    if type == 'col':\n",
    "        Axis = 0\n",
    "    elif type == 'row':\n",
    "        Axis = 1\n",
    "    return sim.apply(lambda x: x**r/sum(x**r), axis = Axis)\n",
    "\n",
    "# remove weak connections, values < epsilon\n",
    "def clean(sim):\n",
    "    return sim.applymap(lambda x:0 if x<epsilon else x)\n",
    "    \n",
    "def difference(old, new):\n",
    "    # relative zeroes over entire array\n",
    "    #return (new.apply(lambda x: numpy.ceil(x-epsilon)) - old.apply(lambda x: numpy.ceil(x-epsilon))).sum().sum()/len(old)**2    \n",
    "    return abs(new - old).sum().sum()/len(old)**2    \n",
    "\n",
    "#patient_sim = normalise(patient_sim, type = Orientation)\n",
    "_sim_a = patient_sim\n",
    "for i in range(0,num_iter):\n",
    "    # repeat E and I until convergence, the row-wise elements form the clusters.\n",
    "    _sim_b = clean(inflation(expansion(_sim_a), type = Orientation))\n",
    "    _sim_a = normalise(_sim_a, type = Orientation)\n",
    "    #if ((difference(_sim_a, _sim_b)) < convergence) & (i>0):\n",
    "    #    print(difference(_sim_a, _sim_b))\n",
    "    #    print(\"CONVERGED after \", i, \" iterations\")\n",
    "    #    break;\n",
    "    _sim_a = _sim_b\n",
    "\n",
    "result_mcl = clean(_sim_b)\n",
    "result_mcl.loc[result_mcl.loc['9827_corr2.CEL',:]>epsilon, '9827_corr2.CEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 patient clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'pearson', normalised = False, inflation=1, minkowski_dim=1)\n",
    "##### apply Affinity Propagation\n",
    "#######################\n",
    "X = numpy.array(patient_sim)\n",
    "af = AffinityPropagation(preference=-10).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "patient_clusters = patient_sim.keys()[cluster_centers_indices].values\n",
    "patient_cluster_members = af.labels_\n",
    "print(\"There are {} patient clusters\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AggResults = Rocket.DATA_merged\n",
    "AggResults = _helpers._preprocess(AggResults, Rclass = Rocket)\n",
    "#AggResults = _helpers._group_patients(AggResults, method = 'mean')\n",
    "AggResults['cluster_ap'] = patient_cluster_members\n",
    "\n",
    "#AggResults.groupby(['Treatment risk group in ALL10', 'cluster_ap']).agg({'Microarray file': pandas.Series.nunique})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "AggResults['FU_RFS'] = pandas.to_numeric(AggResults['FU_RFS'])\n",
    "AggResults['FU_EFS'] = pandas.to_numeric(AggResults['FU_EFS'])\n",
    "AggResults['FU_OS'] = pandas.to_numeric(AggResults['FU_OS'])\n",
    "AggResults['WhiteBloodCellcount'] = pandas.to_numeric(AggResults['WhiteBloodCellcount'])\n",
    "AggResults['Age'] = pandas.to_numeric(AggResults['Age'])\n",
    "AggResults['Gender'] = pandas.to_numeric(AggResults['Gender'])\n",
    "AggResults['code_RFS']= pandas.to_numeric(AggResults['code_RFS'])\n",
    "AggResults['code_EFS']= pandas.to_numeric(AggResults['code_EFS'])\n",
    "AggResults['code_OS']= pandas.to_numeric(AggResults['code_OS'])\n",
    "\n",
    "AggResults['mutations_NOTCH_pathway'] = pandas.to_numeric(AggResults['mutations_NOTCH_pathway'])\n",
    "AggResults['mutations_PTEN_AKT_pathway'] = pandas.to_numeric(AggResults['mutations_PTEN_AKT_pathway'])\n",
    "AggResults['mutations_IL7R_pathway'] = pandas.to_numeric(AggResults['mutations_IL7R_pathway'])\n",
    "#AggResults.replace(to_replace=9999, value=0.5, inplace=True)\n",
    "AggResults[['mutations_NOTCH_pathway', \n",
    "            'mutations_PTEN_AKT_pathway', \n",
    "            'mutations_IL7R_pathway']] = AggResults[['mutations_NOTCH_pathway', \n",
    "                                                    'mutations_PTEN_AKT_pathway', \n",
    "                                                    'mutations_IL7R_pathway']].replace([9999],[numpy.nan],\n",
    "                                                                                       inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "AggResults['comb_mutations_NOTCH_IL7R'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_NOTCH_PTEN'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_PTEN_AKT_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN'] =  AggResults['mutations_PTEN_AKT_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN_NOTCH'] =  AggResults['mutations_PTEN_AKT_pathway']\\\n",
    "                                                + AggResults['mutations_IL7R_pathway']\\\n",
    "                                                + AggResults['mutations_NOTCH_pathway']\n",
    "\n",
    "\n",
    "patient_count = AggResults.groupby(['cluster_ap']).agg({'labnr_patient': pandas.Series.nunique})\n",
    "Clustered_by_patients_whitebloodcells = AggResults[AggResults['WhiteBloodCellcount'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'WhiteBloodCellcount': numpy.mean,\n",
    "    'Age': numpy.mean, \n",
    "    'Gender': numpy.mean})\n",
    "\n",
    "# Cancer_gene\n",
    "# Treatment_protocol\n",
    "# Treatment_risk_group_in_ALL_10\n",
    "\n",
    "Clustered_by_patients_CODE = AggResults.groupby(['cluster_ap']).agg(\n",
    "    {'code_RFS': numpy.mean, \n",
    "     'code_EFS': numpy.mean,\n",
    "     'code_OS': numpy.mean})\n",
    "\n",
    "Clustered_by_patients_FU_RFS = AggResults[AggResults['FU_RFS'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'FU_RFS': numpy.median, \n",
    "     'FU_EFS': numpy.median,\n",
    "     'FU_OS': numpy.median})\n",
    "Clustered_by_patients_NotchPath = AggResults[AggResults['mutations_NOTCH_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_NOTCH_pathway': numpy.mean})\n",
    "Clustered_by_patients_IL7RPath = AggResults[AggResults['mutations_IL7R_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_IL7R_pathway': numpy.mean})\n",
    "Clustered_by_patients_PTENAKTPath = AggResults[AggResults['mutations_PTEN_AKT_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_PTEN_AKT_pathway': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_IL7R = AggResults[AggResults['comb_mutations_NOTCH_IL7R'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_IL7R': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_PTEN = AggResults[AggResults['comb_mutations_NOTCH_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN = AggResults[AggResults['comb_mutations_IL7R_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN_NOTCH = AggResults[AggResults['comb_mutations_IL7R_PTEN_NOTCH'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN_NOTCH': numpy.mean})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_agg = pandas.merge(Clustered_by_patients_whitebloodcells, Clustered_by_patients_CODE, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN_NOTCH, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_IL7R, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_PTEN, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_FU_RFS, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_IL7RPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_NotchPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_PTENAKTPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, patient_count, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Cluster centers:\",patient_sim.keys()[cluster_centers_indices].values)\n",
    "print(patient_cluster_members)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    class_members = patient_cluster_members == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.', \n",
    "             label = patient_sim.keys()[cluster_centers_indices[k]])\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.legend()\n",
    "        \n",
    "plt.title('Estimated number of clusters from Affinity Propagation: %d' % n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATE graph from similarity matrix\n",
    "##################\n",
    "# nodes\n",
    "VarList = TransPosed.keys()\n",
    "nodes = []\n",
    "node_index = 0\n",
    "for patient_name in VarList:\n",
    "    nodes.append((node_index, {'name': patient_name}))\n",
    "    node_index = node_index + 1\n",
    "\n",
    "edges = []\n",
    "# edges\n",
    "patient_sim = patient_similarity(Normal, sim_type = 'pearson', normalised = True, inflation=2)\n",
    "node_index_x = 0\n",
    "node_index_y = 0\n",
    "for patient_name_x in VarList:\n",
    "    for patient_name_y in VarList:        \n",
    "        edges.append((node_index_x, node_index_y, patient_sim.iloc[node_index_x, node_index_y]))\n",
    "        node_index_y = node_index_y + 1\n",
    "    node_index_x = node_index_x + 1\n",
    "    node_index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_weighted_edges_from(edges, weight = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:126: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  b = plt.ishold()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:138: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold(b)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFCCAYAAABSJMy8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtwVOXh//HP2U2y2RCIJCGiQRHlIo0gVFJFLCJCNQIi\nIFpBBhD8AWpAgcHS4qVVy4yDo1YdryNUR7GUSlEErYpGBBwC4ZKEBIlyUS5JIDESkqwJe35/bPGr\nFUIu5+zZy/s102mFc57nM2rz4Xn27HMM0zRNAQAAS7mcDgAAQCSiYAEAsAEFCwCADShYAABsQMEC\nAGADChYAABtQsAAA2ICCBQDABhQsAAA2oGABALABBQsAgA0oWAAAbEDBAgBgAwoWAAAbULAAANiA\nggUAwAYULAAANqBgAQCwAQULAIANKFgAAGxAwQIAYAMKFgAAG1CwAADYgIIFAMAGFCwAADaIcTpA\nSCgrk5YskXbskKqqpKQkqXdvafJkqUMHp9MBAMKQYZqm6XQIx+TmSgsXSmvWBP66ru7/fs/rlUxT\nysqS5s+XMjOdyQgACEvRW7DPPy/NnSvV1gaK9HQMI1C2ixZJM2YELx8AIKxF5xbxyXKtqTnztaYZ\nuG7u3MBfU7IAgCaIvhVsbq40aFDTyvV/JSRIOTlSv36WxwIARJboe4p44cLAtnBL1NYG7gcA4Ayi\nawVbViZ17vzzh5maKz5e2r+fp4sBAI2KrhXskiWtH8MwrBkHABDRoqtgd+xo3epVCmwT5+dbkwcA\nELGiq2CrqqwZp7LSmnEAABErugo2Kcmacdq3t2YcAEDEiq6C7d078JBSa3i9Uq9e1uQBAEQsniJu\nJjM+XgZPEQMAziC6VrBpaYGzhQ2jRbefkPR2XZ2MtDRrcwEAIk50FawUOLjf623RrXWSTh4zYRiG\nrrzySstiAQAiS/QVbGZm4OD+hIRm3WYmJGiOpC0/+bWNGzfKMAx9+umnViYEAESA6PoM9qda+DYd\no5HtZZ/Pp7i4OBvCAgDCTfStYE+aMSNwcP+oUYEni/9329jrDfz6qFGB6/77Fh3TNHX77befckiP\nx9NoAQMAokf0rmB/qrw8cPxhfn7gEIn27QNfxZk06bRPC1dXV6tt27anHfL666/XmpMvcgcARB0K\ntpXOtGJds2aNrr/++iClAQCEiujdIraIaZqaOnXqaX8/KytLhmHo2LFjQUwFAHAaK1iLVFRUKCUl\npdFrXC6XGhoa+JwWAKIAK1iLJCcny+/3N3qN3++Xy+ViyxgAogAFayHDMM64ZSxJH3zwgQzD0LJl\ny4KUDAAQbGwR2+TgwYNKT09v0rXl5eVKTU21OREAIJgoWBv5/X653e4mXevxeHT8+PEmXw8ACG1s\nEdvI5XI1ejDFT/l8PsXExGjgwIFBSAYAsBsr2CD58ssv1aNHjyZf/9JLL+nOO++0MREAwE4UbBDV\n19fL4/GoOX/L9+zZowsuuMC+UAAAW7BFHESxsbHy+/0aO3Zsk+/p0qWL4uPj5fP5bEwGALAaBeuA\nZcuWKS8vr8nX+3w+xcfH6/LLL2/W6hcA4BwK1iF9+/ZVTU1Ns+7ZtGmTXC6XFi1aZFMqAIBV+Aw2\nBIwYMUKrVq1q9n0FBQXKyMiwIREAoLUo2BCRk5OjQYMGNfu+2NhYVVRUKDEx0fpQAIAWo2BDSFVV\nlc4666wW3durVy9t27ZNLhe7/gAQCvhpHEKSkpLk9/t17bXXNvve/Px8ud1uzZ8/34ZkAIDmYgUb\not59913deOONLb4/JyeHU6EAwEEUbAgrLS3VOeec0+Kv5rjdbh0+fJgXCQCAA9giDmFnn3226uvr\n1b9//xbdf+LECXXo0EHdunXTDz/8YHE6AEBjKNgQ53a7tWHDBv39739v8RglJSXyeDyaNm0aB1UA\nQJCwRRxG9u3bZ8m5xCtXrmzV57sAgDOjYMOMz+fT5Zdfru3bt7dqHJfLpa+++ooXCQCATdgiDjMe\nj0fbtm3T008/3apx/H6/unTpok6dOun48eMWpQMAnMQKNowVFhbqkksusWSsW265RUuXLuWgCgCw\nCD9Nw1hGRoaOHTum7t27t3qsZcuWye12a/HixRYkAwBQsGEuMTFRxcXFevDBBy0Z74477pDL5VJB\nQYEl4wFAtGKLOIJ88cUXLf7O7Kl06NBBRUVFSklJsWxMAIgWrGAjyBVXXKGjR4/q/PPPt2S88vJy\npaamKisri4MqAKCZKNgIk5ycrD179mjmzJmWjfn+++/L4/Fo0aJFHFQBAE3EFnEE+89//qPrrrvO\n8nHXrVunq666yvJxASCSULAR7uDBg+rTp4/Ky8stHTcpKUkFBQXq1KmTpeMCQKRgizjCnXvuuTpw\n4IAmTJhg6bhVVVU677zzdOWVV3JQBQCcAgUbBWJjY/Xaa69p2bJllo+9ceNGJSYm6g9/+IP8fr/l\n4wNAuGKLOMqUlJSoX79+qqqqsmX8VatWadiwYbaMDQDhhBVslOnatasOHTqk4cOH2zL+8OHD1aZN\nG+3atcuW8QEgXFCwUcjr9erdd9/VCy+8YMv4NTU1uvjii9WrVy9VVFTYMgcAhDq2iKPc9u3b1b9/\nf9XW1to2x5133qlnn31WcXFxts0BAKGGFWyUu/TSS3Xo0CENHDjQtjlefvlleTweLVmyhIMqAEQN\nChZKSkrSp59+qoULF9o6z+TJk+X1erV582Zb5wGAUMAWMX5m/fr1Gjx4sO1nD1900UX67LPPdO65\n59o6DwA4hRUsfmbAgAH65ptvdOmll9o6z1dffaX09HSNHTuWgyoARCQKFr+QlpamLVu2aN68ebbP\ntXz5ciUmJurJJ5/koAoAEYUtYjRq9erVGjlypBoaGmyfKyYmRqtXr9bQoUNtnwsA7EbB4oz27dun\noUOHavfu3UGZr2PHjsrJyVH37t2DMh8A2IEtYpxR586dlZ+frylTpgRlvsOHD6tHjx4aOnQoB1UA\nCFsULJrE4/HolVde0RtvvKGYmJigzPnRRx8pJSVFf/rTn2x/qhkArMYWMZqtqKhIQ4YM0cGDB4M2\np8vl0tKlSzV27FgZhhG0eQGgpVjBotl69uypXbt2afTo0UGb0+/369Zbb1VKSory8vKCNi8AtBQF\nixZJTEzU8uXL9dxzz8ntdgdt3srKSl122WXKzMwM6goaAJqLLWK0Wm5urrKysnT06NGgzz19+nQt\nWrRIbdq0CfrcANAYChaWqKio0NixY7V27VpH5n/xxRc1depUuVxsygAIDfw0giWSk5P14Ycf6tFH\nHw3aU8Y/NW3aNLVr106ffPJJ0OcGgFNhBQvLrV27VqNGjdL333/vyPw9e/bUypUr1a1bN0fmBwCJ\nFSxsMHjwYO3cuVP9+vVzZP6ioiJ1795dv//971VZWelIBgCgYGGL9PR0bdiwQbNnz1ZsbKwjGf7x\nj38oOTlZCxcuVH19vSMZAEQvtohhuxUrVmjChAmOvpbO4/Horbfe0siRIzmoAkBQULAIipKSEo0Y\nMULFxcWO5jjvvPO0cuVK9e3b19EcACIfW8QIiq5duyovL0+TJ09WXFycYzm++eYb/frXv1ZWVhYH\nVQCwFQWLoPF6vXr11Vf1wgsvKCEhwdEs77//vtLT0zVv3jxHt64BRC62iOGI7du3a8SIEfrmm2+c\njiK3262XXnpJkyZN4qAKAJahYOGYqqoqTZw4UR988IHq6uqcjqOUlBQtX75cgwYNcjoKgAjAH9fh\nmKSkJK1YsUJ//etfHd8ylqSjR4/qmmuuUf/+/VVSUuJ0HABhjhUsQsL69es1evRolZWVOR3lR1On\nTtXjjz+u9u3bOx0FQBiiYBEyysrKdMstt2jTpk2qra11Oo4kyTAMLVq0SNnZ2Y4dmAEgPLFFjJCR\nlpamjz/+WHPmzFFiYqLTcSRJpmlqzpw5SklJ0cqVK8WfRwE0FStYhKQ1a9Zo3LhxqqqqCqlS69mz\np95880316dPH6SgAQhwrWISkrKwsbdu2TX369AmJB6BOKioqUt++fTV27FgOqgDQKAoWIatz587a\nuHGjJk2apKSkJKfj/Mzy5cuVnp6uBQsWqKamxuk4AEIQW8QIC0uXLtX06dN17NixkNoylgIvEnjx\nxRc1YcIEDqoA8CMKFmGjqKhII0eO1MGDB0PyeMPzzz9fr7/+ugYOHOh0FAAhgD9uI2z07NlTeXl5\nuvHGG5WSkuJ0nF/Yv3+/rr76ag0ZMoSDKgBQsAgviYmJeuONN/TII48oKSkpJN/t+vHHH6tbt266\n5557VFlZ6XQcAA5hixhhKzc3V6NHj9Z3332n6upqp+Ocktvt1hNPPKG77rqLgyqAKEPBIqwdPXpU\nEyZM0JYtW0LqmMX/lZqaqldffVXDhw8PyVU3AOuxRYywlpKSolWrVmnWrFkhfWbwkSNHdOONNyoz\nM1Pbtm1zOg6AIGAFi4ixdu1a3XbbbaqpqQnZLeOTxo0bp0WLFumcc85xOgoAm1CwiCgHDhzQrbfe\nqq+//lqHDh1yOk6jDMPQgw8+qHnz5oXUaVUArMEWMSJKenq6PvnkE40fP16pqalOx2mUaZr685//\nrLPPPluvvfaa/H6/05EAWIgVLCLWihUrNHXqVPl8vpA8mKKDpImSeks6S5K/bVv1GjdOFz7yiNSh\ng7PhALQaBYuIVlJSojFjxqiiokLffvut03EkSf0kzZeUJcmU9NPN4RpJMS6Xfrj2WiU+9piUmelE\nRAAWoGAR8Wpra5Wdna3Vq1c7/rnsNElPSIqX5G7kuhOSTsTEqOFPfwp8Prtpk1RYKB0/LtXVSfHx\nUmKi9KtfSb/5jTR5MqteIMRQsIgaixcv1uzZs1VfX+/IlvHJcm3TjHvM//6nSQ9LJCVJF14omaZk\nGIHC7dBB6t2bAgYcQMEiqmzfvl1jxoxRQ0OD9u3bF7R5+0n6VM0rV8u43VJMjHTDDdL8+Ww7A0HC\nU8SIKpdeeqm2bNmiyy67TBdccEHQ5p2vwLawI06ckHw+acUKacAA6fnnnUoCRBVWsIhKpmnqySef\n1GOPPaa6ujpbX5reQdI+SV7bZmiB3/5W+uwzp1MAEY0VLKKSYRiaPXu2Vq5cqfbt29u6mp2owOeo\nIWXdusDntM8843QSIGJRsIhqV111lfLy8nTRRRepe/futszRWz//Kk5ImTlTOuccKTfX6SRAxKFg\nEfXS0tL0wQcf6JZbbtHZZ58tr9fazdyzLB3NBocPB77qc//9TicBIgqfwQI/sWbNGk2cOFGJiYna\ns2ePJWO+JmmCJSMFwe9/Ly1d6nQKICKwggV+IisrS7m5uUpNTVVGRoYlY1YoBD+DPZ233pL+8Aen\nUwARgRUscAo+n09z5szRO++8o/LyctXV1bV4rGoFPoMNm9esG0bg5Kh+/ZxOAoQ1ChZoxNKlS5Wd\nna127do1e8v45OESYVWuJ116qcSL4YFWoWCBMygqKtLo0aOVkJCgvLy8Jt0zTdJzCnwGE3blelJZ\nGccrAq3AZ7DAGfTs2VO5ubnq0aOHunbtKo/H0+j10yQ9rcBh/mFbrpI0caLTCYCwxgoWaCLTNPXC\nCy/ogQceUNu2bbV3795fXOPomcN24McD0GKsYIEmMgxDM2bM0Jo1a+T3+3X55Zf/4pr5CrEjEVur\nqMjpBEDYomCBZsrMzFReXp5SUlJ0ySWXKC4uTlLgzOEsRdj/qe64w+kEQNiKqJ8FQLCkpKTo3Xff\n1W233ab27durc+fOishPLDdvdjoBELb4DBZopbVr12r8+PFacuKErisvdzqO9fgRAbQIBQtY4MCB\nA9rbq5cGVFY6HcV6/IgAWoQtYsAC6enp6n/99U7HABBCKFjAIq4+fQLHDAKA2CIGrFNWJnXsGFFb\nqn5JZkOD3G6301GAsMMKFrBKWpqUmup0CkuZkmJjY1VSUuJ0FCDsULCAlS67zOkEljElHVfgBKtu\n3brpiSeecDoSEFYoWMBK11zjdAJL/b+f/O+5c+ee8vQqAKfGZ7CAlcrKpLPPdjqFJUyd+k/gHo9H\nBw8eVHJycrAjAWGFFSxgpbQ0KSPD6RSW2H+aX/f5fEpJSdHbb78d1DxAuKFgAatFwGeVpqRnznDN\nmDFjNHbs2GDEAcISW8SAHfr0kbZvdzpFi9VKOl/SkSZcm5ycrEOHDv340gMAAaxgATu8/LIUE+N0\nihbxS1qtppWrJFVUVMjj8WjLli02pgLCDwUL2CEzU/rb36QwPKChVtLCFtzXr18/3X///VbHAcIW\nW8SAnZ5/Xrr77rA53ckv6S5JL7ZijK5du2rXrl1yufjzO6IbBQvY7T//ka67zukUZ2RKWixpigVj\nuVwu7d+/X+np6RaMBoQn/ogJ2O13v5MGDXI6RaNMSWtlTblKkt/vV6dOnfTyyy9bNCIQfljBAsGQ\nmysNGCDV1zud5BdMBb7zeoFN4w8cOFA5OTk2jQ6ELlawQDBkZkpPPy3Fxjqd5Bd8ksbYOP5nn32m\nNm3aqLq62sZZgNBDwQLBMmNGoGQ9HqeT/Oi4pHsl2f0Fm5qaGrVt21YffvihzTMBoYOCBYJpxgzp\n88+l0aMDRevQ13j8CpTrHLXuieHm+t3vfqfJkycHcUbAOXwGCzilvFxaskTKz5fy8qTiYunECVun\nbPjvf95T4LuuTh0NkZaWpoMHD/Iid0Q0ChYIJUVF0ty5UkGBVFEh+f2B79D6/YEHpPz+Jg1z8v/U\nPkkVkiolFUjKlfR3Nf2UJrvt3LlTPXv2dDoGYAsKFgg35eXSs89Kq1ZJpaWB0jUM/ZCUpF3ffadd\n332n2m7dNHvHjpAp0sY88sgjWrBggdMxAMtRsECE2bhxo7KzsxUXF6fvv/9ehYWFTkc6o4yMDBUU\nFDgdA7AUDzkBEaZ///7atGmTpkyZoiNHjujmm2+W1+t1OlajCgsLFRsbq6NHjzodBbAMBQtEIJfL\npSlTpqi4uFjp6elKTEzUTTfd5HSsRjU0NCg1NVVLly51OgpgCbaIgShQWFiomTNnqrS0VKZpaufO\nnU5HalRWVpZWr17tdAygVShYIEqYpqm3335bc+bM0cUXX6x169appqbG6VinlZiYqPLycsXHxzsd\nBWgRtoiBKGEYhsaMGaOdO3fqiiuukNfr1bBhw5yOdVrV1dXyer364osvnI4CtAgFC0SZhIQEPfzw\nw9q8ebPi4+PVpUsXZWRkOB3rtPr376+ZM2c6HQNoNraIgSj30UcfadasWUpJSVFeXp6OHz/udKRT\n6tSpk/bt28eL3BE2+DcViHJDhgzRtm3bNGbMGHm9Xg0dOtTpSKf07bffyu12a+/evU5HAZqEggWg\n2NhYzZo1S4WFhTr//PPVsWNH/epXv3I61il16dJFTz31lNMxgDNiixjAL+Tm5io7O1s+n08lJSUh\n+S7XzMxMbdq0yekYwGlRsABOye/36/XXX9f8+fPVtWtXrVu3zulIvxAbG6sjR46oXbt2TkcBfoEt\nYgCn5HK5NHHiRBUVFek3v/mNUlJSdPHFFzsd62fq6+uVlJSk9957z+kowC+wggXQJMXFxZo1a5b2\n7NmjQ4cOhdy28ZgxY7R8+XKnYwA/omABNJlpmnrnnXd03333KSUlRZs3b3Y60s8kJSXpyJEjiomJ\ncToKwBYxgKYzDEMjR47Uzp07NXLkSCUnJ6t79+5Ox/pRVVWVYmNjlZ+f73QUgIIF0Hzx8fFasGCB\ntm3bpr59+6pTp05q27at07F+1Lt3b/3xj390OgaiHFvEAFotJydH2dnZMk0zpF6cfsEFF+jrr7+W\nYRhOR0EUYgULoNWuvvpq5eXlafr06UpNTVW3bt2cjiRJ2rt3r1wulw4fPux0FEQhChaAJWJiYnT3\n3XerqKhIgwcPVlpaWshsG59zzjlavHix0zEQZdgiBmCLrVu3Kjs7W6WlpSopKXE6jiRpwIAB+vzz\nz52OgShBwQKwjWmaevPNNzVv3jx5PB7t2bPH6UiKi4vTd999J6/X63QURDi2iAHYxjAMjR8/XsXF\nxbrllluUnJzs+LbxDz/8oISEBH366aeO5kDkYwULIGh2796te++9V9u3b9eBAwecjqMJEybotdde\nczoGIhQFCyDo3nvvPc2aNUs+n0/ffvuto1mSk5NVXl7Oi9xhOf6NAhB0w4YNU2Fhoe655x4lJycr\nMTHRsSwVFRVyu90h8yAWIgcFC8ARHo9H999/v3bs2KGRI0cqNTXV0TzdunXTY4895mgGRBa2iAGE\nhM8//1zZ2dk6dOiQSktLHcvRo0cPFRcXOzY/IgcFCyBknDhxQq+88ooeeOAB1dTU6Pjx445lqays\n1FlnneXY/Ah/bBEDCBlut1vTpk1TcXGxJk2a5GjBtW/fXsuWLXNsfoQ/VrAAQtaOHTs0c+ZMFRQU\n6OjRo45kGDJkiD788ENH5kZ4o2ABhDTTNLVs2TLNnTtXFRUVqqmpCXoGj8ejY8eOKTY2NuhzI3yx\nRQwgpBmGoVtvvVXFxcWaPXu2kpKSgp7B5/MpLi5OmzZtCvrcCF+sYAGEla+//lqzZ8/WJ598ou+/\n/z7o80+fPl3PP/980OdF+KFgAYSlDz74QDNnztT+/ftVV1cX1LlTU1NVWlrK6U9oFP92AAhL1113\nnfLz8/Xoo48G/WnjI0eOyO12h8R5yghdFCyAsBUXF6c5c+Zo586dmjhxotq0aRPU+Tt16qS//e1v\nQZ0T4YMtYgAR44svvtA999yjgoIC+Xy+oM2bkZGhgoKCoM2H8EDBAogofr9fixcv1v333x/U784a\nhqHq6molJCQEbU6ENraIAUQUl8ulKVOmqKSkRLNmzVJ8fHxQ5jVNU23atNHq1auDMh9CHytYABGt\nsLBQ2dnZ+vzzz1VfXx+UOYcNG6ZVq1YFZS6ELgoWQMQzTVMrVqxQdna2Dh48GJQ5PR6Pampq+CpP\nFOOfPICIZxiGRo8erd27d+vhhx+Wx+OxfU6fzye3283DT1GMggUQNRISEvTQQw9p165dGj16tNxu\nt+1z9urVS3PnzrV9HoQetogBRK2PP/5YM2bM0O7du22fKy0tzdEXySP4WMECiFrXXnutCgsL9dRT\nT8nr9do6V1lZmQzD0JEjR2ydB6GDggUQ1WJjYzVr1izt3btXd9xxhwzDsHW+Dh066OWXX7Z1DoQG\ntogB4Cdyc3M1bdo0bd261dZ5evfure3bt9s6B5xFwQLA//D7/Xr99dd1zz33qLq62rZ5DMNQXV2d\n4uLibJsDzmGLGAD+h8vl0sSJE3XgwAHNmTPHtnlM05TH41FOTo5tc8A5rGAB4AyKi4s1ffp0W4tw\n9OjR+te//mXb+Ag+ChYAmsA0Tb377ru64447bHuJQHx8vI4fP87pTxGCf4oA0ASGYejGG2/Ut99+\nq0ceecSWOerq6uR2u7Vnzx5bxkdwUbAA0Azx8fFasGCB9u/fr5tuusmWOS688EItWLDAlrERPGwR\nA0Ar5OTkaPz48Tpw4IDlY3fs2FGHDh2yfFwEBytYAGiFq6++Wnv37tWzzz5r+SEVhw8flmEYOnbs\nmKXjIjgoWABopZiYGN19990qKyvT5MmTLR+/Xbt2evPNNy0fF/ZiixgALLZ161bddttt2rVrl6Xj\n9u3bV3l5eZaOCftQsABgA9M0tXTpUk2cOFENDQ2WjWsYhurr64Pyqj20DlvEAGADwzA0btw4VVRU\n6L777rNsXNM0FRMTo9zcXMvGhD1YwQJAEOzevVvjxo3T5s2bLRvz5ptv1j//+U/LxoO1KFgACKL3\n3ntPY8aMkc/ns2S8+Ph41dbWWjIWrMUWMQAE0bBhw1RVVWXZaVB1dXUyDIPvy4YgChYAgszj8WjB\nggU6cOCABg8ebMmY5557rh566CFLxoI12CIGAIetX79eWVlZlhwokZaWptLSUgtSobVYwQKAwwYM\nGKDKyko999xzrR6rrKxMhmGopqbGgmRoDQoWAEKA2+3WXXfdpaNHj2rUqFGtHq9NmzZavny5BcnQ\nUmwRA0AI2rFjh6655hpVVFS0apw+ffpo69atFqVCc1CwABCiTNPUW2+9pXHjxrV6rBMnTvAi9yDj\n7zYAhCjDMHTbbbepurq61S8RcLvdys/PtygZmoIVLACEia+//loDBgzQ4cOHWzwGpz8FDwULAGFm\n9erVGjZsWIvvj4uLs+wkKZweW8QAEGZuuOEG+Xw+3XvvvS26/4cffpBhGK1+gAqNo2ABIAzFxcXp\nySef1KFDh3ThhRe2aIyUlBTLjmzEL7FFDAARYMOGDRowYECL7k1OTtbRo0ctTgRWsAAQAa688kqd\nOHFCf/nLX5p9b0VFxY8vcod1KFgAiBAul0sPPPCAKisr1bNnz2bfHxcXp3feeceGZNGJLWIAiFCF\nhYXq1auXmvtjPiMjQwUFBTalih6sYAEgQmVkZOjEiRN65plnmnVfYWGhDMOwKVX0YAULAFGgtrZW\nV111lfLy8pp135dffqlu3brZlCqysYIFgCjg9Xq1ZcsW7dmzp1mr0+7du1vydp9oxAoWAKLQG2+8\nodtvv71Z91AXzUPBAkCUqq+v16BBg7Rhw4Ym33Ps2DElJibamCpysEUMAFEqNjZW69evV2lpqdxu\nd5Puadu2rR5++GF7g0UIVrAAAEnSqlWrNGLEiCZd6/V6VVNTY3Oi8EbBAgB+5Pf7NXToUK1du7ZJ\n1zc0NDR59Rtt2CIGAPzI5XLp448/VlVVVZOuj4mJ0cqVK21OFZ5YwQIATmvdunUaOHDgGa/r1KmT\nvvnmmyAkCh8ULACgUaZpavjw4Vq9enWTrkUABQsAaJK6ujp5vd4zXrdr1y517949CIlCG5/BAgCa\nJD4+XqZpnvG4xR49eui3v/1tkFKFLlawAIAWGTVqlP797383ek00VwwFCwBosYaGBsXGxjZ6TXV1\ntdq0aROkRKGDLWIAQIvFxMTINE3t3r37tNckJiZq+vTpQUwVGljBAgAsc9NNNzX6vdhoqhwKFgBg\nKdM05XKdfoM0WmqHLWIAgKUMw5Bpmtq/f/9pf/+VV145/QBlZdLjj0u33y6NGBH478cfl8rLbUps\nD1awAABnE4a/AAACi0lEQVRbZWVl6f333z/l7/2sgnJzpYULpTVrAn9dV/d/v+f1SqYpZWVJ8+dL\nmZk2JrYGBQsACArDME7566ZpSs8/L82dK9XWBor09IMEynbRImnGDJuSWoOCBQAETWlpqTp27Piz\nX5sm6bn4eLl/umI9k4SEkC9ZChYAEHT9+vXTli1b1E/Sp5Ja9C3ZhAQpJ0fq18/SbFahYAEAjnnb\nMDRSUoveKGsY0qhR0r/+ZXEqa1CwAABnlJVJnTv//GGm5oqPl/bvlzp0sC6XRfiaDgDAGUuWtH4M\nw7BmHBtQsAAAZ+zY0brVqxR46jg/35o8FqNgAQDOqKqyZpzKSmvGsRgFCwBwRlKSNeO0b2/NOBaj\nYAEAzujdO/CQUmt4vVKvXtbksRhPEQMAnMFTxAAA2CAtLXC28GmOUDwjw5BuuCEky1ViBQsAcFJu\nrjRokFRT0/x7Q/wkJ1awAADnZGYGzhROSGjefSfPIg7RcpWkGKcDAACi3MkD+3mbDgAANti8OfA+\n2NWrA0VaW/t/v3fyfbA33BB4H2wIr1xPomABAKGlvDxw/GF+fuAQifbtA1/FmTQpZB9oOhUKFgAA\nG/CQEwAANqBgAQCwAQULAIANKFgAAGxAwQIAYAMKFgAAG1CwAADYgIIFAMAGFCwAADagYAEAsAEF\nCwCADShYAABsQMECAGADChYAABtQsAAA2ICCBQDABhQsAAA2oGABALABBQsAgA0oWAAAbEDBAgBg\nAwoWAAAbULAAANiAggUAwAYULAAANqBgAQCwAQULAIANKFgAAGxAwQIAYAMKFgAAG1CwAADYgIIF\nAMAGFCwAADagYAEAsAEFCwCADShYAABsQMECAGADChYAABv8f4noJqN/OpfrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f31beac128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### apply Spring-force\n",
    "#######################\n",
    "pos = nx.spring_layout(G, k = None, dim = 3, scale = 1.0)\n",
    "nx.draw_spring(G, k = 30, dim = 2, scale = 1.0, iterations =1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### APPLY community detector\n",
    "# maximize betweenness and modularity\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### LOAD IN DATA\n",
    "###################\n",
    "# https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
