{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ Firing up RexR! ++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from RexR import *\n",
    "import _helpers\n",
    "Rocket = RexR(datalocation = None, #'_data/genomic_data/data.pkl', \n",
    "              seed = 3123, \n",
    "              debug = False, \n",
    "              write_out=True,\n",
    "              set_name = 'ALL_10') # data to read in ALL_10, or MELA\n",
    "Rocket.load_probeset_data();\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD_LIST = ['RF','XGB', 'LGBM', 'ExtraTrees','SVM', 'LR'] #, 'NaiveBayes','MLNN', 'XGB'] # \n",
    "Runs = []\n",
    "nruns = 10\n",
    "SCALER = \"minmax\" # minmax, standard, normaliser\n",
    "GROUPING = \"mean\"\n",
    "FEAT_SELECTOR = None # \"low_variance\"\n",
    "SELECTOR_METHOD = \"FDR\" # mannwhitney\n",
    "DIM_TYPE =  None #\"LDA\", \"PCA\", \"PLS\" \n",
    "DIM_NUM = 1000\n",
    "Results = None\n",
    "ACC = pd.DataFrame()\n",
    "Rocket.VIZ = False\n",
    "MODELS  = []\n",
    "Rocket.DATA_merged_processed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filtered\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 45 of 54613 features using FDR with p = 0.02\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 45) and class vector (58,)\n",
      "MODEL: RF accuracy:  0.8275862068965517 +/-: 0.023303786977984357\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filtered\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 45 of 54613 features using FDR with p = 0.02\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 45) and class vector (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: XGB accuracy:  0.7413793103448277 +/-: 0.02337134724173964\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filtered\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 45 of 54613 features using FDR with p = 0.02\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 45) and class vector (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: LGBM accuracy:  0.8275862068965516 +/-: 0.02134453932908154\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filtered\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 45 of 54613 features using FDR with p = 0.02\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 45) and class vector (58,)\n",
      "MODEL: ET accuracy:  0.8103448275862069 +/-: 0.02137156343458364\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filtered\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 45 of 54613 features using FDR with p = 0.02\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 45) and class vector (58,)\n",
      "MODEL: SVM accuracy:  0.8275862068965517 +/-: 0.014683097322811953\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filtered\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 45 of 54613 features using FDR with p = 0.02\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 45) and class vector (58,)\n",
      "MODEL: LR accuracy:  0.8275862068965517 +/-: 0.014683097322811953\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, nruns):\n",
    "    Rocket.SEED = np.random.randint(0,10000)    \n",
    "    for idx, METHOD in enumerate(METHOD_LIST):\n",
    "        preds, class_model, accuracy = Rocket.classify_treatment(model_type = METHOD, \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                        \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                          \"bias_removal\": False},\n",
    "                                                  \"feature_selection\": {\"type\": FEAT_SELECTOR, \n",
    "                                                                        \"pvalue\": 0.05,\n",
    "                                                                        \"method\": SELECTOR_METHOD}, # mannwhitney         \n",
    "                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \n",
    "                                                                    \"n_comp\": DIM_NUM}})\n",
    "        MODELS.append({'method': METHOD, 'model': class_model})\n",
    "        ACC = ACC.append(accuracy, ignore_index= True)\n",
    "        preds = [pred_[1]for pred_ in preds]\n",
    "        #len(Rocket.DATA_merged[Rocket.DATA_merged[\"array-batch\"].isin([\"cohort 1\", \"cohort 2\", \"JB\", \"IA\", \"ALL-10\"])])\n",
    "        if Results is None:\n",
    "            Results = Rocket.DATA_merged_processed.copy()\n",
    "        Results['pred'] = preds\n",
    "        Results['method'] = METHOD\n",
    "        if idx == 0:\n",
    "            AllResults = Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]]\n",
    "        else:\n",
    "            AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], \n",
    "                                                    'pred', \n",
    "                                                    'method', \n",
    "                                                    Rocket.MODEL_PARAMETERS['target']]], \n",
    "                                      ignore_index = True)\n",
    "\n",
    "    ####\n",
    "    ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613) and class vector (58,)\n",
      "Initial alpha = [[ 0.0833703]]\n",
      "   1 - L=-531.2776165 - Gamma= 1.9999588 (M=   2) - s=0.0100\n",
      "   2 - L=-354.2726923 - Gamma= 2.9998972 (M=   3) - s=0.0100\n",
      "   3 - L=-252.9503310 - Gamma= 3.9997715 (M=   4) - s=0.0100\n",
      "   4 - L=-164.9637113 - Gamma= 4.9996312 (M=   5) - s=0.0100\n",
      "   5 - L=-117.7610346 - Gamma= 5.9993920 (M=   6) - s=0.0100\n",
      "   6 - L=-77.3102847 - Gamma= 6.9991093 (M=   7) - s=0.0100\n",
      "   7 - L=-45.3074246 - Gamma= 7.9987309 (M=   8) - s=0.0100\n",
      "   8 - L=-24.6134908 - Gamma= 8.9981559 (M=   9) - s=0.0100\n",
      "   9 - L=-14.4232859 - Gamma= 9.9970633 (M=  10) - s=0.0100\n",
      "  10 - L=-8.0290595 - Gamma=10.9953084 (M=  11) - s=0.0100\n",
      "  11 - L=-3.6847554 - Gamma=11.9926930 (M=  12) - s=0.0100\n",
      "  12 - L=-1.2980519 - Gamma=12.9882301 (M=  13) - s=0.0100\n",
      "  13 - L= 0.2761288 - Gamma=13.9812992 (M=  14) - s=0.0100\n",
      "  14 - L= 1.4030036 - Gamma=14.9704733 (M=  15) - s=0.0100\n",
      "  15 - L= 2.1534367 - Gamma=15.9554559 (M=  16) - s=0.0100\n",
      "  16 - L= 2.4803700 - Gamma=16.9247765 (M=  17) - s=0.0100\n",
      "  17 - L= 2.6737069 - Gamma=17.8770996 (M=  18) - s=0.0100\n",
      "  18 - L= 2.7513623 - Gamma=18.7784993 (M=  19) - s=0.0100\n",
      "  19 - L= 2.7950108 - Gamma=19.6285315 (M=  20) - s=0.0100\n",
      "  20 - L= 2.8065694 - Gamma=20.3073541 (M=  21) - s=0.0100\n",
      "  21 - L= 2.8117420 - Gamma=20.8430395 (M=  22) - s=0.0100\n",
      "  22 - L= 2.8159162 - Gamma=20.8432563 (M=  22) - s=0.0100\n",
      "  23 - L= 2.8194057 - Gamma=20.8433312 (M=  22) - s=0.0100\n",
      "  24 - L= 2.8223958 - Gamma=20.8434466 (M=  22) - s=0.0100\n",
      "  25 - L= 2.8253664 - Gamma=20.8476230 (M=  22) - s=0.0100\n",
      "  26 - L= 2.8278068 - Gamma=20.8477826 (M=  22) - s=0.0100\n",
      "  27 - L= 2.8299287 - Gamma=21.2702469 (M=  23) - s=0.0100\n",
      "  28 - L= 2.8319978 - Gamma=21.2702721 (M=  23) - s=0.0100\n",
      "  29 - L= 2.8340399 - Gamma=21.2705809 (M=  23) - s=0.0100\n",
      "  30 - L= 2.8353990 - Gamma=21.2711085 (M=  23) - s=0.0100\n",
      "  31 - L= 2.8366953 - Gamma=21.2711691 (M=  23) - s=0.0100\n",
      "  32 - L= 2.8378041 - Gamma=21.2757409 (M=  23) - s=0.0100\n",
      "  33 - L= 2.8385976 - Gamma=21.1992985 (M=  23) - s=0.0100\n",
      "  34 - L= 2.8390160 - Gamma=21.4130214 (M=  24) - s=0.0100\n",
      "  35 - L= 2.8393170 - Gamma=21.4001324 (M=  24) - s=0.0100\n",
      "  36 - L= 2.8396052 - Gamma=21.3991673 (M=  24) - s=0.0100\n",
      "  37 - L= 2.8398868 - Gamma=21.2923868 (M=  24) - s=0.0100\n",
      "  38 - L= 2.8401381 - Gamma=21.2166004 (M=  24) - s=0.0100\n",
      "  39 - L= 2.8403288 - Gamma=21.2175692 (M=  24) - s=0.0100\n",
      "  40 - L= 2.8405083 - Gamma=21.2139198 (M=  24) - s=0.0100\n",
      "  41 - L= 2.8406634 - Gamma=21.3520810 (M=  25) - s=0.0100\n",
      "  42 - L= 2.8408075 - Gamma=21.3515804 (M=  25) - s=0.0100\n",
      "  43 - L= 2.8408740 - Gamma=21.3515735 (M=  25) - s=0.0100\n",
      "  44 - L= 2.8409252 - Gamma=21.3479859 (M=  25) - s=0.0100\n",
      "  45 - L= 2.8409692 - Gamma=21.3480294 (M=  25) - s=0.0100\n",
      "  46 - L= 2.8410087 - Gamma=21.3023540 (M=  25) - s=0.0100\n",
      "  47 - L= 2.8410442 - Gamma=21.3444283 (M=  25) - s=0.0100\n",
      "  48 - L= 2.8410632 - Gamma=21.3500683 (M=  25) - s=0.0100\n",
      "  49 - L= 2.8410678 - Gamma=21.3336855 (M=  25) - s=0.0100\n",
      "  50 - L= 2.8410762 - Gamma=21.3633221 (M=  25) - s=0.0100\n",
      "  51 - L= 2.8410841 - Gamma=21.3485784 (M=  25) - s=0.0100\n",
      "  52 - L= 2.8410903 - Gamma=21.3777106 (M=  26) - s=0.0100\n",
      "  53 - L= 2.8410946 - Gamma=21.3778334 (M=  26) - s=0.0100\n",
      "  54 - L= 2.8410980 - Gamma=21.3635537 (M=  26) - s=0.0100\n",
      "  55 - L= 2.8411023 - Gamma=21.3840996 (M=  26) - s=0.0100\n",
      "  56 - L= 2.8411063 - Gamma=21.3824668 (M=  26) - s=0.0100\n",
      "  57 - L= 2.8411090 - Gamma=21.3823667 (M=  26) - s=0.0100\n",
      "  58 - L= 2.8411102 - Gamma=21.3766101 (M=  26) - s=0.0100\n",
      "  59 - L= 2.8411115 - Gamma=21.3869380 (M=  26) - s=0.0100\n",
      "  60 - L= 2.8411135 - Gamma=21.3755711 (M=  26) - s=0.0100\n",
      "  61 - L= 2.8411146 - Gamma=21.3879429 (M=  26) - s=0.0100\n",
      "  62 - L= 2.8411155 - Gamma=21.3880462 (M=  26) - s=0.0100\n",
      "  63 - L= 2.8411161 - Gamma=21.3876465 (M=  26) - s=0.0100\n",
      "  64 - L= 2.8411166 - Gamma=21.3866933 (M=  26) - s=0.0100\n",
      "  65 - L= 2.8411171 - Gamma=21.3936705 (M=  26) - s=0.0100\n",
      "  66 - L= 2.8411175 - Gamma=21.3937111 (M=  26) - s=0.0100\n",
      "  67 - L= 2.8411179 - Gamma=21.3904040 (M=  26) - s=0.0100\n",
      "  68 - L= 2.8411183 - Gamma=21.3885289 (M=  26) - s=0.0100\n",
      "  69 - L= 2.8411188 - Gamma=21.3965157 (M=  26) - s=0.0100\n",
      "  70 - L= 2.8411191 - Gamma=21.3926998 (M=  26) - s=0.0100\n",
      "  71 - L= 2.8411193 - Gamma=21.3926986 (M=  26) - s=0.0100\n",
      "  72 - L= 2.8411195 - Gamma=21.3966880 (M=  26) - s=0.0100\n",
      "  73 - L= 2.8411198 - Gamma=21.3921791 (M=  26) - s=0.0100\n",
      "  74 - L= 2.8411201 - Gamma=21.3982004 (M=  26) - s=0.0100\n",
      "  75 - L= 2.8411202 - Gamma=21.3976773 (M=  26) - s=0.0100\n",
      "  76 - L= 2.8411204 - Gamma=21.4014640 (M=  26) - s=0.0100\n",
      "  77 - L= 2.8411206 - Gamma=21.4002206 (M=  26) - s=0.0100\n",
      "  78 - L= 2.8411207 - Gamma=21.3981866 (M=  26) - s=0.0100\n",
      "  79 - L= 2.8411208 - Gamma=21.4023589 (M=  26) - s=0.0100\n",
      "  80 - L= 2.8411210 - Gamma=21.4023441 (M=  26) - s=0.0100\n",
      "  81 - L= 2.8411211 - Gamma=21.4023446 (M=  26) - s=0.0100\n",
      "  82 - L= 2.8411212 - Gamma=21.4024324 (M=  26) - s=0.0100\n",
      "  83 - L= 2.8411212 - Gamma=21.4007360 (M=  26) - s=0.0100\n",
      "  84 - L= 2.8411213 - Gamma=21.4032738 (M=  26) - s=0.0100\n",
      "  85 - L= 2.8411214 - Gamma=21.4032724 (M=  26) - s=0.0100\n",
      "  86 - L= 2.8411214 - Gamma=21.4013070 (M=  26) - s=0.0100\n",
      "  87 - L= 2.8411215 - Gamma=21.4010716 (M=  26) - s=0.0100\n",
      "  88 - L= 2.8411215 - Gamma=21.4032908 (M=  26) - s=0.0100\n",
      "  89 - L= 2.8411215 - Gamma=21.4055740 (M=  26) - s=0.0100\n",
      "  90 - L= 2.8411216 - Gamma=21.4048300 (M=  26) - s=0.0100\n",
      "  91 - L= 2.8411217 - Gamma=21.4035725 (M=  26) - s=0.0100\n",
      "  92 - L= 2.8411217 - Gamma=21.4035611 (M=  26) - s=0.0100\n",
      "  93 - L= 2.8411217 - Gamma=21.4055226 (M=  26) - s=0.0100\n",
      "  94 - L= 2.8411218 - Gamma=21.4043679 (M=  26) - s=0.0100\n",
      "  95 - L= 2.8411218 - Gamma=21.4043660 (M=  26) - s=0.0100\n",
      "  96 - L= 2.8411218 - Gamma=21.4041806 (M=  26) - s=0.0100\n",
      "  97 - L= 2.8411218 - Gamma=21.4057022 (M=  26) - s=0.0100\n",
      "  98 - L= 2.8411218 - Gamma=21.4074435 (M=  26) - s=0.0100\n",
      "  99 - L= 2.8411219 - Gamma=21.4069594 (M=  26) - s=0.0100\n",
      " 100 - L= 2.8411219 - Gamma=21.4069664 (M=  26) - s=0.0100\n",
      " 101 - L= 2.8411219 - Gamma=21.4069665 (M=  26) - s=0.0100\n",
      " 102 - L= 2.8411219 - Gamma=21.4065203 (M=  26) - s=0.0100\n",
      " 103 - L= 2.8411219 - Gamma=21.4073593 (M=  26) - s=0.0100\n",
      " 104 - L= 2.8411219 - Gamma=21.4065064 (M=  26) - s=0.0100\n",
      " 105 - L= 2.8411219 - Gamma=21.4076789 (M=  26) - s=0.0100\n",
      " 106 - L= 2.8411219 - Gamma=21.4075619 (M=  26) - s=0.0100\n",
      " 107 - L= 2.8411219 - Gamma=21.4069409 (M=  26) - s=0.0100\n",
      " 108 - L= 2.8411220 - Gamma=21.4078484 (M=  26) - s=0.0100\n",
      " 109 - L= 2.8411220 - Gamma=21.4078484 (M=  26) - s=0.0100\n",
      "Stopping at iteration 109 - max_delta_ml=1.858071147801065e-07\n",
      "L=2.8411219597347888 - Gamma=21.407848404089755 (M=26) - s=0.01\n",
      "Initial alpha = [[ 0.08514892]]\n",
      "   1 - L=-604.1385543 - Gamma= 1.9999516 (M=   2) - s=0.0100\n",
      "   2 - L=-420.1456972 - Gamma= 2.9998819 (M=   3) - s=0.0100\n",
      "   3 - L=-256.5768992 - Gamma= 3.9997976 (M=   4) - s=0.0100\n",
      "   4 - L=-151.0577464 - Gamma= 4.9996873 (M=   5) - s=0.0100\n",
      "   5 - L=-96.6522384 - Gamma= 5.9994616 (M=   6) - s=0.0100\n",
      "   6 - L=-66.7590744 - Gamma= 6.9990687 (M=   7) - s=0.0100\n",
      "   7 - L=-40.4394120 - Gamma= 7.9986379 (M=   8) - s=0.0100\n",
      "   8 - L=-27.1681527 - Gamma= 8.9977860 (M=   9) - s=0.0100\n",
      "   9 - L=-16.9107800 - Gamma= 9.9966311 (M=  10) - s=0.0100\n",
      "  10 - L=-9.7688692 - Gamma=10.9949289 (M=  11) - s=0.0100\n",
      "  11 - L=-4.7406116 - Gamma=11.9923379 (M=  12) - s=0.0100\n",
      "  12 - L=-1.9433990 - Gamma=12.9884401 (M=  13) - s=0.0100\n",
      "  13 - L=-0.1449522 - Gamma=13.9822375 (M=  14) - s=0.0100\n",
      "  14 - L= 1.0530277 - Gamma=14.9733080 (M=  15) - s=0.0100\n",
      "  15 - L= 1.7509550 - Gamma=15.9579367 (M=  16) - s=0.0100\n",
      "  16 - L= 2.1230175 - Gamma=16.9309289 (M=  17) - s=0.0100\n",
      "  17 - L= 2.4436392 - Gamma=17.9009349 (M=  18) - s=0.0100\n",
      "  18 - L= 2.5977995 - Gamma=18.8437009 (M=  19) - s=0.0100\n",
      "  19 - L= 2.6843104 - Gamma=19.7341036 (M=  20) - s=0.0100\n",
      "  20 - L= 2.7300167 - Gamma=20.5901372 (M=  21) - s=0.0100\n",
      "  21 - L= 2.7474709 - Gamma=20.5902640 (M=  21) - s=0.0100\n",
      "  22 - L= 2.7614934 - Gamma=20.5899841 (M=  21) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23 - L= 2.7737052 - Gamma=21.2520300 (M=  22) - s=0.0100\n",
      "  24 - L= 2.7799271 - Gamma=21.2521547 (M=  22) - s=0.0100\n",
      "  25 - L= 2.7849578 - Gamma=21.2521978 (M=  22) - s=0.0100\n",
      "  26 - L= 2.7898966 - Gamma=21.2515369 (M=  22) - s=0.0100\n",
      "  27 - L= 2.7938445 - Gamma=21.2555284 (M=  22) - s=0.0100\n",
      "  28 - L= 2.7977271 - Gamma=21.2566850 (M=  22) - s=0.0100\n",
      "  29 - L= 2.8012463 - Gamma=21.2572383 (M=  22) - s=0.0100\n",
      "  30 - L= 2.8047294 - Gamma=21.2601772 (M=  22) - s=0.0100\n",
      "  31 - L= 2.8077108 - Gamma=21.2746834 (M=  22) - s=0.0100\n",
      "  32 - L= 2.8103473 - Gamma=21.7225994 (M=  23) - s=0.0100\n",
      "  33 - L= 2.8126787 - Gamma=21.7217373 (M=  23) - s=0.0100\n",
      "  34 - L= 2.8149440 - Gamma=21.7223787 (M=  23) - s=0.0100\n",
      "  35 - L= 2.8170902 - Gamma=21.6253353 (M=  23) - s=0.0100\n",
      "  36 - L= 2.8181115 - Gamma=21.6042006 (M=  23) - s=0.0100\n",
      "  37 - L= 2.8187065 - Gamma=21.6042265 (M=  23) - s=0.0100\n",
      "  38 - L= 2.8192255 - Gamma=21.6030898 (M=  23) - s=0.0100\n",
      "  39 - L= 2.8196199 - Gamma=21.8186721 (M=  24) - s=0.0100\n",
      "  40 - L= 2.8200069 - Gamma=21.8216340 (M=  24) - s=0.0100\n",
      "  41 - L= 2.8203905 - Gamma=21.8217443 (M=  24) - s=0.0100\n",
      "  42 - L= 2.8207131 - Gamma=21.8361707 (M=  24) - s=0.0100\n",
      "  43 - L= 2.8207333 - Gamma=21.8351316 (M=  24) - s=0.0100\n",
      "  44 - L= 2.8207472 - Gamma=21.8080979 (M=  24) - s=0.0100\n",
      "  45 - L= 2.8207684 - Gamma=21.8620711 (M=  25) - s=0.0100\n",
      "  46 - L= 2.8207782 - Gamma=21.8305003 (M=  25) - s=0.0100\n",
      "  47 - L= 2.8207917 - Gamma=21.8437683 (M=  25) - s=0.0100\n",
      "  48 - L= 2.8207970 - Gamma=21.8699705 (M=  25) - s=0.0100\n",
      "  49 - L= 2.8208051 - Gamma=21.8485471 (M=  25) - s=0.0100\n",
      "  50 - L= 2.8208111 - Gamma=21.8230947 (M=  25) - s=0.0100\n",
      "  51 - L= 2.8208204 - Gamma=21.8565739 (M=  25) - s=0.0100\n",
      "  52 - L= 2.8208251 - Gamma=21.8335313 (M=  25) - s=0.0100\n",
      "  53 - L= 2.8208317 - Gamma=21.8401307 (M=  25) - s=0.0100\n",
      "  54 - L= 2.8208359 - Gamma=21.8401214 (M=  25) - s=0.0100\n",
      "  55 - L= 2.8208386 - Gamma=21.8369876 (M=  25) - s=0.0100\n",
      "  56 - L= 2.8208405 - Gamma=21.8380511 (M=  25) - s=0.0100\n",
      "  57 - L= 2.8208428 - Gamma=21.8542525 (M=  25) - s=0.0100\n",
      "  58 - L= 2.8208460 - Gamma=21.8403836 (M=  25) - s=0.0100\n",
      "  59 - L= 2.8208483 - Gamma=21.8414717 (M=  25) - s=0.0100\n",
      "  60 - L= 2.8208504 - Gamma=21.8253294 (M=  25) - s=0.0100\n",
      "  61 - L= 2.8208538 - Gamma=21.8445268 (M=  25) - s=0.0100\n",
      "  62 - L= 2.8208562 - Gamma=21.8444505 (M=  25) - s=0.0100\n",
      "  63 - L= 2.8208579 - Gamma=21.8298032 (M=  25) - s=0.0100\n",
      "  64 - L= 2.8208609 - Gamma=21.8358752 (M=  25) - s=0.0100\n",
      "  65 - L= 2.8208621 - Gamma=21.8356392 (M=  25) - s=0.0100\n",
      "  66 - L= 2.8208633 - Gamma=21.8469109 (M=  25) - s=0.0100\n",
      "  67 - L= 2.8208651 - Gamma=21.8319432 (M=  25) - s=0.0100\n",
      "  68 - L= 2.8208663 - Gamma=21.8232885 (M=  25) - s=0.0100\n",
      "  69 - L= 2.8208684 - Gamma=21.8379763 (M=  25) - s=0.0100\n",
      "  70 - L= 2.8208695 - Gamma=21.8380154 (M=  25) - s=0.0100\n",
      "  71 - L= 2.8208704 - Gamma=21.8273293 (M=  25) - s=0.0100\n",
      "  72 - L= 2.8208712 - Gamma=21.8273414 (M=  25) - s=0.0100\n",
      "  73 - L= 2.8208720 - Gamma=21.8273284 (M=  25) - s=0.0100\n",
      "  74 - L= 2.8208728 - Gamma=21.8271217 (M=  25) - s=0.0100\n",
      "  75 - L= 2.8208733 - Gamma=21.8296319 (M=  25) - s=0.0100\n",
      "  76 - L= 2.8208738 - Gamma=21.8365819 (M=  25) - s=0.0100\n",
      "  77 - L= 2.8208745 - Gamma=21.8296653 (M=  25) - s=0.0100\n",
      "  78 - L= 2.8208750 - Gamma=21.8295674 (M=  25) - s=0.0100\n",
      "  79 - L= 2.8208754 - Gamma=21.8225277 (M=  25) - s=0.0100\n",
      "  80 - L= 2.8208761 - Gamma=21.8310524 (M=  25) - s=0.0100\n",
      "  81 - L= 2.8208767 - Gamma=21.8295845 (M=  25) - s=0.0100\n",
      "  82 - L= 2.8208771 - Gamma=21.8222298 (M=  25) - s=0.0100\n",
      "  83 - L= 2.8208774 - Gamma=21.8275772 (M=  25) - s=0.0100\n",
      "  84 - L= 2.8208776 - Gamma=21.8290416 (M=  25) - s=0.0100\n",
      "  85 - L= 2.8208778 - Gamma=21.8230237 (M=  25) - s=0.0100\n",
      "  86 - L= 2.8208780 - Gamma=21.8230025 (M=  25) - s=0.0100\n",
      "  87 - L= 2.8208782 - Gamma=21.8230028 (M=  25) - s=0.0100\n",
      "  88 - L= 2.8208783 - Gamma=21.8268541 (M=  25) - s=0.0100\n",
      "  89 - L= 2.8208786 - Gamma=21.8226079 (M=  25) - s=0.0100\n",
      "  90 - L= 2.8208787 - Gamma=21.8235304 (M=  25) - s=0.0100\n",
      "  91 - L= 2.8208789 - Gamma=21.8238008 (M=  25) - s=0.0100\n",
      "  92 - L= 2.8208790 - Gamma=21.8196948 (M=  25) - s=0.0100\n",
      "  93 - L= 2.8208792 - Gamma=21.8239067 (M=  25) - s=0.0100\n",
      "  94 - L= 2.8208793 - Gamma=21.8239043 (M=  25) - s=0.0100\n",
      "  95 - L= 2.8208794 - Gamma=21.8204140 (M=  25) - s=0.0100\n",
      "  96 - L= 2.8208794 - Gamma=21.8204125 (M=  25) - s=0.0100\n",
      "  97 - L= 2.8208795 - Gamma=21.8183184 (M=  25) - s=0.0100\n",
      "  98 - L= 2.8208796 - Gamma=21.8215557 (M=  25) - s=0.0100\n",
      "  99 - L= 2.8208797 - Gamma=21.8215531 (M=  25) - s=0.0100\n",
      " 100 - L= 2.8208797 - Gamma=21.8223387 (M=  25) - s=0.0100\n",
      " 101 - L= 2.8208798 - Gamma=21.8189902 (M=  25) - s=0.0100\n",
      " 102 - L= 2.8208799 - Gamma=21.8185607 (M=  25) - s=0.0100\n",
      " 103 - L= 2.8208799 - Gamma=21.8208063 (M=  25) - s=0.0100\n",
      " 104 - L= 2.8208799 - Gamma=21.8192502 (M=  25) - s=0.0100\n",
      " 105 - L= 2.8208800 - Gamma=21.8192056 (M=  25) - s=0.0100\n",
      " 106 - L= 2.8208800 - Gamma=21.8191807 (M=  25) - s=0.0100\n",
      " 107 - L= 2.8208800 - Gamma=21.8173143 (M=  25) - s=0.0100\n",
      " 108 - L= 2.8208801 - Gamma=21.8193708 (M=  25) - s=0.0100\n",
      " 109 - L= 2.8208801 - Gamma=21.8199018 (M=  25) - s=0.0100\n",
      " 110 - L= 2.8208801 - Gamma=21.8175478 (M=  25) - s=0.0100\n",
      " 111 - L= 2.8208802 - Gamma=21.8179471 (M=  25) - s=0.0100\n",
      " 112 - L= 2.8208802 - Gamma=21.8179469 (M=  25) - s=0.0100\n",
      " 113 - L= 2.8208802 - Gamma=21.8179452 (M=  25) - s=0.0100\n",
      " 114 - L= 2.8208802 - Gamma=21.8179519 (M=  25) - s=0.0100\n",
      " 115 - L= 2.8208802 - Gamma=21.8180451 (M=  25) - s=0.0100\n",
      " 116 - L= 2.8208803 - Gamma=21.8193176 (M=  25) - s=0.0100\n",
      " 117 - L= 2.8208803 - Gamma=21.8181425 (M=  25) - s=0.0100\n",
      " 118 - L= 2.8208803 - Gamma=21.8181166 (M=  25) - s=0.0100\n",
      " 119 - L= 2.8208803 - Gamma=21.8182014 (M=  25) - s=0.0100\n",
      " 120 - L= 2.8208803 - Gamma=21.8181954 (M=  25) - s=0.0100\n",
      " 121 - L= 2.8208803 - Gamma=21.8168109 (M=  25) - s=0.0100\n",
      " 122 - L= 2.8208804 - Gamma=21.8183130 (M=  25) - s=0.0100\n",
      " 123 - L= 2.8208804 - Gamma=21.8183130 (M=  25) - s=0.0100\n",
      " 124 - L= 2.8208804 - Gamma=21.8170296 (M=  25) - s=0.0100\n",
      " 125 - L= 2.8208804 - Gamma=21.8173584 (M=  25) - s=0.0100\n",
      " 126 - L= 2.8208804 - Gamma=21.8173613 (M=  25) - s=0.0100\n",
      " 127 - L= 2.8208804 - Gamma=21.8172038 (M=  25) - s=0.0100\n",
      " 128 - L= 2.8208804 - Gamma=21.8180385 (M=  25) - s=0.0100\n",
      " 129 - L= 2.8208804 - Gamma=21.8168511 (M=  25) - s=0.0100\n",
      " 130 - L= 2.8208804 - Gamma=21.8161758 (M=  25) - s=0.0100\n",
      " 131 - L= 2.8208804 - Gamma=21.8172230 (M=  25) - s=0.0100\n",
      " 132 - L= 2.8208804 - Gamma=21.8164180 (M=  25) - s=0.0100\n",
      " 133 - L= 2.8208804 - Gamma=21.8164180 (M=  25) - s=0.0100\n",
      "Stopping at iteration 133 - max_delta_ml=1.263740327413351e-07\n",
      "L=2.820880441190187 - Gamma=21.816417956354 (M=25) - s=0.01\n",
      "Initial alpha = [[ 0.08488695]]\n",
      "   1 - L=-492.0730880 - Gamma= 1.9999640 (M=   2) - s=0.0100\n",
      "   2 - L=-356.5931238 - Gamma= 2.9998816 (M=   3) - s=0.0100\n",
      "   3 - L=-239.7742323 - Gamma= 3.9997875 (M=   4) - s=0.0100\n",
      "   4 - L=-148.3961790 - Gamma= 4.9996663 (M=   5) - s=0.0100\n",
      "   5 - L=-93.3845292 - Gamma= 5.9994658 (M=   6) - s=0.0100\n",
      "   6 - L=-59.0399598 - Gamma= 6.9991297 (M=   7) - s=0.0100\n",
      "   7 - L=-31.2204377 - Gamma= 7.9986406 (M=   8) - s=0.0100\n",
      "   8 - L=-13.9482502 - Gamma= 8.9979005 (M=   9) - s=0.0100\n",
      "   9 - L=-8.1448400 - Gamma= 9.9958649 (M=  10) - s=0.0100\n",
      "  10 - L=-3.1679659 - Gamma=10.9934498 (M=  11) - s=0.0100\n",
      "  11 - L=-1.0204514 - Gamma=11.9883262 (M=  12) - s=0.0100\n",
      "  12 - L= 0.7706139 - Gamma=12.9820894 (M=  13) - s=0.0100\n",
      "  13 - L= 1.8972032 - Gamma=13.9717984 (M=  14) - s=0.0100\n",
      "  14 - L= 2.4570439 - Gamma=14.9534909 (M=  15) - s=0.0100\n",
      "  15 - L= 2.6887969 - Gamma=15.9138644 (M=  16) - s=0.0100\n",
      "  16 - L= 2.8413745 - Gamma=16.8573715 (M=  17) - s=0.0100\n",
      "  17 - L= 2.9173165 - Gamma=17.7530349 (M=  18) - s=0.0100\n",
      "  18 - L= 2.9492536 - Gamma=18.5663837 (M=  19) - s=0.0100\n",
      "  19 - L= 2.9544257 - Gamma=19.1269288 (M=  20) - s=0.0100\n",
      "  20 - L= 2.9569536 - Gamma=19.1273558 (M=  20) - s=0.0100\n",
      "  21 - L= 2.9585803 - Gamma=19.1277413 (M=  20) - s=0.0100\n",
      "  22 - L= 2.9600528 - Gamma=19.1279682 (M=  20) - s=0.0100\n",
      "  23 - L= 2.9611099 - Gamma=19.1302779 (M=  20) - s=0.0100\n",
      "  24 - L= 2.9620792 - Gamma=19.4482296 (M=  21) - s=0.0100\n",
      "  25 - L= 2.9629213 - Gamma=19.4503901 (M=  21) - s=0.0100\n",
      "  26 - L= 2.9636259 - Gamma=19.4504291 (M=  21) - s=0.0100\n",
      "  27 - L= 2.9641593 - Gamma=19.4504495 (M=  21) - s=0.0100\n",
      "  28 - L= 2.9646751 - Gamma=19.4503340 (M=  21) - s=0.0100\n",
      "  29 - L= 2.9651446 - Gamma=19.4503867 (M=  21) - s=0.0100\n",
      "  30 - L= 2.9655988 - Gamma=19.4606236 (M=  21) - s=0.0100\n",
      "  31 - L= 2.9659579 - Gamma=19.4612397 (M=  21) - s=0.0100\n",
      "  32 - L= 2.9662464 - Gamma=19.4612629 (M=  21) - s=0.0100\n",
      "  33 - L= 2.9665255 - Gamma=19.4349932 (M=  21) - s=0.0100\n",
      "  34 - L= 2.9668691 - Gamma=19.6320290 (M=  22) - s=0.0100\n",
      "  35 - L= 2.9669863 - Gamma=19.5631598 (M=  22) - s=0.0100\n",
      "  36 - L= 2.9670567 - Gamma=19.5608552 (M=  22) - s=0.0100\n",
      "  37 - L= 2.9671211 - Gamma=19.6270046 (M=  22) - s=0.0100\n",
      "  38 - L= 2.9671716 - Gamma=19.6272310 (M=  22) - s=0.0100\n",
      "  39 - L= 2.9672050 - Gamma=19.6168409 (M=  22) - s=0.0100\n",
      "  40 - L= 2.9672456 - Gamma=19.6755786 (M=  22) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41 - L= 2.9672817 - Gamma=19.7475791 (M=  23) - s=0.0100\n",
      "  42 - L= 2.9673131 - Gamma=19.7449755 (M=  23) - s=0.0100\n",
      "  43 - L= 2.9673216 - Gamma=19.7249779 (M=  23) - s=0.0100\n",
      "  44 - L= 2.9673322 - Gamma=19.7531629 (M=  23) - s=0.0100\n",
      "  45 - L= 2.9673428 - Gamma=19.7469271 (M=  23) - s=0.0100\n",
      "  46 - L= 2.9673513 - Gamma=19.7442868 (M=  23) - s=0.0100\n",
      "  47 - L= 2.9673575 - Gamma=19.7442875 (M=  23) - s=0.0100\n",
      "  48 - L= 2.9673629 - Gamma=19.7441277 (M=  23) - s=0.0100\n",
      "  49 - L= 2.9673678 - Gamma=19.7444207 (M=  23) - s=0.0100\n",
      "  50 - L= 2.9673716 - Gamma=19.7665822 (M=  23) - s=0.0100\n",
      "  51 - L= 2.9673736 - Gamma=19.7831675 (M=  24) - s=0.0100\n",
      "  52 - L= 2.9673760 - Gamma=19.7722261 (M=  24) - s=0.0100\n",
      "  53 - L= 2.9673782 - Gamma=19.7687043 (M=  24) - s=0.0100\n",
      "  54 - L= 2.9673801 - Gamma=19.7803249 (M=  24) - s=0.0100\n",
      "  55 - L= 2.9673814 - Gamma=19.7929241 (M=  24) - s=0.0100\n",
      "  56 - L= 2.9673824 - Gamma=19.7929324 (M=  24) - s=0.0100\n",
      "  57 - L= 2.9673834 - Gamma=19.7924550 (M=  24) - s=0.0100\n",
      "  58 - L= 2.9673843 - Gamma=19.7905846 (M=  24) - s=0.0100\n",
      "  59 - L= 2.9673852 - Gamma=19.8020173 (M=  25) - s=0.0100\n",
      "  60 - L= 2.9673859 - Gamma=19.7999557 (M=  25) - s=0.0100\n",
      "  61 - L= 2.9673866 - Gamma=19.7938304 (M=  25) - s=0.0100\n",
      "  62 - L= 2.9673878 - Gamma=19.8061756 (M=  25) - s=0.0100\n",
      "  63 - L= 2.9673883 - Gamma=19.8059679 (M=  25) - s=0.0100\n",
      "  64 - L= 2.9673889 - Gamma=19.8141025 (M=  25) - s=0.0100\n",
      "  65 - L= 2.9673895 - Gamma=19.8133784 (M=  25) - s=0.0100\n",
      "  66 - L= 2.9673899 - Gamma=19.8090589 (M=  25) - s=0.0100\n",
      "  67 - L= 2.9673903 - Gamma=19.8074011 (M=  25) - s=0.0100\n",
      "  68 - L= 2.9673910 - Gamma=19.8170232 (M=  25) - s=0.0100\n",
      "  69 - L= 2.9673914 - Gamma=19.8232041 (M=  25) - s=0.0100\n",
      "  70 - L= 2.9673918 - Gamma=19.8179647 (M=  25) - s=0.0100\n",
      "  71 - L= 2.9673921 - Gamma=19.8179822 (M=  25) - s=0.0100\n",
      "  72 - L= 2.9673924 - Gamma=19.8180167 (M=  25) - s=0.0100\n",
      "  73 - L= 2.9673927 - Gamma=19.8144677 (M=  25) - s=0.0100\n",
      "  74 - L= 2.9673930 - Gamma=19.8131620 (M=  25) - s=0.0100\n",
      "  75 - L= 2.9673934 - Gamma=19.8203656 (M=  25) - s=0.0100\n",
      "  76 - L= 2.9673938 - Gamma=19.8276196 (M=  25) - s=0.0100\n",
      "  77 - L= 2.9673941 - Gamma=19.8276220 (M=  25) - s=0.0100\n",
      "  78 - L= 2.9673943 - Gamma=19.8276253 (M=  25) - s=0.0100\n",
      "  79 - L= 2.9673945 - Gamma=19.8266840 (M=  25) - s=0.0100\n",
      "  80 - L= 2.9673947 - Gamma=19.8266815 (M=  25) - s=0.0100\n",
      "  81 - L= 2.9673949 - Gamma=19.8236315 (M=  25) - s=0.0100\n",
      "  82 - L= 2.9673951 - Gamma=19.8277205 (M=  25) - s=0.0100\n",
      "  83 - L= 2.9673954 - Gamma=19.8263544 (M=  25) - s=0.0100\n",
      "  84 - L= 2.9673956 - Gamma=19.8263012 (M=  25) - s=0.0100\n",
      "  85 - L= 2.9673958 - Gamma=19.8261109 (M=  25) - s=0.0100\n",
      "  86 - L= 2.9673959 - Gamma=19.8297946 (M=  25) - s=0.0100\n",
      "  87 - L= 2.9673961 - Gamma=19.8345315 (M=  25) - s=0.0100\n",
      "  88 - L= 2.9673963 - Gamma=19.8312131 (M=  25) - s=0.0100\n",
      "  89 - L= 2.9673964 - Gamma=19.8311157 (M=  25) - s=0.0100\n",
      "  90 - L= 2.9673965 - Gamma=19.8303436 (M=  25) - s=0.0100\n",
      "  91 - L= 2.9673966 - Gamma=19.8343271 (M=  25) - s=0.0100\n",
      "  92 - L= 2.9673967 - Gamma=19.8378664 (M=  25) - s=0.0100\n",
      "  93 - L= 2.9673969 - Gamma=19.8343430 (M=  25) - s=0.0100\n",
      "  94 - L= 2.9673970 - Gamma=19.8366749 (M=  25) - s=0.0100\n",
      "  95 - L= 2.9673970 - Gamma=19.8360457 (M=  25) - s=0.0100\n",
      "  96 - L= 2.9673971 - Gamma=19.8340277 (M=  25) - s=0.0100\n",
      "  97 - L= 2.9673973 - Gamma=19.8384019 (M=  25) - s=0.0100\n",
      "  98 - L= 2.9673973 - Gamma=19.8381762 (M=  25) - s=0.0100\n",
      "  99 - L= 2.9673974 - Gamma=19.8405816 (M=  25) - s=0.0100\n",
      " 100 - L= 2.9673974 - Gamma=19.8405950 (M=  25) - s=0.0100\n",
      " 101 - L= 2.9673975 - Gamma=19.8390053 (M=  25) - s=0.0100\n",
      " 102 - L= 2.9673975 - Gamma=19.8383907 (M=  25) - s=0.0100\n",
      " 103 - L= 2.9673976 - Gamma=19.8414397 (M=  25) - s=0.0100\n",
      " 104 - L= 2.9673976 - Gamma=19.8397129 (M=  25) - s=0.0100\n",
      " 105 - L= 2.9673977 - Gamma=19.8423063 (M=  25) - s=0.0100\n",
      " 106 - L= 2.9673977 - Gamma=19.8422918 (M=  25) - s=0.0100\n",
      " 107 - L= 2.9673978 - Gamma=19.8419168 (M=  25) - s=0.0100\n",
      " 108 - L= 2.9673978 - Gamma=19.8419218 (M=  25) - s=0.0100\n",
      " 109 - L= 2.9673978 - Gamma=19.8415131 (M=  25) - s=0.0100\n",
      " 110 - L= 2.9673979 - Gamma=19.8437873 (M=  25) - s=0.0100\n",
      " 111 - L= 2.9673979 - Gamma=19.8423232 (M=  25) - s=0.0100\n",
      " 112 - L= 2.9673979 - Gamma=19.8410908 (M=  25) - s=0.0100\n",
      " 113 - L= 2.9673980 - Gamma=19.8428760 (M=  25) - s=0.0100\n",
      " 114 - L= 2.9673980 - Gamma=19.8450741 (M=  25) - s=0.0100\n",
      " 115 - L= 2.9673980 - Gamma=19.8446196 (M=  25) - s=0.0100\n",
      " 116 - L= 2.9673981 - Gamma=19.8445691 (M=  25) - s=0.0100\n",
      " 117 - L= 2.9673981 - Gamma=19.8445692 (M=  25) - s=0.0100\n",
      " 118 - L= 2.9673981 - Gamma=19.8445690 (M=  25) - s=0.0100\n",
      " 119 - L= 2.9673981 - Gamma=19.8460563 (M=  25) - s=0.0100\n",
      " 120 - L= 2.9673982 - Gamma=19.8447016 (M=  25) - s=0.0100\n",
      " 121 - L= 2.9673982 - Gamma=19.8438421 (M=  25) - s=0.0100\n",
      " 122 - L= 2.9673982 - Gamma=19.8453880 (M=  25) - s=0.0100\n",
      " 123 - L= 2.9673982 - Gamma=19.8468243 (M=  25) - s=0.0100\n",
      " 124 - L= 2.9673982 - Gamma=19.8464130 (M=  25) - s=0.0100\n",
      " 125 - L= 2.9673982 - Gamma=19.8463217 (M=  25) - s=0.0100\n",
      " 126 - L= 2.9673983 - Gamma=19.8463270 (M=  25) - s=0.0100\n",
      " 127 - L= 2.9673983 - Gamma=19.8456573 (M=  25) - s=0.0100\n",
      " 128 - L= 2.9673983 - Gamma=19.8468859 (M=  25) - s=0.0100\n",
      " 129 - L= 2.9673983 - Gamma=19.8481493 (M=  25) - s=0.0100\n",
      " 130 - L= 2.9673983 - Gamma=19.8470453 (M=  25) - s=0.0100\n",
      " 131 - L= 2.9673983 - Gamma=19.8481392 (M=  25) - s=0.0100\n",
      " 132 - L= 2.9673983 - Gamma=19.8478816 (M=  25) - s=0.0100\n",
      " 133 - L= 2.9673983 - Gamma=19.8487765 (M=  25) - s=0.0100\n",
      " 134 - L= 2.9673983 - Gamma=19.8480222 (M=  25) - s=0.0100\n",
      " 135 - L= 2.9673983 - Gamma=19.8474385 (M=  25) - s=0.0100\n",
      " 136 - L= 2.9673984 - Gamma=19.8485225 (M=  25) - s=0.0100\n",
      " 137 - L= 2.9673984 - Gamma=19.8478480 (M=  25) - s=0.0100\n",
      " 138 - L= 2.9673984 - Gamma=19.8478479 (M=  25) - s=0.0100\n",
      " 139 - L= 2.9673984 - Gamma=19.8478479 (M=  25) - s=0.0100\n",
      "Stopping at iteration 139 - max_delta_ml=1.9828695451974384e-07\n",
      "L=2.9673983709001863 - Gamma=19.84784786933794 (M=25) - s=0.01\n",
      "Initial alpha = [[ 0.08277253]]\n",
      "   1 - L=-498.2055329 - Gamma= 1.9999604 (M=   2) - s=0.0100\n",
      "   2 - L=-340.5573094 - Gamma= 2.9998910 (M=   3) - s=0.0100\n",
      "   3 - L=-212.4302345 - Gamma= 3.9998052 (M=   4) - s=0.0100\n",
      "   4 - L=-138.7418008 - Gamma= 4.9996571 (M=   5) - s=0.0100\n",
      "   5 - L=-96.9345349 - Gamma= 5.9993763 (M=   6) - s=0.0100\n",
      "   6 - L=-72.6464426 - Gamma= 6.9989127 (M=   7) - s=0.0100\n",
      "   7 - L=-50.6792217 - Gamma= 7.9984038 (M=   8) - s=0.0100\n",
      "   8 - L=-33.8979037 - Gamma= 8.9977463 (M=   9) - s=0.0100\n",
      "   9 - L=-19.0614003 - Gamma= 9.9969700 (M=  10) - s=0.0100\n",
      "  10 - L=-9.7499495 - Gamma=10.9956919 (M=  11) - s=0.0100\n",
      "  11 - L=-3.8850050 - Gamma=11.9935150 (M=  12) - s=0.0100\n",
      "  12 - L=-0.2114513 - Gamma=12.9900854 (M=  13) - s=0.0100\n",
      "  13 - L= 1.1844960 - Gamma=13.9816438 (M=  14) - s=0.0100\n",
      "  14 - L= 1.9215929 - Gamma=14.9680337 (M=  15) - s=0.0100\n",
      "  15 - L= 2.3640306 - Gamma=15.9458946 (M=  16) - s=0.0100\n",
      "  16 - L= 2.6739110 - Gamma=16.9138197 (M=  17) - s=0.0100\n",
      "  17 - L= 2.7992587 - Gamma=17.8453240 (M=  18) - s=0.0100\n",
      "  18 - L= 2.8620532 - Gamma=18.7317433 (M=  19) - s=0.0100\n",
      "  19 - L= 2.8810314 - Gamma=19.4887086 (M=  20) - s=0.0100\n",
      "  20 - L= 2.8867590 - Gamma=19.4903008 (M=  20) - s=0.0100\n",
      "  21 - L= 2.8918388 - Gamma=20.0398054 (M=  21) - s=0.0100\n",
      "  22 - L= 2.8961008 - Gamma=20.0408907 (M=  21) - s=0.0100\n",
      "  23 - L= 2.9002878 - Gamma=20.0414604 (M=  21) - s=0.0100\n",
      "  24 - L= 2.9042729 - Gamma=20.0440832 (M=  21) - s=0.0100\n",
      "  25 - L= 2.9075219 - Gamma=20.0441872 (M=  21) - s=0.0100\n",
      "  26 - L= 2.9094378 - Gamma=20.4522499 (M=  22) - s=0.0100\n",
      "  27 - L= 2.9111243 - Gamma=20.4524889 (M=  22) - s=0.0100\n",
      "  28 - L= 2.9124794 - Gamma=20.4624524 (M=  22) - s=0.0100\n",
      "  29 - L= 2.9137855 - Gamma=20.4626369 (M=  22) - s=0.0100\n",
      "  30 - L= 2.9144605 - Gamma=20.4626003 (M=  22) - s=0.0100\n",
      "  31 - L= 2.9149610 - Gamma=20.4625516 (M=  22) - s=0.0100\n",
      "  32 - L= 2.9154458 - Gamma=20.4625409 (M=  22) - s=0.0100\n",
      "  33 - L= 2.9156396 - Gamma=20.4650693 (M=  22) - s=0.0100\n",
      "  34 - L= 2.9157554 - Gamma=20.4802486 (M=  22) - s=0.0100\n",
      "  35 - L= 2.9158501 - Gamma=20.5957674 (M=  23) - s=0.0100\n",
      "  36 - L= 2.9159687 - Gamma=20.5561490 (M=  23) - s=0.0100\n",
      "  37 - L= 2.9160448 - Gamma=20.4835183 (M=  23) - s=0.0100\n",
      "  38 - L= 2.9161025 - Gamma=20.4834135 (M=  23) - s=0.0100\n",
      "  39 - L= 2.9161458 - Gamma=20.4803297 (M=  23) - s=0.0100\n",
      "  40 - L= 2.9161903 - Gamma=20.5515922 (M=  23) - s=0.0100\n",
      "  41 - L= 2.9162220 - Gamma=20.5523039 (M=  23) - s=0.0100\n",
      "  42 - L= 2.9162389 - Gamma=20.5359870 (M=  23) - s=0.0100\n",
      "  43 - L= 2.9162461 - Gamma=20.5122679 (M=  23) - s=0.0100\n",
      "  44 - L= 2.9162561 - Gamma=20.5507269 (M=  24) - s=0.0100\n",
      "  45 - L= 2.9162665 - Gamma=20.5319083 (M=  24) - s=0.0100\n",
      "  46 - L= 2.9162758 - Gamma=20.5346481 (M=  24) - s=0.0100\n",
      "  47 - L= 2.9162837 - Gamma=20.5686787 (M=  25) - s=0.0100\n",
      "  48 - L= 2.9162935 - Gamma=20.5399173 (M=  25) - s=0.0100\n",
      "  49 - L= 2.9163001 - Gamma=20.5698441 (M=  25) - s=0.0100\n",
      "  50 - L= 2.9163048 - Gamma=20.5697715 (M=  25) - s=0.0100\n",
      "  51 - L= 2.9163087 - Gamma=20.5491358 (M=  25) - s=0.0100\n",
      "  52 - L= 2.9163143 - Gamma=20.5770668 (M=  25) - s=0.0100\n",
      "  53 - L= 2.9163211 - Gamma=20.5613490 (M=  25) - s=0.0100\n",
      "  54 - L= 2.9163264 - Gamma=20.5878094 (M=  26) - s=0.0100\n",
      "  55 - L= 2.9163361 - Gamma=20.5583013 (M=  26) - s=0.0100\n",
      "  56 - L= 2.9163425 - Gamma=20.5478611 (M=  26) - s=0.0100\n",
      "  57 - L= 2.9163533 - Gamma=20.5840492 (M=  26) - s=0.0100\n",
      "  58 - L= 2.9163615 - Gamma=20.5559449 (M=  26) - s=0.0100\n",
      "  59 - L= 2.9163746 - Gamma=20.5968656 (M=  26) - s=0.0100\n",
      "  60 - L= 2.9163857 - Gamma=20.5997100 (M=  26) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  61 - L= 2.9163962 - Gamma=20.5648669 (M=  26) - s=0.0100\n",
      "  62 - L= 2.9164060 - Gamma=20.5980819 (M=  26) - s=0.0100\n",
      "  63 - L= 2.9164178 - Gamma=20.5631651 (M=  26) - s=0.0100\n",
      "  64 - L= 2.9164341 - Gamma=20.6067750 (M=  26) - s=0.0100\n",
      "  65 - L= 2.9164525 - Gamma=20.5795317 (M=  26) - s=0.0100\n",
      "  66 - L= 2.9164666 - Gamma=20.5776582 (M=  26) - s=0.0100\n",
      "  67 - L= 2.9164795 - Gamma=20.6178272 (M=  26) - s=0.0100\n",
      "  68 - L= 2.9165006 - Gamma=20.5658719 (M=  26) - s=0.0100\n",
      "  69 - L= 2.9165141 - Gamma=20.6037684 (M=  26) - s=0.0100\n",
      "  70 - L= 2.9165319 - Gamma=20.5588858 (M=  26) - s=0.0100\n",
      "  71 - L= 2.9165607 - Gamma=20.6133155 (M=  26) - s=0.0100\n",
      "  72 - L= 2.9165938 - Gamma=20.5881329 (M=  26) - s=0.0100\n",
      "  73 - L= 2.9166260 - Gamma=20.5241051 (M=  26) - s=0.0100\n",
      "  74 - L= 2.9166680 - Gamma=20.6003747 (M=  27) - s=0.0100\n",
      "  75 - L= 2.9166908 - Gamma=20.5545198 (M=  26) - s=0.0100\n",
      "  76 - L= 2.9167402 - Gamma=20.6275521 (M=  26) - s=0.0100\n",
      "  77 - L= 2.9168253 - Gamma=20.5627099 (M=  26) - s=0.0100\n",
      "  78 - L= 2.9168797 - Gamma=20.6424272 (M=  26) - s=0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bramiozo/DEV/GIT/RexR/rvm.py:289: RuntimeWarning: invalid value encountered in log\n",
      "  change = np.abs(np.log(newAlpha) - np.log(self.Alpha[j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  79 - L= 2.9169232 - Gamma=20.5727142 (M=  25) - s=0.0100\n",
      "  80 - L= 2.9169908 - Gamma=20.6496850 (M=  25) - s=0.0100\n",
      "  81 - L= 2.9170591 - Gamma=20.6561402 (M=  25) - s=0.0100\n",
      "  82 - L= 2.9171144 - Gamma=20.5976352 (M=  25) - s=0.0100\n",
      "  83 - L= 2.9171525 - Gamma=20.6586075 (M=  25) - s=0.0100\n",
      "  84 - L= 2.9171840 - Gamma=20.6556158 (M=  25) - s=0.0100\n",
      "  85 - L= 2.9172109 - Gamma=20.7001785 (M=  25) - s=0.0100\n",
      "  86 - L= 2.9172456 - Gamma=20.6720270 (M=  25) - s=0.0100\n",
      "  87 - L= 2.9172601 - Gamma=20.6667774 (M=  25) - s=0.0100\n",
      "  88 - L= 2.9172756 - Gamma=20.6336814 (M=  25) - s=0.0100\n",
      "  89 - L= 2.9172986 - Gamma=20.6776865 (M=  25) - s=0.0100\n",
      "  90 - L= 2.9173082 - Gamma=20.7029639 (M=  25) - s=0.0100\n",
      "  91 - L= 2.9173170 - Gamma=20.6769363 (M=  25) - s=0.0100\n",
      "  92 - L= 2.9173250 - Gamma=20.7086413 (M=  26) - s=0.0100\n",
      "  93 - L= 2.9173335 - Gamma=20.6781516 (M=  26) - s=0.0100\n",
      "  94 - L= 2.9173437 - Gamma=20.7061380 (M=  26) - s=0.0100\n",
      "  95 - L= 2.9173494 - Gamma=20.6942754 (M=  26) - s=0.0100\n",
      "  96 - L= 2.9173560 - Gamma=20.7220980 (M=  26) - s=0.0100\n",
      "  97 - L= 2.9173678 - Gamma=20.6907377 (M=  26) - s=0.0100\n",
      "  98 - L= 2.9173825 - Gamma=20.7270111 (M=  26) - s=0.0100\n",
      "  99 - L= 2.9173989 - Gamma=20.7685206 (M=  26) - s=0.0100\n",
      " 100 - L= 2.9174164 - Gamma=20.7466277 (M=  26) - s=0.0100\n",
      " 101 - L= 2.9174369 - Gamma=20.7400100 (M=  26) - s=0.0100\n",
      " 102 - L= 2.9174558 - Gamma=20.7785814 (M=  26) - s=0.0100\n",
      " 103 - L= 2.9174781 - Gamma=20.7333013 (M=  26) - s=0.0100\n",
      " 104 - L= 2.9175341 - Gamma=20.8039170 (M=  26) - s=0.0100\n",
      " 105 - L= 2.9175736 - Gamma=20.7512989 (M=  26) - s=0.0100\n",
      " 106 - L= 2.9176333 - Gamma=20.8135761 (M=  26) - s=0.0100\n",
      " 107 - L= 2.9176996 - Gamma=20.7668777 (M=  26) - s=0.0100\n",
      " 108 - L= 2.9177942 - Gamma=20.8477295 (M=  26) - s=0.0100\n",
      " 109 - L= 2.9178834 - Gamma=20.7388497 (M=  26) - s=0.0100\n",
      " 110 - L= 2.9179442 - Gamma=20.6582134 (M=  26) - s=0.0100\n",
      " 111 - L= 2.9180521 - Gamma=20.7310868 (M=  26) - s=0.0100\n",
      " 112 - L= 2.9181718 - Gamma=20.8109921 (M=  26) - s=0.0100\n",
      " 113 - L= 2.9182759 - Gamma=20.7326295 (M=  25) - s=0.0100\n",
      " 114 - L= 2.9184662 - Gamma=20.6389447 (M=  25) - s=0.0100\n",
      " 115 - L= 2.9184858 - Gamma=20.5913973 (M=  24) - s=0.0100\n",
      " 116 - L= 2.9186877 - Gamma=20.7012131 (M=  24) - s=0.0100\n",
      " 117 - L= 2.9188693 - Gamma=20.6782841 (M=  24) - s=0.0100\n",
      " 118 - L= 2.9190296 - Gamma=20.7579418 (M=  24) - s=0.0100\n",
      " 119 - L= 2.9192796 - Gamma=20.6011205 (M=  24) - s=0.0100\n",
      " 120 - L= 2.9196502 - Gamma=20.7109554 (M=  24) - s=0.0100\n",
      " 121 - L= 2.9197654 - Gamma=20.6219343 (M=  23) - s=0.0100\n",
      " 122 - L= 2.9202652 - Gamma=20.4177892 (M=  23) - s=0.0100\n",
      " 123 - L= 2.9208178 - Gamma=20.5346654 (M=  23) - s=0.0100\n",
      " 124 - L= 2.9209635 - Gamma=20.4322245 (M=  22) - s=0.0100\n",
      " 125 - L= 2.9213522 - Gamma=20.5209471 (M=  22) - s=0.0100\n",
      " 126 - L= 2.9215812 - Gamma=20.4890530 (M=  22) - s=0.0100\n",
      " 127 - L= 2.9217220 - Gamma=20.6183848 (M=  23) - s=0.0100\n",
      " 128 - L= 2.9218117 - Gamma=20.5438404 (M=  23) - s=0.0100\n",
      " 129 - L= 2.9218659 - Gamma=20.5395567 (M=  23) - s=0.0100\n",
      " 130 - L= 2.9219042 - Gamma=20.5408515 (M=  23) - s=0.0100\n",
      " 131 - L= 2.9219350 - Gamma=20.6044541 (M=  24) - s=0.0100\n",
      " 132 - L= 2.9219541 - Gamma=20.6049031 (M=  24) - s=0.0100\n",
      " 133 - L= 2.9219689 - Gamma=20.6049623 (M=  24) - s=0.0100\n",
      " 134 - L= 2.9219828 - Gamma=20.6020318 (M=  24) - s=0.0100\n",
      " 135 - L= 2.9219909 - Gamma=20.5954267 (M=  24) - s=0.0100\n",
      " 136 - L= 2.9220050 - Gamma=20.6326662 (M=  24) - s=0.0100\n",
      " 137 - L= 2.9220191 - Gamma=20.6008852 (M=  24) - s=0.0100\n",
      " 138 - L= 2.9220239 - Gamma=20.6219440 (M=  24) - s=0.0100\n",
      " 139 - L= 2.9220280 - Gamma=20.6219263 (M=  24) - s=0.0100\n",
      " 140 - L= 2.9220315 - Gamma=20.6141977 (M=  24) - s=0.0100\n",
      " 141 - L= 2.9220336 - Gamma=20.6106800 (M=  24) - s=0.0100\n",
      " 142 - L= 2.9220353 - Gamma=20.6163815 (M=  24) - s=0.0100\n",
      " 143 - L= 2.9220367 - Gamma=20.6163758 (M=  24) - s=0.0100\n",
      " 144 - L= 2.9220381 - Gamma=20.6164135 (M=  24) - s=0.0100\n",
      " 145 - L= 2.9220391 - Gamma=20.6164067 (M=  24) - s=0.0100\n",
      " 146 - L= 2.9220399 - Gamma=20.6164105 (M=  24) - s=0.0100\n",
      " 147 - L= 2.9220404 - Gamma=20.6158541 (M=  24) - s=0.0100\n",
      " 148 - L= 2.9220408 - Gamma=20.6100691 (M=  24) - s=0.0100\n",
      " 149 - L= 2.9220419 - Gamma=20.6196890 (M=  24) - s=0.0100\n",
      " 150 - L= 2.9220423 - Gamma=20.6196880 (M=  24) - s=0.0100\n",
      " 151 - L= 2.9220427 - Gamma=20.6196865 (M=  24) - s=0.0100\n",
      " 152 - L= 2.9220431 - Gamma=20.6143895 (M=  24) - s=0.0100\n",
      " 153 - L= 2.9220436 - Gamma=20.6221688 (M=  24) - s=0.0100\n",
      " 154 - L= 2.9220438 - Gamma=20.6218957 (M=  24) - s=0.0100\n",
      " 155 - L= 2.9220440 - Gamma=20.6219023 (M=  24) - s=0.0100\n",
      " 156 - L= 2.9220442 - Gamma=20.6219029 (M=  24) - s=0.0100\n",
      " 157 - L= 2.9220443 - Gamma=20.6219700 (M=  24) - s=0.0100\n",
      " 158 - L= 2.9220443 - Gamma=20.6195774 (M=  24) - s=0.0100\n",
      " 159 - L= 2.9220444 - Gamma=20.6193585 (M=  24) - s=0.0100\n",
      " 160 - L= 2.9220445 - Gamma=20.6223799 (M=  24) - s=0.0100\n",
      " 161 - L= 2.9220446 - Gamma=20.6210722 (M=  24) - s=0.0100\n",
      " 162 - L= 2.9220446 - Gamma=20.6210723 (M=  24) - s=0.0100\n",
      " 163 - L= 2.9220447 - Gamma=20.6210749 (M=  24) - s=0.0100\n",
      " 164 - L= 2.9220447 - Gamma=20.6201417 (M=  24) - s=0.0100\n",
      " 165 - L= 2.9220448 - Gamma=20.6219666 (M=  24) - s=0.0100\n",
      " 166 - L= 2.9220448 - Gamma=20.6203369 (M=  24) - s=0.0100\n",
      " 167 - L= 2.9220448 - Gamma=20.6221580 (M=  24) - s=0.0100\n",
      " 168 - L= 2.9220449 - Gamma=20.6221780 (M=  24) - s=0.0100\n",
      " 169 - L= 2.9220449 - Gamma=20.6219171 (M=  24) - s=0.0100\n",
      " 170 - L= 2.9220449 - Gamma=20.6218260 (M=  24) - s=0.0100\n",
      " 171 - L= 2.9220449 - Gamma=20.6226517 (M=  24) - s=0.0100\n",
      " 172 - L= 2.9220449 - Gamma=20.6217357 (M=  24) - s=0.0100\n",
      " 173 - L= 2.9220449 - Gamma=20.6217345 (M=  24) - s=0.0100\n",
      " 174 - L= 2.9220449 - Gamma=20.6214036 (M=  24) - s=0.0100\n",
      " 175 - L= 2.9220449 - Gamma=20.6220992 (M=  24) - s=0.0100\n",
      " 176 - L= 2.9220449 - Gamma=20.6221016 (M=  24) - s=0.0100\n",
      " 177 - L= 2.9220449 - Gamma=20.6221016 (M=  24) - s=0.0100\n",
      "Stopping at iteration 177 - max_delta_ml=1.6746438782005513e-07\n",
      "L=2.9220449209184327 - Gamma=20.622101624799768 (M=24) - s=0.01\n",
      "Initial alpha = [[ 0.08397002]]\n",
      "   1 - L=-501.2078166 - Gamma= 1.9999613 (M=   2) - s=0.0100\n",
      "   2 - L=-329.1986542 - Gamma= 2.9998973 (M=   3) - s=0.0100\n",
      "   3 - L=-226.1238935 - Gamma= 3.9997881 (M=   4) - s=0.0100\n",
      "   4 - L=-156.3004651 - Gamma= 4.9995976 (M=   5) - s=0.0100\n",
      "   5 - L=-113.8021313 - Gamma= 5.9993297 (M=   6) - s=0.0100\n",
      "   6 - L=-78.9966180 - Gamma= 6.9990048 (M=   7) - s=0.0100\n",
      "   7 - L=-54.5756378 - Gamma= 7.9985303 (M=   8) - s=0.0100\n",
      "   8 - L=-38.3941237 - Gamma= 8.9978350 (M=   9) - s=0.0100\n",
      "   9 - L=-25.8472342 - Gamma= 9.9968335 (M=  10) - s=0.0100\n",
      "  10 - L=-14.6749942 - Gamma=10.9957932 (M=  11) - s=0.0100\n",
      "  11 - L=-8.3347000 - Gamma=11.9939860 (M=  12) - s=0.0100\n",
      "  12 - L=-3.7555968 - Gamma=12.9915861 (M=  13) - s=0.0100\n",
      "  13 - L=-1.1481500 - Gamma=13.9873497 (M=  14) - s=0.0100\n",
      "  14 - L= 0.3190403 - Gamma=14.9801380 (M=  15) - s=0.0100\n",
      "  15 - L= 1.1475302 - Gamma=15.9671807 (M=  16) - s=0.0100\n",
      "  16 - L= 1.8212228 - Gamma=16.9514217 (M=  17) - s=0.0100\n",
      "  17 - L= 2.2700571 - Gamma=17.9277197 (M=  18) - s=0.0100\n",
      "  18 - L= 2.5261637 - Gamma=18.8867598 (M=  19) - s=0.0100\n",
      "  19 - L= 2.6083650 - Gamma=19.7919527 (M=  20) - s=0.0100\n",
      "  20 - L= 2.6953655 - Gamma=20.7007430 (M=  21) - s=0.0100\n",
      "  21 - L= 2.7122556 - Gamma=20.7009747 (M=  21) - s=0.0100\n",
      "  22 - L= 2.7274097 - Gamma=21.4324490 (M=  22) - s=0.0100\n",
      "  23 - L= 2.7384317 - Gamma=21.4367425 (M=  22) - s=0.0100\n",
      "  24 - L= 2.7457349 - Gamma=22.0579330 (M=  23) - s=0.0100\n",
      "  25 - L= 2.7514060 - Gamma=22.0575823 (M=  23) - s=0.0100\n",
      "  26 - L= 2.7558268 - Gamma=22.0578784 (M=  23) - s=0.0100\n",
      "  27 - L= 2.7586524 - Gamma=22.5082200 (M=  24) - s=0.0100\n",
      "  28 - L= 2.7610736 - Gamma=22.5088107 (M=  24) - s=0.0100\n",
      "  29 - L= 2.7632819 - Gamma=22.5151863 (M=  24) - s=0.0100\n",
      "  30 - L= 2.7653334 - Gamma=22.5152823 (M=  24) - s=0.0100\n",
      "  31 - L= 2.7672484 - Gamma=22.5155097 (M=  24) - s=0.0100\n",
      "  32 - L= 2.7685505 - Gamma=22.5083205 (M=  24) - s=0.0100\n",
      "  33 - L= 2.7694743 - Gamma=22.5140771 (M=  24) - s=0.0100\n",
      "  34 - L= 2.7703956 - Gamma=22.5140948 (M=  24) - s=0.0100\n",
      "  35 - L= 2.7712992 - Gamma=22.5439709 (M=  24) - s=0.0100\n",
      "  36 - L= 2.7720327 - Gamma=22.5246372 (M=  24) - s=0.0100\n",
      "  37 - L= 2.7726921 - Gamma=22.4015549 (M=  24) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  38 - L= 2.7733347 - Gamma=22.4017449 (M=  24) - s=0.0100\n",
      "  39 - L= 2.7739556 - Gamma=22.6530136 (M=  25) - s=0.0100\n",
      "  40 - L= 2.7745335 - Gamma=22.6537606 (M=  25) - s=0.0100\n",
      "  41 - L= 2.7750175 - Gamma=22.6540601 (M=  25) - s=0.0100\n",
      "  42 - L= 2.7752361 - Gamma=22.8038783 (M=  26) - s=0.0100\n",
      "  43 - L= 2.7753432 - Gamma=22.7902599 (M=  26) - s=0.0100\n",
      "  44 - L= 2.7754309 - Gamma=22.7902531 (M=  26) - s=0.0100\n",
      "  45 - L= 2.7755070 - Gamma=22.7390343 (M=  26) - s=0.0100\n",
      "  46 - L= 2.7755721 - Gamma=22.6950524 (M=  26) - s=0.0100\n",
      "  47 - L= 2.7756292 - Gamma=22.6974941 (M=  26) - s=0.0100\n",
      "  48 - L= 2.7756617 - Gamma=22.6975797 (M=  26) - s=0.0100\n",
      "  49 - L= 2.7756750 - Gamma=22.6944757 (M=  26) - s=0.0100\n",
      "  50 - L= 2.7756875 - Gamma=22.7377646 (M=  27) - s=0.0100\n",
      "  51 - L= 2.7756917 - Gamma=22.7555370 (M=  27) - s=0.0100\n",
      "  52 - L= 2.7756974 - Gamma=22.7417404 (M=  27) - s=0.0100\n",
      "  53 - L= 2.7757018 - Gamma=22.7281228 (M=  27) - s=0.0100\n",
      "  54 - L= 2.7757082 - Gamma=22.7578083 (M=  27) - s=0.0100\n",
      "  55 - L= 2.7757148 - Gamma=22.7792265 (M=  27) - s=0.0100\n",
      "  56 - L= 2.7757192 - Gamma=22.7666722 (M=  27) - s=0.0100\n",
      "  57 - L= 2.7757236 - Gamma=22.7666127 (M=  27) - s=0.0100\n",
      "  58 - L= 2.7757268 - Gamma=22.7868469 (M=  27) - s=0.0100\n",
      "  59 - L= 2.7757318 - Gamma=22.7718128 (M=  27) - s=0.0100\n",
      "  60 - L= 2.7757363 - Gamma=22.7890129 (M=  27) - s=0.0100\n",
      "  61 - L= 2.7757402 - Gamma=22.8107822 (M=  27) - s=0.0100\n",
      "  62 - L= 2.7757451 - Gamma=22.7971893 (M=  27) - s=0.0100\n",
      "  63 - L= 2.7757496 - Gamma=22.7941744 (M=  27) - s=0.0100\n",
      "  64 - L= 2.7757557 - Gamma=22.8133144 (M=  27) - s=0.0100\n",
      "  65 - L= 2.7757590 - Gamma=22.8331077 (M=  27) - s=0.0100\n",
      "  66 - L= 2.7757635 - Gamma=22.8186191 (M=  27) - s=0.0100\n",
      "  67 - L= 2.7757671 - Gamma=22.8066414 (M=  27) - s=0.0100\n",
      "  68 - L= 2.7757707 - Gamma=22.8210958 (M=  27) - s=0.0100\n",
      "  69 - L= 2.7757756 - Gamma=22.8443048 (M=  27) - s=0.0100\n",
      "  70 - L= 2.7757786 - Gamma=22.8440076 (M=  27) - s=0.0100\n",
      "  71 - L= 2.7757817 - Gamma=22.8272535 (M=  27) - s=0.0100\n",
      "  72 - L= 2.7757849 - Gamma=22.8157303 (M=  27) - s=0.0100\n",
      "  73 - L= 2.7757913 - Gamma=22.8342529 (M=  27) - s=0.0100\n",
      "  74 - L= 2.7757952 - Gamma=22.8542803 (M=  27) - s=0.0100\n",
      "  75 - L= 2.7757998 - Gamma=22.8511351 (M=  27) - s=0.0100\n",
      "  76 - L= 2.7758032 - Gamma=22.8330367 (M=  27) - s=0.0100\n",
      "  77 - L= 2.7758077 - Gamma=22.8188875 (M=  27) - s=0.0100\n",
      "  78 - L= 2.7758163 - Gamma=22.8394230 (M=  27) - s=0.0100\n",
      "  79 - L= 2.7758216 - Gamma=22.8623148 (M=  27) - s=0.0100\n",
      "  80 - L= 2.7758263 - Gamma=22.8474439 (M=  27) - s=0.0100\n",
      "  81 - L= 2.7758313 - Gamma=22.8251018 (M=  27) - s=0.0100\n",
      "  82 - L= 2.7758387 - Gamma=22.8435014 (M=  27) - s=0.0100\n",
      "  83 - L= 2.7758440 - Gamma=22.8572957 (M=  27) - s=0.0100\n",
      "  84 - L= 2.7758483 - Gamma=22.8358469 (M=  27) - s=0.0100\n",
      "  85 - L= 2.7758539 - Gamma=22.8377904 (M=  27) - s=0.0100\n",
      "  86 - L= 2.7758590 - Gamma=22.8596519 (M=  27) - s=0.0100\n",
      "  87 - L= 2.7758673 - Gamma=22.8391502 (M=  27) - s=0.0100\n",
      "  88 - L= 2.7758751 - Gamma=22.8572575 (M=  27) - s=0.0100\n",
      "  89 - L= 2.7758832 - Gamma=22.8528821 (M=  27) - s=0.0100\n",
      "  90 - L= 2.7758908 - Gamma=22.8234274 (M=  27) - s=0.0100\n",
      "  91 - L= 2.7758979 - Gamma=22.8481937 (M=  27) - s=0.0100\n",
      "  92 - L= 2.7759053 - Gamma=22.8280067 (M=  27) - s=0.0100\n",
      "  93 - L= 2.7759193 - Gamma=22.8511369 (M=  27) - s=0.0100\n",
      "  94 - L= 2.7759252 - Gamma=22.8242443 (M=  27) - s=0.0100\n",
      "  95 - L= 2.7759326 - Gamma=22.8400033 (M=  27) - s=0.0100\n",
      "  96 - L= 2.7759345 - Gamma=22.8253602 (M=  26) - s=0.0100\n",
      "  97 - L= 2.7759423 - Gamma=22.8037221 (M=  26) - s=0.0100\n",
      "  98 - L= 2.7759528 - Gamma=22.8325347 (M=  26) - s=0.0100\n",
      "  99 - L= 2.7759648 - Gamma=22.8530157 (M=  26) - s=0.0100\n",
      " 100 - L= 2.7759731 - Gamma=22.8483896 (M=  26) - s=0.0100\n",
      " 101 - L= 2.7759807 - Gamma=22.8261752 (M=  26) - s=0.0100\n",
      " 102 - L= 2.7759887 - Gamma=22.8057445 (M=  26) - s=0.0100\n",
      " 103 - L= 2.7759996 - Gamma=22.8337966 (M=  26) - s=0.0100\n",
      " 104 - L= 2.7760094 - Gamma=22.8514473 (M=  26) - s=0.0100\n",
      " 105 - L= 2.7760181 - Gamma=22.8266964 (M=  26) - s=0.0100\n",
      " 106 - L= 2.7760246 - Gamma=22.8264061 (M=  26) - s=0.0100\n",
      " 107 - L= 2.7760311 - Gamma=22.8255924 (M=  26) - s=0.0100\n",
      " 108 - L= 2.7760361 - Gamma=22.8256608 (M=  26) - s=0.0100\n",
      " 109 - L= 2.7760403 - Gamma=22.8103932 (M=  26) - s=0.0100\n",
      " 110 - L= 2.7760484 - Gamma=22.8336353 (M=  26) - s=0.0100\n",
      " 111 - L= 2.7760554 - Gamma=22.8479495 (M=  26) - s=0.0100\n",
      " 112 - L= 2.7760612 - Gamma=22.8473696 (M=  26) - s=0.0100\n",
      " 113 - L= 2.7760666 - Gamma=22.8272551 (M=  26) - s=0.0100\n",
      " 114 - L= 2.7760713 - Gamma=22.8268667 (M=  26) - s=0.0100\n",
      " 115 - L= 2.7760754 - Gamma=22.8113015 (M=  26) - s=0.0100\n",
      " 116 - L= 2.7760817 - Gamma=22.8310201 (M=  26) - s=0.0100\n",
      " 117 - L= 2.7760881 - Gamma=22.8579221 (M=  27) - s=0.0100\n",
      " 118 - L= 2.7760969 - Gamma=22.8529385 (M=  27) - s=0.0100\n",
      " 119 - L= 2.7761038 - Gamma=22.8320420 (M=  27) - s=0.0100\n",
      " 120 - L= 2.7761121 - Gamma=22.8342671 (M=  27) - s=0.0100\n",
      " 121 - L= 2.7761201 - Gamma=22.8631033 (M=  27) - s=0.0100\n",
      " 122 - L= 2.7761264 - Gamma=22.8426515 (M=  27) - s=0.0100\n",
      " 123 - L= 2.7761347 - Gamma=22.8646088 (M=  27) - s=0.0100\n",
      " 124 - L= 2.7761449 - Gamma=22.8356812 (M=  27) - s=0.0100\n",
      " 125 - L= 2.7761616 - Gamma=22.8566614 (M=  27) - s=0.0100\n",
      " 126 - L= 2.7761664 - Gamma=22.8565931 (M=  27) - s=0.0100\n",
      " 127 - L= 2.7761713 - Gamma=22.8358592 (M=  27) - s=0.0100\n",
      " 128 - L= 2.7761777 - Gamma=22.8142962 (M=  27) - s=0.0100\n",
      " 129 - L= 2.7761905 - Gamma=22.8402848 (M=  27) - s=0.0100\n",
      " 130 - L= 2.7761997 - Gamma=22.8703009 (M=  27) - s=0.0100\n",
      " 131 - L= 2.7762130 - Gamma=22.8638478 (M=  27) - s=0.0100\n",
      " 132 - L= 2.7762254 - Gamma=22.8327476 (M=  27) - s=0.0100\n",
      " 133 - L= 2.7762379 - Gamma=22.8660714 (M=  27) - s=0.0100\n",
      " 134 - L= 2.7762495 - Gamma=22.8463976 (M=  27) - s=0.0100\n",
      " 135 - L= 2.7762605 - Gamma=22.8455796 (M=  27) - s=0.0100\n",
      " 136 - L= 2.7762689 - Gamma=22.8658158 (M=  27) - s=0.0100\n",
      " 137 - L= 2.7762814 - Gamma=22.8311185 (M=  27) - s=0.0100\n",
      " 138 - L= 2.7762996 - Gamma=22.8518090 (M=  27) - s=0.0100\n",
      " 139 - L= 2.7763131 - Gamma=22.8179317 (M=  27) - s=0.0100\n",
      " 140 - L= 2.7763303 - Gamma=22.8551951 (M=  27) - s=0.0100\n",
      " 141 - L= 2.7763483 - Gamma=22.8471676 (M=  27) - s=0.0100\n",
      " 142 - L= 2.7763637 - Gamma=22.8732717 (M=  27) - s=0.0100\n",
      " 143 - L= 2.7763853 - Gamma=22.8248174 (M=  27) - s=0.0100\n",
      " 144 - L= 2.7764162 - Gamma=22.7701339 (M=  27) - s=0.0100\n",
      " 145 - L= 2.7764454 - Gamma=22.8157125 (M=  27) - s=0.0100\n",
      " 146 - L= 2.7764645 - Gamma=22.8187955 (M=  27) - s=0.0100\n",
      " 147 - L= 2.7764870 - Gamma=22.7689209 (M=  27) - s=0.0100\n",
      " 148 - L= 2.7765192 - Gamma=22.8043749 (M=  27) - s=0.0100\n",
      " 149 - L= 2.7765382 - Gamma=22.7558993 (M=  27) - s=0.0100\n",
      " 150 - L= 2.7765747 - Gamma=22.7833662 (M=  27) - s=0.0100\n",
      " 151 - L= 2.7765951 - Gamma=22.7741496 (M=  27) - s=0.0100\n",
      " 152 - L= 2.7766144 - Gamma=22.7250076 (M=  27) - s=0.0100\n",
      " 153 - L= 2.7766527 - Gamma=22.7739537 (M=  27) - s=0.0100\n",
      " 154 - L= 2.7766649 - Gamma=22.7509938 (M=  26) - s=0.0100\n",
      " 155 - L= 2.7766966 - Gamma=22.6833386 (M=  26) - s=0.0100\n",
      " 156 - L= 2.7767418 - Gamma=22.7222099 (M=  26) - s=0.0100\n",
      " 157 - L= 2.7767443 - Gamma=22.7112777 (M=  25) - s=0.0100\n",
      " 158 - L= 2.7767889 - Gamma=22.6698062 (M=  25) - s=0.0100\n",
      " 159 - L= 2.7768350 - Gamma=22.7491693 (M=  26) - s=0.0100\n",
      " 160 - L= 2.7768670 - Gamma=22.7905089 (M=  26) - s=0.0100\n",
      " 161 - L= 2.7769179 - Gamma=22.7420637 (M=  26) - s=0.0100\n",
      " 162 - L= 2.7769816 - Gamma=22.7238346 (M=  26) - s=0.0100\n",
      " 163 - L= 2.7770276 - Gamma=22.7688569 (M=  26) - s=0.0100\n",
      " 164 - L= 2.7770621 - Gamma=22.7672673 (M=  26) - s=0.0100\n",
      " 165 - L= 2.7770948 - Gamma=22.7248455 (M=  26) - s=0.0100\n",
      " 166 - L= 2.7771498 - Gamma=22.8037532 (M=  26) - s=0.0100\n",
      " 167 - L= 2.7771722 - Gamma=22.8028209 (M=  26) - s=0.0100\n",
      " 168 - L= 2.7771936 - Gamma=22.7984681 (M=  26) - s=0.0100\n",
      " 169 - L= 2.7772177 - Gamma=22.8247038 (M=  26) - s=0.0100\n",
      " 170 - L= 2.7772437 - Gamma=22.8119004 (M=  26) - s=0.0100\n",
      " 171 - L= 2.7772778 - Gamma=22.7652948 (M=  26) - s=0.0100\n",
      " 172 - L= 2.7773546 - Gamma=22.8171931 (M=  26) - s=0.0100\n",
      " 173 - L= 2.7773863 - Gamma=22.7683555 (M=  26) - s=0.0100\n",
      " 174 - L= 2.7774394 - Gamma=22.8390299 (M=  26) - s=0.0100\n",
      " 175 - L= 2.7774745 - Gamma=22.8227002 (M=  26) - s=0.0100\n",
      " 176 - L= 2.7775032 - Gamma=22.7729585 (M=  26) - s=0.0100\n",
      " 177 - L= 2.7775697 - Gamma=22.8162652 (M=  26) - s=0.0100\n",
      " 178 - L= 2.7775978 - Gamma=22.7632056 (M=  26) - s=0.0100\n",
      " 179 - L= 2.7776589 - Gamma=22.8515091 (M=  27) - s=0.0100\n",
      " 180 - L= 2.7776858 - Gamma=22.7965632 (M=  27) - s=0.0100\n",
      " 181 - L= 2.7777343 - Gamma=22.8783580 (M=  28) - s=0.0100\n",
      " 182 - L= 2.7777556 - Gamma=22.8457325 (M=  27) - s=0.0100\n",
      " 183 - L= 2.7778056 - Gamma=22.8238106 (M=  27) - s=0.0100\n",
      " 184 - L= 2.7778545 - Gamma=22.8580591 (M=  27) - s=0.0100\n",
      " 185 - L= 2.7778874 - Gamma=22.9099383 (M=  27) - s=0.0100\n",
      " 186 - L= 2.7779331 - Gamma=22.8776742 (M=  27) - s=0.0100\n",
      " 187 - L= 2.7779563 - Gamma=22.8226146 (M=  27) - s=0.0100\n",
      " 188 - L= 2.7779727 - Gamma=22.8570586 (M=  27) - s=0.0100\n",
      " 189 - L= 2.7779852 - Gamma=22.8535190 (M=  27) - s=0.0100\n",
      " 190 - L= 2.7779976 - Gamma=22.8979416 (M=  28) - s=0.0100\n",
      " 191 - L= 2.7780085 - Gamma=22.8642860 (M=  27) - s=0.0100\n",
      " 192 - L= 2.7780275 - Gamma=22.8496166 (M=  27) - s=0.0100\n",
      " 193 - L= 2.7780392 - Gamma=22.8500100 (M=  27) - s=0.0100\n",
      " 194 - L= 2.7780466 - Gamma=22.8804232 (M=  27) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 195 - L= 2.7780549 - Gamma=22.8657759 (M=  27) - s=0.0100\n",
      " 196 - L= 2.7780603 - Gamma=22.8941048 (M=  28) - s=0.0100\n",
      " 197 - L= 2.7780648 - Gamma=22.8940900 (M=  28) - s=0.0100\n",
      " 198 - L= 2.7780692 - Gamma=22.8940988 (M=  28) - s=0.0100\n",
      " 199 - L= 2.7780734 - Gamma=22.8926631 (M=  28) - s=0.0100\n",
      " 200 - L= 2.7780776 - Gamma=22.8919697 (M=  28) - s=0.0100\n",
      "MODEL: RVM accuracy:  0.534482758621 +/-: 0.0207545130256\n",
      "Initial alpha = [[ 0.06842466]]\n",
      "   1 - L=-533.8767583 - Gamma= 1.9999690 (M=   2) - s=0.0100\n",
      "   2 - L=-401.1326235 - Gamma= 2.9999013 (M=   3) - s=0.0100\n",
      "   3 - L=-314.6557264 - Gamma= 3.9998001 (M=   4) - s=0.0100\n",
      "   4 - L=-230.4649697 - Gamma= 4.9996848 (M=   5) - s=0.0100\n",
      "   5 - L=-168.4427812 - Gamma= 5.9995453 (M=   6) - s=0.0100\n",
      "   6 - L=-124.5886979 - Gamma= 6.9993379 (M=   7) - s=0.0100\n",
      "   7 - L=-85.6788568 - Gamma= 7.9991076 (M=   8) - s=0.0100\n",
      "   8 - L=-60.6315230 - Gamma= 8.9987308 (M=   9) - s=0.0100\n",
      "   9 - L=-39.5693949 - Gamma= 9.9982879 (M=  10) - s=0.0100\n",
      "  10 - L=-26.7530268 - Gamma=10.9975931 (M=  11) - s=0.0100\n",
      "  11 - L=-17.8198787 - Gamma=11.9964672 (M=  12) - s=0.0100\n",
      "  12 - L=-12.6658705 - Gamma=12.9947616 (M=  13) - s=0.0100\n",
      "  13 - L=-8.4645632 - Gamma=13.9925180 (M=  14) - s=0.0100\n",
      "  14 - L=-4.7573195 - Gamma=14.9898505 (M=  15) - s=0.0100\n",
      "  15 - L=-2.5663243 - Gamma=15.9856908 (M=  16) - s=0.0100\n",
      "  16 - L=-0.8421159 - Gamma=16.9804673 (M=  17) - s=0.0100\n",
      "  17 - L= 0.5714243 - Gamma=17.9741503 (M=  18) - s=0.0100\n",
      "  18 - L= 1.2816298 - Gamma=18.9624887 (M=  19) - s=0.0100\n",
      "  19 - L= 1.8082271 - Gamma=19.9465609 (M=  20) - s=0.0100\n",
      "  20 - L= 2.2483812 - Gamma=20.9230540 (M=  21) - s=0.0100\n",
      "  21 - L= 2.4766343 - Gamma=21.8889705 (M=  22) - s=0.0100\n",
      "  22 - L= 2.5986364 - Gamma=22.8306293 (M=  23) - s=0.0100\n",
      "  23 - L= 2.6870513 - Gamma=23.7493630 (M=  24) - s=0.0100\n",
      "  24 - L= 2.7451293 - Gamma=24.6466444 (M=  25) - s=0.0100\n",
      "  25 - L= 2.7863358 - Gamma=25.5103445 (M=  26) - s=0.0100\n",
      "  26 - L= 2.8008515 - Gamma=26.2567840 (M=  27) - s=0.0100\n",
      "  27 - L= 2.8043539 - Gamma=26.7784382 (M=  28) - s=0.0100\n",
      "  28 - L= 2.8071203 - Gamma=26.7839040 (M=  28) - s=0.0100\n",
      "  29 - L= 2.8099130 - Gamma=27.2546807 (M=  29) - s=0.0100\n",
      "  30 - L= 2.8127619 - Gamma=27.2561481 (M=  29) - s=0.0100\n",
      "  31 - L= 2.8154834 - Gamma=27.2562906 (M=  29) - s=0.0100\n",
      "  32 - L= 2.8175460 - Gamma=27.2575016 (M=  29) - s=0.0100\n",
      "  33 - L= 2.8191970 - Gamma=27.2578481 (M=  29) - s=0.0100\n",
      "  34 - L= 2.8208402 - Gamma=27.2579336 (M=  29) - s=0.0100\n",
      "  35 - L= 2.8223425 - Gamma=27.2655897 (M=  29) - s=0.0100\n",
      "  36 - L= 2.8234879 - Gamma=27.2650975 (M=  29) - s=0.0100\n",
      "  37 - L= 2.8246211 - Gamma=27.1623974 (M=  29) - s=0.0100\n",
      "  38 - L= 2.8255129 - Gamma=27.1645523 (M=  29) - s=0.0100\n",
      "  39 - L= 2.8262019 - Gamma=27.1245567 (M=  29) - s=0.0100\n",
      "  40 - L= 2.8267267 - Gamma=27.1257000 (M=  29) - s=0.0100\n",
      "  41 - L= 2.8272539 - Gamma=27.1437046 (M=  29) - s=0.0100\n",
      "  42 - L= 2.8277367 - Gamma=27.1438118 (M=  29) - s=0.0100\n",
      "  43 - L= 2.8282041 - Gamma=27.3852699 (M=  30) - s=0.0100\n",
      "  44 - L= 2.8286548 - Gamma=27.3852491 (M=  30) - s=0.0100\n",
      "  45 - L= 2.8289794 - Gamma=27.3884272 (M=  30) - s=0.0100\n",
      "  46 - L= 2.8291978 - Gamma=27.3884466 (M=  30) - s=0.0100\n",
      "  47 - L= 2.8293982 - Gamma=27.3923814 (M=  30) - s=0.0100\n",
      "  48 - L= 2.8295584 - Gamma=27.4098155 (M=  30) - s=0.0100\n",
      "  49 - L= 2.8296845 - Gamma=27.4099912 (M=  30) - s=0.0100\n",
      "  50 - L= 2.8298080 - Gamma=27.4107699 (M=  30) - s=0.0100\n",
      "  51 - L= 2.8299287 - Gamma=27.4107335 (M=  30) - s=0.0100\n",
      "  52 - L= 2.8300247 - Gamma=27.5279753 (M=  31) - s=0.0100\n",
      "  53 - L= 2.8301659 - Gamma=27.4401669 (M=  31) - s=0.0100\n",
      "  54 - L= 2.8302454 - Gamma=27.4401743 (M=  31) - s=0.0100\n",
      "  55 - L= 2.8303018 - Gamma=27.5193874 (M=  31) - s=0.0100\n",
      "  56 - L= 2.8303490 - Gamma=27.4917231 (M=  31) - s=0.0100\n",
      "  57 - L= 2.8304002 - Gamma=27.4414207 (M=  31) - s=0.0100\n",
      "  58 - L= 2.8304727 - Gamma=27.5192861 (M=  31) - s=0.0100\n",
      "  59 - L= 2.8305023 - Gamma=27.5890632 (M=  32) - s=0.0100\n",
      "  60 - L= 2.8305663 - Gamma=27.5264864 (M=  32) - s=0.0100\n",
      "  61 - L= 2.8306181 - Gamma=27.5942891 (M=  32) - s=0.0100\n",
      "  62 - L= 2.8307231 - Gamma=27.5056210 (M=  32) - s=0.0100\n",
      "  63 - L= 2.8307758 - Gamma=27.4732286 (M=  32) - s=0.0100\n",
      "  64 - L= 2.8308476 - Gamma=27.5437335 (M=  32) - s=0.0100\n",
      "  65 - L= 2.8309106 - Gamma=27.6355516 (M=  32) - s=0.0100\n",
      "  66 - L= 2.8310438 - Gamma=27.5314377 (M=  32) - s=0.0100\n",
      "  67 - L= 2.8311493 - Gamma=27.6124151 (M=  32) - s=0.0100\n",
      "  68 - L= 2.8312452 - Gamma=27.5619111 (M=  32) - s=0.0100\n",
      "  69 - L= 2.8313437 - Gamma=27.4565070 (M=  32) - s=0.0100\n",
      "  70 - L= 2.8314605 - Gamma=27.5678680 (M=  32) - s=0.0100\n",
      "  71 - L= 2.8315150 - Gamma=27.5001046 (M=  31) - s=0.0100\n",
      "  72 - L= 2.8316357 - Gamma=27.5788483 (M=  31) - s=0.0100\n",
      "  73 - L= 2.8318182 - Gamma=27.4361651 (M=  31) - s=0.0100\n",
      "  74 - L= 2.8319266 - Gamma=27.4427567 (M=  31) - s=0.0100\n",
      "  75 - L= 2.8320237 - Gamma=27.5103383 (M=  31) - s=0.0100\n",
      "  76 - L= 2.8321436 - Gamma=27.4425113 (M=  31) - s=0.0100\n",
      "  77 - L= 2.8322680 - Gamma=27.5101015 (M=  31) - s=0.0100\n",
      "  78 - L= 2.8323318 - Gamma=27.4113390 (M=  31) - s=0.0100\n",
      "  79 - L= 2.8323560 - Gamma=27.4385000 (M=  31) - s=0.0100\n",
      "  80 - L= 2.8323627 - Gamma=27.4213732 (M=  30) - s=0.0100\n",
      "  81 - L= 2.8323852 - Gamma=27.4183952 (M=  30) - s=0.0100\n",
      "  82 - L= 2.8324063 - Gamma=27.4618172 (M=  30) - s=0.0100\n",
      "  83 - L= 2.8324205 - Gamma=27.4618363 (M=  30) - s=0.0100\n",
      "  84 - L= 2.8324294 - Gamma=27.4415829 (M=  30) - s=0.0100\n",
      "  85 - L= 2.8324387 - Gamma=27.4433700 (M=  30) - s=0.0100\n",
      "  86 - L= 2.8324476 - Gamma=27.4622424 (M=  30) - s=0.0100\n",
      "  87 - L= 2.8324558 - Gamma=27.4614683 (M=  30) - s=0.0100\n",
      "  88 - L= 2.8324634 - Gamma=27.4615825 (M=  30) - s=0.0100\n",
      "  89 - L= 2.8324696 - Gamma=27.4615289 (M=  30) - s=0.0100\n",
      "  90 - L= 2.8324757 - Gamma=27.4613217 (M=  30) - s=0.0100\n",
      "  91 - L= 2.8324812 - Gamma=27.4609163 (M=  30) - s=0.0100\n",
      "  92 - L= 2.8324858 - Gamma=27.4608718 (M=  30) - s=0.0100\n",
      "  93 - L= 2.8324892 - Gamma=27.4705609 (M=  30) - s=0.0100\n",
      "  94 - L= 2.8324915 - Gamma=27.4705833 (M=  30) - s=0.0100\n",
      "  95 - L= 2.8324937 - Gamma=27.4706085 (M=  30) - s=0.0100\n",
      "  96 - L= 2.8324957 - Gamma=27.4708329 (M=  30) - s=0.0100\n",
      "  97 - L= 2.8324974 - Gamma=27.4617622 (M=  30) - s=0.0100\n",
      "  98 - L= 2.8324988 - Gamma=27.4596321 (M=  30) - s=0.0100\n",
      "  99 - L= 2.8325002 - Gamma=27.4596375 (M=  30) - s=0.0100\n",
      " 100 - L= 2.8325015 - Gamma=27.4637607 (M=  30) - s=0.0100\n",
      " 101 - L= 2.8325028 - Gamma=27.4630076 (M=  30) - s=0.0100\n",
      " 102 - L= 2.8325039 - Gamma=27.4630000 (M=  30) - s=0.0100\n",
      " 103 - L= 2.8325047 - Gamma=27.4640836 (M=  30) - s=0.0100\n",
      " 104 - L= 2.8325052 - Gamma=27.4640852 (M=  30) - s=0.0100\n",
      " 105 - L= 2.8325054 - Gamma=27.4640866 (M=  30) - s=0.0100\n",
      " 106 - L= 2.8325056 - Gamma=27.4688489 (M=  31) - s=0.0100\n",
      " 107 - L= 2.8325057 - Gamma=27.4721512 (M=  31) - s=0.0100\n",
      " 108 - L= 2.8325058 - Gamma=27.4721516 (M=  31) - s=0.0100\n",
      " 109 - L= 2.8325059 - Gamma=27.4723415 (M=  31) - s=0.0100\n",
      " 110 - L= 2.8325060 - Gamma=27.4702776 (M=  31) - s=0.0100\n",
      " 111 - L= 2.8325061 - Gamma=27.4718388 (M=  31) - s=0.0100\n",
      " 112 - L= 2.8325061 - Gamma=27.4718539 (M=  31) - s=0.0100\n",
      " 113 - L= 2.8325062 - Gamma=27.4718581 (M=  31) - s=0.0100\n",
      " 114 - L= 2.8325062 - Gamma=27.4717502 (M=  31) - s=0.0100\n",
      " 115 - L= 2.8325062 - Gamma=27.4707878 (M=  31) - s=0.0100\n",
      " 116 - L= 2.8325063 - Gamma=27.4737859 (M=  31) - s=0.0100\n",
      " 117 - L= 2.8325063 - Gamma=27.4721696 (M=  31) - s=0.0100\n",
      " 118 - L= 2.8325064 - Gamma=27.4739362 (M=  31) - s=0.0100\n",
      " 119 - L= 2.8325064 - Gamma=27.4725595 (M=  31) - s=0.0100\n",
      " 120 - L= 2.8325064 - Gamma=27.4740999 (M=  31) - s=0.0100\n",
      " 121 - L= 2.8325065 - Gamma=27.4740696 (M=  31) - s=0.0100\n",
      " 122 - L= 2.8325065 - Gamma=27.4757638 (M=  31) - s=0.0100\n",
      " 123 - L= 2.8325065 - Gamma=27.4746604 (M=  31) - s=0.0100\n",
      " 124 - L= 2.8325065 - Gamma=27.4745566 (M=  31) - s=0.0100\n",
      " 125 - L= 2.8325066 - Gamma=27.4761395 (M=  31) - s=0.0100\n",
      " 126 - L= 2.8325066 - Gamma=27.4748588 (M=  31) - s=0.0100\n",
      " 127 - L= 2.8325066 - Gamma=27.4764258 (M=  31) - s=0.0100\n",
      " 128 - L= 2.8325066 - Gamma=27.4753714 (M=  31) - s=0.0100\n",
      " 129 - L= 2.8325067 - Gamma=27.4753632 (M=  31) - s=0.0100\n",
      " 130 - L= 2.8325067 - Gamma=27.4753682 (M=  31) - s=0.0100\n",
      " 131 - L= 2.8325067 - Gamma=27.4752251 (M=  31) - s=0.0100\n",
      " 132 - L= 2.8325067 - Gamma=27.4765468 (M=  31) - s=0.0100\n",
      " 133 - L= 2.8325067 - Gamma=27.4757348 (M=  31) - s=0.0100\n",
      " 134 - L= 2.8325067 - Gamma=27.4769563 (M=  31) - s=0.0100\n",
      " 135 - L= 2.8325067 - Gamma=27.4768893 (M=  31) - s=0.0100\n",
      " 136 - L= 2.8325067 - Gamma=27.4780722 (M=  31) - s=0.0100\n",
      " 137 - L= 2.8325068 - Gamma=27.4770917 (M=  31) - s=0.0100\n",
      " 138 - L= 2.8325068 - Gamma=27.4781446 (M=  31) - s=0.0100\n",
      " 139 - L= 2.8325068 - Gamma=27.4772730 (M=  31) - s=0.0100\n",
      " 140 - L= 2.8325068 - Gamma=27.4783286 (M=  31) - s=0.0100\n",
      " 141 - L= 2.8325068 - Gamma=27.4776629 (M=  31) - s=0.0100\n",
      " 142 - L= 2.8325068 - Gamma=27.4776866 (M=  31) - s=0.0100\n",
      " 143 - L= 2.8325068 - Gamma=27.4776848 (M=  31) - s=0.0100\n",
      " 144 - L= 2.8325068 - Gamma=27.4776860 (M=  31) - s=0.0100\n",
      " 145 - L= 2.8325068 - Gamma=27.4785316 (M=  31) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 146 - L= 2.8325068 - Gamma=27.4779527 (M=  31) - s=0.0100\n",
      " 147 - L= 2.8325069 - Gamma=27.4778970 (M=  31) - s=0.0100\n",
      " 148 - L= 2.8325069 - Gamma=27.4786338 (M=  31) - s=0.0100\n",
      " 149 - L= 2.8325069 - Gamma=27.4795040 (M=  31) - s=0.0100\n",
      " 150 - L= 2.8325069 - Gamma=27.4788642 (M=  31) - s=0.0100\n",
      " 151 - L= 2.8325069 - Gamma=27.4788513 (M=  31) - s=0.0100\n",
      " 152 - L= 2.8325069 - Gamma=27.4795590 (M=  31) - s=0.0100\n",
      " 153 - L= 2.8325069 - Gamma=27.4789480 (M=  31) - s=0.0100\n",
      " 154 - L= 2.8325069 - Gamma=27.4791919 (M=  31) - s=0.0100\n",
      " 155 - L= 2.8325069 - Gamma=27.4791919 (M=  31) - s=0.0100\n",
      "Stopping at iteration 155 - max_delta_ml=1.8594140632896203e-07\n",
      "L=2.8325068949086756 - Gamma=27.47919194655743 (M=31) - s=0.01\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"RVM\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "MODELS.append({'method': \"RVM\", 'model': class_model})\n",
    "ACC = ACC.append(accuracy, ignore_index = True)\n",
    "Results = Rocket.DATA_merged_processed.copy()\n",
    "preds = [pred_ for pred_ in preds]\n",
    "Results['pred'] = preds\n",
    "Results['method'] = \"RVM\"\n",
    "AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]], ignore_index = True)\n",
    "\n",
    "    ####\n",
    "    ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613, 1) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 4s - loss: 6.4463 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 7.0079 - acc: 0.5652 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 3s - loss: 6.8588 - acc: 0.5745 - val_loss: 7.3264 - val_acc: 0.5455\n",
      "MODEL: CNN accuracy:  0.568965517241 +/-: 0.000337801318776\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613, 1) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 4s - loss: 5.6294 - acc: 0.5435 - val_loss: 9.2997 - val_acc: 0.4167\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 8.5937 - acc: 0.4348 - val_loss: 9.2997 - val_acc: 0.4167\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 8.3405 - acc: 0.4565 - val_loss: 9.2997 - val_acc: 0.4167\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 8.8521 - acc: 0.4348 - val_loss: 9.2997 - val_acc: 0.4167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 6.7615 - acc: 0.4130 - val_loss: 0.7107 - val_acc: 0.4167\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 3s - loss: 0.6964 - acc: 0.7174 - val_loss: 0.5731 - val_acc: 0.8333\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 0.5598 - acc: 0.6957 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 0.3269 - acc: 0.8696 - val_loss: 0.5219 - val_acc: 0.8333\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 0.1428 - acc: 0.9565 - val_loss: 0.6793 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 0.2126 - acc: 0.8696 - val_loss: 0.7520 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 3s - loss: 0.3332 - acc: 0.8478 - val_loss: 0.0398 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 0.0738 - acc: 0.9783 - val_loss: 0.0104 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s - loss: 0.0727 - acc: 0.9574 - val_loss: 9.2570e-04 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 3s - loss: 0.0202 - acc: 1.0000 - val_loss: 6.3220e-04 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 3s - loss: 0.0083 - acc: 1.0000 - val_loss: 2.8958e-04 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 3s - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 3s - loss: 0.0012 - acc: 1.0000 - val_loss: 7.6310e-05 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s - loss: 0.1715 - acc: 0.9787 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 3s - loss: 0.2533 - acc: 0.8936 - val_loss: 6.3689e-04 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 3s - loss: 0.0318 - acc: 0.9787 - val_loss: 3.7942e-04 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 3s - loss: 7.6904e-04 - acc: 1.0000 - val_loss: 1.2228e-04 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 3s - loss: 2.4624e-04 - acc: 1.0000 - val_loss: 6.4776e-05 - val_acc: 1.0000\n",
      "MODEL: CNN accuracy:  0.793103448276 +/-: 0.0635156559651\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613, 1) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 4s - loss: 7.3011 - acc: 0.3913 - val_loss: 6.7159 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 5.8864 - acc: 0.5435 - val_loss: 1.0849 - val_acc: 0.4167\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 0.9738 - acc: 0.6957 - val_loss: 0.8683 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 0.5161 - acc: 0.7609 - val_loss: 0.6834 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 0.2844 - acc: 0.8696 - val_loss: 0.6292 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 3s - loss: 0.4554 - acc: 0.8043 - val_loss: 0.2959 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 0.2599 - acc: 0.9348 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.0365 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 3s - loss: 0.0467 - acc: 0.9783 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 3s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 3s - loss: 0.0052 - acc: 1.0000 - val_loss: 2.2682e-04 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 3s - loss: 0.1910 - acc: 0.9348 - val_loss: 0.0356 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 3s - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0424 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 3s - loss: 0.0135 - acc: 1.0000 - val_loss: 3.3867e-04 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 3s - loss: 7.6667e-04 - acc: 1.0000 - val_loss: 8.7275e-05 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 3s - loss: 0.0083 - acc: 1.0000 - val_loss: 1.3449e-04 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 3s - loss: 0.0140 - acc: 1.0000 - val_loss: 6.5437e-05 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 3s - loss: 6.1408e-04 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 3s - loss: 0.0014 - acc: 1.0000 - val_loss: 1.4439e-05 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 3s - loss: 3.2821e-04 - acc: 1.0000 - val_loss: 3.6464e-06 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 3s - loss: 2.9321e-04 - acc: 1.0000 - val_loss: 3.7107e-07 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 3s - loss: 1.3075e-04 - acc: 1.0000 - val_loss: 6.5828e-07 - val_acc: 1.0000\n",
      "MODEL: CNN accuracy:  0.913793103448 +/-: 0.0284879112168\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "nruns = 3\n",
    "for i in range(0, nruns):\n",
    "    preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"CNN\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "    MODELS.append({'method': \"CNN\", 'model': class_model})\n",
    "    ACC = ACC.append(accuracy, ignore_index = True)\n",
    "    Results = Rocket.DATA_merged_processed.copy()\n",
    "    preds = [pred_ for pred_ in preds]\n",
    "    Results['pred'] = preds\n",
    "    Results['method'] = \"CNN\"\n",
    "    AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]],\n",
    "                                   ignore_index = True)\n",
    "\n",
    "    AllResults[Rocket.MODEL_PARAMETERS['ID']] = AllResults[Rocket.MODEL_PARAMETERS['ID']].astype('str')\n",
    "    AllResults = AllResults.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "    #AllResults[AllResults['Treatment_risk_group_in_ALL10'].notnull()]\n",
    "    ####\n",
    "    ####\n",
    "    Runs.append(AllResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 1s - loss: 0.6957 - acc: 0.5652 - val_loss: 0.7158 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6902 - acc: 0.5652 - val_loss: 0.6550 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6861 - acc: 0.5870 - val_loss: 0.6515 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.7024 - acc: 0.5652 - val_loss: 0.6817 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.6941 - acc: 0.5652 - val_loss: 0.6624 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6984 - acc: 0.5652 - val_loss: 0.6524 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6914 - acc: 0.5652 - val_loss: 0.6586 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6746 - acc: 0.5652 - val_loss: 0.6487 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6699 - acc: 0.5652 - val_loss: 0.7886 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.7079 - acc: 0.5652 - val_loss: 0.7779 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7496 - acc: 0.5435 - val_loss: 0.6704 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6528 - acc: 0.5652 - val_loss: 0.6571 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6725 - acc: 0.5652 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6447 - acc: 0.5652 - val_loss: 0.6952 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.6131 - acc: 0.5652 - val_loss: 0.6532 - val_acc: 0.5833\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.6835 - acc: 0.5957 - val_loss: 0.6919 - val_acc: 0.5455\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.6295 - acc: 0.5745 - val_loss: 0.6816 - val_acc: 0.6364\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.5759 - acc: 0.5745 - val_loss: 0.6284 - val_acc: 0.5455\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.5379 - acc: 0.6809 - val_loss: 0.6607 - val_acc: 0.3636\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.6015 - acc: 0.6809 - val_loss: 0.6658 - val_acc: 0.5455\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.5582 - acc: 0.6383 - val_loss: 0.5438 - val_acc: 0.7273\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.5003 - acc: 0.6596 - val_loss: 0.5754 - val_acc: 0.7273\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.6517 - acc: 0.5957 - val_loss: 0.6892 - val_acc: 0.5455\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.5930 - acc: 0.6596 - val_loss: 0.6429 - val_acc: 0.7273\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.5475 - acc: 0.7234 - val_loss: 0.6947 - val_acc: 0.4545\n",
      "MODEL: DNN accuracy:  0.551724137931 +/-: 0.00241865744244\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 1s - loss: 0.6965 - acc: 0.5217 - val_loss: 0.7059 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.7050 - acc: 0.4565 - val_loss: 0.6902 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6956 - acc: 0.5217 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.7043 - acc: 0.5000 - val_loss: 0.6868 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.6881 - acc: 0.5435 - val_loss: 0.6785 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6866 - acc: 0.5435 - val_loss: 0.6835 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6859 - acc: 0.5870 - val_loss: 0.6762 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6781 - acc: 0.5870 - val_loss: 0.6685 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6578 - acc: 0.6304 - val_loss: 0.7027 - val_acc: 0.4167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.7072 - acc: 0.6087 - val_loss: 0.6905 - val_acc: 0.4167\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6510 - acc: 0.6522 - val_loss: 0.5631 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6467 - acc: 0.6087 - val_loss: 0.5546 - val_acc: 0.7500\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6346 - acc: 0.5217 - val_loss: 0.5098 - val_acc: 0.6667\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6127 - acc: 0.7826 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.6130 - acc: 0.6522 - val_loss: 0.5156 - val_acc: 0.7500\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.6575 - acc: 0.6809 - val_loss: 0.6299 - val_acc: 0.6364\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.5933 - acc: 0.6809 - val_loss: 0.5217 - val_acc: 0.8182\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.4619 - acc: 0.8085 - val_loss: 0.5762 - val_acc: 0.6364\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.5487 - acc: 0.7021 - val_loss: 0.4357 - val_acc: 0.9091\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.4723 - acc: 0.7660 - val_loss: 0.5586 - val_acc: 0.8182\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.4371 - acc: 0.8085 - val_loss: 0.8303 - val_acc: 0.5455\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.4716 - acc: 0.7872 - val_loss: 0.2879 - val_acc: 0.9091\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.3577 - acc: 0.8085 - val_loss: 0.4591 - val_acc: 0.7273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.4807 - acc: 0.7660 - val_loss: 0.5450 - val_acc: 0.6364\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.3677 - acc: 0.8298 - val_loss: 0.6870 - val_acc: 0.6364\n",
      "MODEL: DNN accuracy:  0.637931034483 +/-: 0.0195069001549\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 54613) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 1s - loss: 0.7013 - acc: 0.5435 - val_loss: 0.6948 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6925 - acc: 0.4783 - val_loss: 0.6869 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.7208 - acc: 0.4783 - val_loss: 0.6857 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6922 - acc: 0.5652 - val_loss: 0.6835 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.6831 - acc: 0.5435 - val_loss: 0.6870 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6752 - acc: 0.5652 - val_loss: 0.6775 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6735 - acc: 0.5652 - val_loss: 0.6874 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.7488 - acc: 0.5870 - val_loss: 0.6731 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.6683 - acc: 0.5652 - val_loss: 0.6911 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.6423 - acc: 0.5870 - val_loss: 0.6277 - val_acc: 0.5833\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6382 - acc: 0.5435 - val_loss: 0.6244 - val_acc: 0.8333\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.6174 - acc: 0.5435 - val_loss: 0.5310 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.6117 - acc: 0.6304 - val_loss: 0.6517 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.5531 - acc: 0.6522 - val_loss: 0.3747 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.5885 - acc: 0.6957 - val_loss: 0.4771 - val_acc: 0.9167\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.5733 - acc: 0.6809 - val_loss: 0.5507 - val_acc: 0.8182\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.6086 - acc: 0.7660 - val_loss: 0.7011 - val_acc: 0.4545\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.4527 - acc: 0.8085 - val_loss: 0.4558 - val_acc: 0.7273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.4317 - acc: 0.8085 - val_loss: 0.8739 - val_acc: 0.4545\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.4134 - acc: 0.8298 - val_loss: 1.1562 - val_acc: 0.4545\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.4027 - acc: 0.7447 - val_loss: 0.4180 - val_acc: 0.8182\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.4750 - acc: 0.7872 - val_loss: 0.4269 - val_acc: 0.8182\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.4651 - acc: 0.7447 - val_loss: 0.3917 - val_acc: 0.8182\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.2876 - acc: 0.8936 - val_loss: 0.3384 - val_acc: 0.9091\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.2758 - acc: 0.8723 - val_loss: 0.5455 - val_acc: 0.7273\n",
      "MODEL: DNN accuracy:  0.655172413793 +/-: 0.0249027132202\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "nruns = 3\n",
    "for i in range(0, nruns):\n",
    "    preds, class_model, accuracy = Rocket.classify_treatment(model_type = \"DNN\", \n",
    "                                                      features = 'genomic',\n",
    "                                                      parameters = {},\n",
    "                                                      pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                                                  \"pre_processing\": {\"patient_grouping\": GROUPING, \n",
    "                                                                                     \"bias_removal\": False},\n",
    "                                                                  \"dim_reduction\": {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM},\n",
    "                                                                  \"feature_selection\": {\"type\": \"RFECV\", \"top_n\": 100}})\n",
    "    MODELS.append({'method': \"DNN\", 'model': class_model})\n",
    "    ACC = ACC.append(accuracy, ignore_index = True)\n",
    "    Results = Rocket.DATA_merged_processed.copy()\n",
    "    preds = [pred_ for pred_ in preds]\n",
    "    Results['pred'] = preds\n",
    "    Results['method'] = \"DNN\"\n",
    "    AllResults = AllResults.append(Results[[Rocket.MODEL_PARAMETERS['ID'], 'pred', 'method', Rocket.MODEL_PARAMETERS['target']]],\n",
    "                                   ignore_index = True)\n",
    "\n",
    "    AllResults[Rocket.MODEL_PARAMETERS['ID']] = AllResults[Rocket.MODEL_PARAMETERS['ID']].astype('str')\n",
    "    AllResults = AllResults.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "    #AllResults[AllResults['Treatment_risk_group_in_ALL10'].notnull()]\n",
    "    ####\n",
    "    ####\n",
    "    Runs.append(AllResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on average: 0.6824712643678161 +- 0.021263091677776572, median: 0.6637931034482759+-0.020322127337585125\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "         acc model       var\n",
      "0   0.655172   XGB  0.013408\n",
      "1   0.775862  LGBM  0.021209\n",
      "2   0.672414    ET  0.015728\n",
      "3   0.724138   SVM  0.024997\n",
      "4   0.706897    LR  0.019890\n",
      "5   0.534483   RVM  0.020755\n",
      "6   0.568966   CNN  0.000338\n",
      "7   0.793103   CNN  0.063516\n",
      "8   0.913793   CNN  0.028488\n",
      "9   0.551724   DNN  0.002419\n",
      "10  0.637931   DNN  0.019507\n",
      "11  0.655172   DNN  0.024903\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on average: {} +- {}, median: {}+-{}\".format(ACC.mean()[0], ACC.mean()[1], ACC.median()[0], ACC.median()[1]))\n",
    "print(\"+\"*40)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "###########\n",
    "##Runs.append(AllResults)\n",
    "final_df = pandas.DataFrame()\n",
    "for idx, df in enumerate(Runs):\n",
    "    df['run'] = idx\n",
    "    final_df = final_df.append(df, ignore_index = True)\n",
    "final_df = final_df.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "final_df['pred']= pandas.to_numeric(final_df['pred'])\n",
    "final_df_agg = final_df.groupby([Rocket.MODEL_PARAMETERS['ID'], 'method']).agg({'pred': [numpy.mean, numpy.median, numpy.std]})\n",
    "final_df_agg = final_df_agg['pred'].groupby(by=Rocket.MODEL_PARAMETERS['ID']).agg({'mean': [numpy.mean, numpy.median, numpy.std]})['mean']\n",
    "final_df.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/patient_results_\"+Rocket.SET_NAME+\".csv\")\n",
    "final_df_agg.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/patient_results_agg_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_weights, top_coeffs = _helpers.get_top_genes(MODELS=MODELS, n_max=40000, RexR=Rocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM</th>\n",
       "      <th>LR</th>\n",
       "      <th>MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240189_at</th>\n",
       "      <td>-1.054229</td>\n",
       "      <td>-1.028414</td>\n",
       "      <td>-1.041321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217511_at</th>\n",
       "      <td>-0.938391</td>\n",
       "      <td>-0.935089</td>\n",
       "      <td>-0.936740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207375_s_at</th>\n",
       "      <td>-0.848248</td>\n",
       "      <td>-0.916260</td>\n",
       "      <td>-0.882254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218862_at</th>\n",
       "      <td>-0.791441</td>\n",
       "      <td>-0.826107</td>\n",
       "      <td>-0.808774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569566_at</th>\n",
       "      <td>-0.735377</td>\n",
       "      <td>-0.827229</td>\n",
       "      <td>-0.781303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554507_at</th>\n",
       "      <td>-0.797081</td>\n",
       "      <td>-0.759114</td>\n",
       "      <td>-0.778098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228551_at</th>\n",
       "      <td>-0.743031</td>\n",
       "      <td>-0.774849</td>\n",
       "      <td>-0.758940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227499_at</th>\n",
       "      <td>-0.742585</td>\n",
       "      <td>-0.771676</td>\n",
       "      <td>-0.757130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217897_at</th>\n",
       "      <td>-0.751281</td>\n",
       "      <td>-0.761960</td>\n",
       "      <td>-0.756621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204005_s_at</th>\n",
       "      <td>-0.744349</td>\n",
       "      <td>-0.764252</td>\n",
       "      <td>-0.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229228_at</th>\n",
       "      <td>-0.751630</td>\n",
       "      <td>-0.749385</td>\n",
       "      <td>-0.750507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230425_at</th>\n",
       "      <td>-0.743030</td>\n",
       "      <td>-0.750474</td>\n",
       "      <td>-0.746752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224534_at</th>\n",
       "      <td>-0.734002</td>\n",
       "      <td>-0.717482</td>\n",
       "      <td>-0.725742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221933_at</th>\n",
       "      <td>-0.679071</td>\n",
       "      <td>-0.760312</td>\n",
       "      <td>-0.719691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557661_at</th>\n",
       "      <td>-0.686287</td>\n",
       "      <td>-0.723251</td>\n",
       "      <td>-0.704769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568713_a_at</th>\n",
       "      <td>-0.661157</td>\n",
       "      <td>-0.725583</td>\n",
       "      <td>-0.693370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556504_at</th>\n",
       "      <td>-0.684503</td>\n",
       "      <td>-0.690343</td>\n",
       "      <td>-0.687423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219863_at</th>\n",
       "      <td>-0.681245</td>\n",
       "      <td>-0.692121</td>\n",
       "      <td>-0.686683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215229_at</th>\n",
       "      <td>-0.657471</td>\n",
       "      <td>-0.703838</td>\n",
       "      <td>-0.680654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554689_a_at</th>\n",
       "      <td>-0.647493</td>\n",
       "      <td>-0.697561</td>\n",
       "      <td>-0.672527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233778_at</th>\n",
       "      <td>-0.665869</td>\n",
       "      <td>-0.669601</td>\n",
       "      <td>-0.667735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552726_at</th>\n",
       "      <td>-0.627445</td>\n",
       "      <td>-0.702185</td>\n",
       "      <td>-0.664815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223017_at</th>\n",
       "      <td>-0.642241</td>\n",
       "      <td>-0.681073</td>\n",
       "      <td>-0.661657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225109_at</th>\n",
       "      <td>-0.671813</td>\n",
       "      <td>-0.647313</td>\n",
       "      <td>-0.659563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223477_s_at</th>\n",
       "      <td>-0.638299</td>\n",
       "      <td>-0.679768</td>\n",
       "      <td>-0.659033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563420_at</th>\n",
       "      <td>-0.659538</td>\n",
       "      <td>-0.658482</td>\n",
       "      <td>-0.659010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230624_at</th>\n",
       "      <td>-0.670555</td>\n",
       "      <td>-0.647420</td>\n",
       "      <td>-0.658987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207069_s_at</th>\n",
       "      <td>-0.631796</td>\n",
       "      <td>-0.684263</td>\n",
       "      <td>-0.658029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242109_at</th>\n",
       "      <td>-0.612161</td>\n",
       "      <td>-0.694763</td>\n",
       "      <td>-0.653462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227250_at</th>\n",
       "      <td>-0.644162</td>\n",
       "      <td>-0.661638</td>\n",
       "      <td>-0.652900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205608_s_at</th>\n",
       "      <td>0.634118</td>\n",
       "      <td>0.660334</td>\n",
       "      <td>0.647226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45297_at</th>\n",
       "      <td>0.624005</td>\n",
       "      <td>0.689344</td>\n",
       "      <td>0.656674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209942_x_at</th>\n",
       "      <td>0.650425</td>\n",
       "      <td>0.670234</td>\n",
       "      <td>0.660329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233173_x_at</th>\n",
       "      <td>0.651068</td>\n",
       "      <td>0.678569</td>\n",
       "      <td>0.664819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234886_at</th>\n",
       "      <td>0.659080</td>\n",
       "      <td>0.683233</td>\n",
       "      <td>0.671157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211807_x_at</th>\n",
       "      <td>0.682074</td>\n",
       "      <td>0.660494</td>\n",
       "      <td>0.671284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221870_at</th>\n",
       "      <td>0.640784</td>\n",
       "      <td>0.710644</td>\n",
       "      <td>0.675714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232247_at</th>\n",
       "      <td>0.664898</td>\n",
       "      <td>0.686675</td>\n",
       "      <td>0.675786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213367_at</th>\n",
       "      <td>0.656896</td>\n",
       "      <td>0.699785</td>\n",
       "      <td>0.678341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202800_at</th>\n",
       "      <td>0.649153</td>\n",
       "      <td>0.716307</td>\n",
       "      <td>0.682730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209930_s_at</th>\n",
       "      <td>0.658430</td>\n",
       "      <td>0.711595</td>\n",
       "      <td>0.685013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209101_at</th>\n",
       "      <td>0.667895</td>\n",
       "      <td>0.703340</td>\n",
       "      <td>0.685618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220445_s_at</th>\n",
       "      <td>0.684201</td>\n",
       "      <td>0.704353</td>\n",
       "      <td>0.694277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567023_at</th>\n",
       "      <td>0.680787</td>\n",
       "      <td>0.708542</td>\n",
       "      <td>0.694665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218692_at</th>\n",
       "      <td>0.704517</td>\n",
       "      <td>0.690464</td>\n",
       "      <td>0.697490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225791_at</th>\n",
       "      <td>0.695384</td>\n",
       "      <td>0.703595</td>\n",
       "      <td>0.699489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205910_s_at</th>\n",
       "      <td>0.714936</td>\n",
       "      <td>0.738675</td>\n",
       "      <td>0.726806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553830_s_at</th>\n",
       "      <td>0.738833</td>\n",
       "      <td>0.755402</td>\n",
       "      <td>0.747118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554097_a_at</th>\n",
       "      <td>0.759798</td>\n",
       "      <td>0.746887</td>\n",
       "      <td>0.753342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206065_s_at</th>\n",
       "      <td>0.759420</td>\n",
       "      <td>0.758005</td>\n",
       "      <td>0.758712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553574_at</th>\n",
       "      <td>0.801893</td>\n",
       "      <td>0.792881</td>\n",
       "      <td>0.797387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553415_at</th>\n",
       "      <td>0.791450</td>\n",
       "      <td>0.806027</td>\n",
       "      <td>0.798738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556065_at</th>\n",
       "      <td>0.802871</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0.809441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201005_at</th>\n",
       "      <td>0.820356</td>\n",
       "      <td>0.818831</td>\n",
       "      <td>0.819593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239860_at</th>\n",
       "      <td>0.853725</td>\n",
       "      <td>0.828164</td>\n",
       "      <td>0.840945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244678_at</th>\n",
       "      <td>0.875170</td>\n",
       "      <td>0.892068</td>\n",
       "      <td>0.883619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558834_s_at</th>\n",
       "      <td>0.900593</td>\n",
       "      <td>0.962146</td>\n",
       "      <td>0.931369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214603_at</th>\n",
       "      <td>0.930711</td>\n",
       "      <td>0.962248</td>\n",
       "      <td>0.946479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210467_x_at</th>\n",
       "      <td>0.976794</td>\n",
       "      <td>0.969593</td>\n",
       "      <td>0.973193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214612_x_at</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   SVM        LR      MEAN\n",
       "240189_at    -1.054229 -1.028414 -1.041321\n",
       "217511_at    -0.938391 -0.935089 -0.936740\n",
       "207375_s_at  -0.848248 -0.916260 -0.882254\n",
       "218862_at    -0.791441 -0.826107 -0.808774\n",
       "1569566_at   -0.735377 -0.827229 -0.781303\n",
       "1554507_at   -0.797081 -0.759114 -0.778098\n",
       "228551_at    -0.743031 -0.774849 -0.758940\n",
       "227499_at    -0.742585 -0.771676 -0.757130\n",
       "217897_at    -0.751281 -0.761960 -0.756621\n",
       "204005_s_at  -0.744349 -0.764252 -0.754300\n",
       "229228_at    -0.751630 -0.749385 -0.750507\n",
       "230425_at    -0.743030 -0.750474 -0.746752\n",
       "224534_at    -0.734002 -0.717482 -0.725742\n",
       "221933_at    -0.679071 -0.760312 -0.719691\n",
       "1557661_at   -0.686287 -0.723251 -0.704769\n",
       "1568713_a_at -0.661157 -0.725583 -0.693370\n",
       "1556504_at   -0.684503 -0.690343 -0.687423\n",
       "219863_at    -0.681245 -0.692121 -0.686683\n",
       "215229_at    -0.657471 -0.703838 -0.680654\n",
       "1554689_a_at -0.647493 -0.697561 -0.672527\n",
       "233778_at    -0.665869 -0.669601 -0.667735\n",
       "1552726_at   -0.627445 -0.702185 -0.664815\n",
       "223017_at    -0.642241 -0.681073 -0.661657\n",
       "225109_at    -0.671813 -0.647313 -0.659563\n",
       "223477_s_at  -0.638299 -0.679768 -0.659033\n",
       "1563420_at   -0.659538 -0.658482 -0.659010\n",
       "230624_at    -0.670555 -0.647420 -0.658987\n",
       "207069_s_at  -0.631796 -0.684263 -0.658029\n",
       "242109_at    -0.612161 -0.694763 -0.653462\n",
       "227250_at    -0.644162 -0.661638 -0.652900\n",
       "...                ...       ...       ...\n",
       "205608_s_at   0.634118  0.660334  0.647226\n",
       "45297_at      0.624005  0.689344  0.656674\n",
       "209942_x_at   0.650425  0.670234  0.660329\n",
       "233173_x_at   0.651068  0.678569  0.664819\n",
       "234886_at     0.659080  0.683233  0.671157\n",
       "211807_x_at   0.682074  0.660494  0.671284\n",
       "221870_at     0.640784  0.710644  0.675714\n",
       "232247_at     0.664898  0.686675  0.675786\n",
       "213367_at     0.656896  0.699785  0.678341\n",
       "202800_at     0.649153  0.716307  0.682730\n",
       "209930_s_at   0.658430  0.711595  0.685013\n",
       "209101_at     0.667895  0.703340  0.685618\n",
       "220445_s_at   0.684201  0.704353  0.694277\n",
       "1567023_at    0.680787  0.708542  0.694665\n",
       "218692_at     0.704517  0.690464  0.697490\n",
       "225791_at     0.695384  0.703595  0.699489\n",
       "205910_s_at   0.714936  0.738675  0.726806\n",
       "1553830_s_at  0.738833  0.755402  0.747118\n",
       "1554097_a_at  0.759798  0.746887  0.753342\n",
       "206065_s_at   0.759420  0.758005  0.758712\n",
       "1553574_at    0.801893  0.792881  0.797387\n",
       "1553415_at    0.791450  0.806027  0.798738\n",
       "1556065_at    0.802871  0.816011  0.809441\n",
       "201005_at     0.820356  0.818831  0.819593\n",
       "239860_at     0.853725  0.828164  0.840945\n",
       "244678_at     0.875170  0.892068  0.883619\n",
       "1558834_s_at  0.900593  0.962146  0.931369\n",
       "214603_at     0.930711  0.962248  0.946479\n",
       "210467_x_at   0.976794  0.969593  0.973193\n",
       "214612_x_at   1.000000  1.000000  1.000000\n",
       "\n",
       "[40000 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f64875ba8d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f648747ea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_coeffs.LR.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualise the top weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_genomes_coeffs.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/coeffs_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_genomes_weights.to_csv(\"C:/Users/Bram van Es/DEV/RexR/out/weights_\"+Rocket.SET_NAME+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF9 = top_genomes_weights['RandomForest'].quantile(q=0.9)\n",
    "GBM9 = top_genomes_weights['GBM'].quantile(q=0.9)\n",
    "ADA9 = top_genomes_weights['AdaBoost'].quantile(q=0.9)\n",
    "ET9 = top_genomes_weights['ExtraTrees'].quantile(q=0.9)\n",
    "Overlapping_genomes = set(top_genomes_weights.loc[top_genomes_weights['RandomForest']>RF9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['GBM']>GBM9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['AdaBoost']>ADA9].index.values)\\\n",
    "                      & set(top_genomes_weights.loc[top_genomes_weights['ExtraTrees']>ET9].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "#from scipy.dspatial.distance import cosine\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cdist\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TransPosed = Rocket.DATA_all_samples.T # all microarrays, may be multiple per patient versus all probesets, may be multiple per genome\n",
    "Normal = Rocket.DATA_merged_processed.loc[:, (Rocket.DATA_merged_processed.columns !='target') & \n",
    "                                             (Rocket.DATA_merged_processed.columns !='ID')]\n",
    "#AllNormal = Rocket.DATA_merged\n",
    "#probeset_weights = Rocket.get_probeset_weights(method = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 9827_corr2.CEL, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'cosine', normalised = True, inflation = 2, minkowski_dim=1)\n",
    "##### apply Markov clustering\n",
    "#######################\n",
    "# non-distributed, non-sparse version, only for small-sized problems (N is order 1000)\n",
    "e = 2\n",
    "r = 2 \n",
    "epsilon = 1e-7\n",
    "convergence = 0.001\n",
    "num_iter = 10\n",
    "Orientation = 'col' # columnwise or rowwise\n",
    "\n",
    "# add loop\n",
    "def add_loop(df_matrix, value=0): \n",
    "    for i in df_matrix.index:\n",
    "        df_matrix.loc[i, i] = value\n",
    "    return df_matrix\n",
    "patient_sim = add_loop(patient_sim, 1)\n",
    "patient_sim = patient_sim - epsilon\n",
    "\n",
    "def normalise(sim, type = 'col'):\n",
    "    if(type == 'col'):\n",
    "        # column normalisation\n",
    "        for variable in sim.keys():\n",
    "            col_vec = sim[variable]\n",
    "            sum_val = sum([p for p in col_vec])\n",
    "            sim[variable] = sim[variable]/sum_val\n",
    "    elif (type == 'row'):\n",
    "        # row normalisation\n",
    "        for variable in sim.keys():\n",
    "            row_vec = sim.loc[variable, :]\n",
    "            sum_val = sum([p for p in row_vec])\n",
    "            sim.loc[variable,:] = sim.loc[variable,:]/sum_val\n",
    "    return sim\n",
    "\n",
    "# step E: expansion, get the nth power of the matrix\n",
    "def expansion(sim):\n",
    "    X = numpy.array(sim)\n",
    "    VarList = sim.keys()\n",
    "    if e == 1:\n",
    "        return sim\n",
    "    elif e > 1:        \n",
    "        return pandas.DataFrame(numpy.linalg.matrix_power(X, e), index = VarList, columns = VarList)\n",
    "     \n",
    "# step I: inflation, per column raise by rth power and column normalise\n",
    "def inflation(sim, type = 'col'):    \n",
    "    if type == 'col':\n",
    "        Axis = 0\n",
    "    elif type == 'row':\n",
    "        Axis = 1\n",
    "    return sim.apply(lambda x: x**r/sum(x**r), axis = Axis)\n",
    "\n",
    "# remove weak connections, values < epsilon\n",
    "def clean(sim):\n",
    "    return sim.applymap(lambda x:0 if x<epsilon else x)\n",
    "    \n",
    "def difference(old, new):\n",
    "    # relative zeroes over entire array\n",
    "    #return (new.apply(lambda x: numpy.ceil(x-epsilon)) - old.apply(lambda x: numpy.ceil(x-epsilon))).sum().sum()/len(old)**2    \n",
    "    return abs(new - old).sum().sum()/len(old)**2    \n",
    "\n",
    "#patient_sim = normalise(patient_sim, type = Orientation)\n",
    "_sim_a = patient_sim\n",
    "for i in range(0,num_iter):\n",
    "    # repeat E and I until convergence, the row-wise elements form the clusters.\n",
    "    _sim_b = clean(inflation(expansion(_sim_a), type = Orientation))\n",
    "    _sim_a = normalise(_sim_a, type = Orientation)\n",
    "    #if ((difference(_sim_a, _sim_b)) < convergence) & (i>0):\n",
    "    #    print(difference(_sim_a, _sim_b))\n",
    "    #    print(\"CONVERGED after \", i, \" iterations\")\n",
    "    #    break;\n",
    "    _sim_a = _sim_b\n",
    "\n",
    "result_mcl = clean(_sim_b)\n",
    "result_mcl.loc[result_mcl.loc['9827_corr2.CEL',:]>epsilon, '9827_corr2.CEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 patient clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'pearson', normalised = False, inflation=1, minkowski_dim=1)\n",
    "##### apply Affinity Propagation\n",
    "#######################\n",
    "X = numpy.array(patient_sim)\n",
    "af = AffinityPropagation(preference=-10).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "patient_clusters = patient_sim.keys()[cluster_centers_indices].values\n",
    "patient_cluster_members = af.labels_\n",
    "print(\"There are {} patient clusters\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AggResults = Rocket.DATA_merged\n",
    "AggResults = _helpers._preprocess(AggResults, Rclass = Rocket)\n",
    "#AggResults = _helpers._group_patients(AggResults, method = 'mean')\n",
    "AggResults['cluster_ap'] = patient_cluster_members\n",
    "\n",
    "#AggResults.groupby(['Treatment risk group in ALL10', 'cluster_ap']).agg({'Microarray file': pandas.Series.nunique})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "AggResults['FU_RFS'] = pandas.to_numeric(AggResults['FU_RFS'])\n",
    "AggResults['FU_EFS'] = pandas.to_numeric(AggResults['FU_EFS'])\n",
    "AggResults['FU_OS'] = pandas.to_numeric(AggResults['FU_OS'])\n",
    "AggResults['WhiteBloodCellcount'] = pandas.to_numeric(AggResults['WhiteBloodCellcount'])\n",
    "AggResults['Age'] = pandas.to_numeric(AggResults['Age'])\n",
    "AggResults['Gender'] = pandas.to_numeric(AggResults['Gender'])\n",
    "AggResults['code_RFS']= pandas.to_numeric(AggResults['code_RFS'])\n",
    "AggResults['code_EFS']= pandas.to_numeric(AggResults['code_EFS'])\n",
    "AggResults['code_OS']= pandas.to_numeric(AggResults['code_OS'])\n",
    "\n",
    "AggResults['mutations_NOTCH_pathway'] = pandas.to_numeric(AggResults['mutations_NOTCH_pathway'])\n",
    "AggResults['mutations_PTEN_AKT_pathway'] = pandas.to_numeric(AggResults['mutations_PTEN_AKT_pathway'])\n",
    "AggResults['mutations_IL7R_pathway'] = pandas.to_numeric(AggResults['mutations_IL7R_pathway'])\n",
    "#AggResults.replace(to_replace=9999, value=0.5, inplace=True)\n",
    "AggResults[['mutations_NOTCH_pathway', \n",
    "            'mutations_PTEN_AKT_pathway', \n",
    "            'mutations_IL7R_pathway']] = AggResults[['mutations_NOTCH_pathway', \n",
    "                                                    'mutations_PTEN_AKT_pathway', \n",
    "                                                    'mutations_IL7R_pathway']].replace([9999],[numpy.nan],\n",
    "                                                                                       inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "AggResults['comb_mutations_NOTCH_IL7R'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_NOTCH_PTEN'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_PTEN_AKT_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN'] =  AggResults['mutations_PTEN_AKT_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN_NOTCH'] =  AggResults['mutations_PTEN_AKT_pathway']\\\n",
    "                                                + AggResults['mutations_IL7R_pathway']\\\n",
    "                                                + AggResults['mutations_NOTCH_pathway']\n",
    "\n",
    "\n",
    "patient_count = AggResults.groupby(['cluster_ap']).agg({'labnr_patient': pandas.Series.nunique})\n",
    "Clustered_by_patients_whitebloodcells = AggResults[AggResults['WhiteBloodCellcount'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'WhiteBloodCellcount': numpy.mean,\n",
    "    'Age': numpy.mean, \n",
    "    'Gender': numpy.mean})\n",
    "\n",
    "# Cancer_gene\n",
    "# Treatment_protocol\n",
    "# Treatment_risk_group_in_ALL_10\n",
    "\n",
    "Clustered_by_patients_CODE = AggResults.groupby(['cluster_ap']).agg(\n",
    "    {'code_RFS': numpy.mean, \n",
    "     'code_EFS': numpy.mean,\n",
    "     'code_OS': numpy.mean})\n",
    "\n",
    "Clustered_by_patients_FU_RFS = AggResults[AggResults['FU_RFS'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'FU_RFS': numpy.median, \n",
    "     'FU_EFS': numpy.median,\n",
    "     'FU_OS': numpy.median})\n",
    "Clustered_by_patients_NotchPath = AggResults[AggResults['mutations_NOTCH_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_NOTCH_pathway': numpy.mean})\n",
    "Clustered_by_patients_IL7RPath = AggResults[AggResults['mutations_IL7R_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_IL7R_pathway': numpy.mean})\n",
    "Clustered_by_patients_PTENAKTPath = AggResults[AggResults['mutations_PTEN_AKT_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_PTEN_AKT_pathway': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_IL7R = AggResults[AggResults['comb_mutations_NOTCH_IL7R'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_IL7R': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_PTEN = AggResults[AggResults['comb_mutations_NOTCH_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN = AggResults[AggResults['comb_mutations_IL7R_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN_NOTCH = AggResults[AggResults['comb_mutations_IL7R_PTEN_NOTCH'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN_NOTCH': numpy.mean})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_agg = pandas.merge(Clustered_by_patients_whitebloodcells, Clustered_by_patients_CODE, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN_NOTCH, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_IL7R, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_PTEN, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_FU_RFS, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_IL7RPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_NotchPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_PTENAKTPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, patient_count, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Cluster centers:\",patient_sim.keys()[cluster_centers_indices].values)\n",
    "print(patient_cluster_members)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    class_members = patient_cluster_members == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.', \n",
    "             label = patient_sim.keys()[cluster_centers_indices[k]])\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.legend()\n",
    "        \n",
    "plt.title('Estimated number of clusters from Affinity Propagation: %d' % n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATE graph from similarity matrix\n",
    "##################\n",
    "# nodes\n",
    "VarList = TransPosed.keys()\n",
    "nodes = []\n",
    "node_index = 0\n",
    "for patient_name in VarList:\n",
    "    nodes.append((node_index, {'name': patient_name}))\n",
    "    node_index = node_index + 1\n",
    "\n",
    "edges = []\n",
    "# edges\n",
    "patient_sim = patient_similarity(Normal, sim_type = 'pearson', normalised = True, inflation=2)\n",
    "node_index_x = 0\n",
    "node_index_y = 0\n",
    "for patient_name_x in VarList:\n",
    "    for patient_name_y in VarList:        \n",
    "        edges.append((node_index_x, node_index_y, patient_sim.iloc[node_index_x, node_index_y]))\n",
    "        node_index_y = node_index_y + 1\n",
    "    node_index_x = node_index_x + 1\n",
    "    node_index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_weighted_edges_from(edges, weight = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:126: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  b = plt.ishold()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:138: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold(b)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFCCAYAAABSJMy8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f31beac128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### apply Spring-force\n",
    "#######################\n",
    "pos = nx.spring_layout(G, k = None, dim = 3, scale = 1.0)\n",
    "nx.draw_spring(G, k = 30, dim = 2, scale = 1.0, iterations =1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### APPLY community detector\n",
    "# maximize betweenness and modularity\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### LOAD IN DATA\n",
    "###################\n",
    "# https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
