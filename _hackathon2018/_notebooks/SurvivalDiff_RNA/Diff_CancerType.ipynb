{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import vaex as vx\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.interpolate import PchipInterpolator as minterp\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import ipyvolume as ipv\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "# Configure Plotly to be rendered inline in the notebook.\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "from numba import jit\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.covariance import OAS\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from sklearn.decomposition import PCA, MiniBatchDictionaryLearning as DL, NMF, FastICA as ICA \n",
    "from sklearn.decomposition import FactorAnalysis as FA, SparsePCA as SPCA\n",
    "\n",
    "from omic_helpers import omic_helpers as hlp\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier as xgb\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as HB\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ET\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier as mlpc\n",
    "from sklearn.neural_network import MLPRegressor as mlpr\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBRegressor as xgbr\n",
    "from lightgbm import LGBMRegressor as lgbmr\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ElasticNet, HuberRegressor, ARDRegression\n",
    "from pygam import GAM, LinearGAM, ExpectileGAM, s, f, l, te\n",
    "from ngboost import NGBRegressor as ngbr\n",
    "from ngboost.distns import LogNormal, Normal,Bernoulli\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.scores import MLE\n",
    "from pyearth import Earth\n",
    "from interpret.glassbox import ExplainableBoostingRegressor as ebr\n",
    "from interpret import show as eb_show\n",
    "\n",
    "from sklearn.inspection import permutation_importance, partial_dependence, plot_partial_dependence\n",
    "from shap import TreeExplainer, force_plot, dependence_plot, summary_plot, KernelExplainer, LinearExplainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, QuantileTransformer, MinMaxScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, average_precision_score\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif as Minfo, f_classif as Fval, chi2\n",
    "from scipy.stats import ks_2samp as ks, wasserstein_distance as wass\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "#from keras.layers import MaxPooling1D\n",
    "#from keras.callbacks import Callback\n",
    "#from keras.layers.convolutional import Conv1D\n",
    "#from keras.layers import Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_1dcnn(dims=None, conv_layers=[(32,3,3,1), (32,3,3,1), (32,3,3,1), (32,3,3,1), (32,3,3,1)], init_dropout=0.55, final_dropout=0.55):\n",
    "    num_feats = dims[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(init_dropout, input_shape=(num_feats, 1)))\n",
    "    #model.add(Conv1D(conv_layers[0][0], conv_layers[0][1], input_shape=(num_feats, 1)))\n",
    "    for _l in conv_layers:\n",
    "        model.add(Conv1D(filters=_l[0], kernel_size=_l[1], strides=_l[3]))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=_l[2]))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(final_dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', dice_loss])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def simple_dnn(dims=None, conv_layers=[(128,0.05), (64,0.05), (48,0.05), (32,0.1)], init_dropout=0.25, final_dropout=0.5):\n",
    "    num_feats = dims[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(init_dropout, input_shape=(num_feats, )))\n",
    "    #model.add(Conv1D(conv_layers[0][0], conv_layers[0][1], input_shape=(num_feats, 1)))\n",
    "    for _l in conv_layers:\n",
    "        model.add(Dense(_l[0], activation='relu'))\n",
    "        model.add(Dropout(_l[1]))\n",
    "        \n",
    "    model.add(Dropout(final_dropout))    \n",
    "    model.add(Dense(1,  activation='sigmoid'))   \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', dice_loss])     # 'binary_crossentropy'     \n",
    "    return model\n",
    "\n",
    "@jit\n",
    "def _diff_entropy(x, eps=1e-6, bins=20):\n",
    "    rhos, xs = np.histogram(x, density=True, bins=bins)\n",
    "    xmean =  (xs[1:] + xs[:-1])/2\n",
    "    xdiff = xs[1:] - xs[:-1]\n",
    "    H = -np.sum(rhos*np.log(rhos+eps)*xdiff)\n",
    "    Hr = H/np.sum(xdiff)\n",
    "    return Hr\n",
    "\n",
    "@jit\n",
    "def _ssqrt(x, y):\n",
    "    return np.sign(x*y)*np.sqrt(np.multiply(np.abs(x), np.abs(y)))\n",
    "@jit\n",
    "def _divisor(x, y, eps=1e-3):    \n",
    "    return np.sign(x)*np.divide((x**2), (x**2+y**2+eps))\n",
    "@jit\n",
    "def _sum(x, y):\n",
    "    return x+y\n",
    "\n",
    "@jit\n",
    "def _hmean(x ,y, eps=1e-6):\n",
    "    return 1/(1/(x+eps)+1/(y+eps))\n",
    "\n",
    "\n",
    "def pairwise_expander(x, y=None, fun=None):\n",
    "    '''\n",
    "        x : np array \n",
    "        fun : expansion function, assumes pairwise expansion\n",
    "    '''\n",
    "    if y is None:\n",
    "        num_rows, num_cols = x.shape[0], x.shape[1]\n",
    "        _num_cols = np.int((num_cols**2-num_cols)/2)\n",
    "        xex = np.zeros(shape=(num_rows, _num_cols)) \n",
    "        k=0\n",
    "        for jl in range(0, num_cols):\n",
    "            for jr in range(jl+1, num_cols):                \n",
    "                xex[:, k] =   fun(x[:,jl], x[:,jr])\n",
    "                k +=1\n",
    "    else:\n",
    "        num_rows_l, num_rows_r, num_cols_l, num_cols_r = x.shape[0], y.shape[0], x.shape[1], y.shape[1]\n",
    "        _num_cols = np.int(num_cols_l*num_cols_r)\n",
    "        xex = np.zeros(shape=(num_rows_l, _num_cols))\n",
    "        k=0\n",
    "        for jl in range(0, num_cols_l):\n",
    "            for jr in range(0, num_cols_r):               \n",
    "                xex[:, k] =   fun(x[:,jl], y[:,jr])\n",
    "                k +=1\n",
    "    return xex\n",
    "\n",
    "def pairwise_bulk_expansion(x, y=None, cols_l=None, cols_r=None, fun=None, index_c=None):\n",
    "    if \"DataFrame\" in str(type(x)):\n",
    "        index_c = x.index\n",
    "        x = x.values\n",
    "    else:\n",
    "        assert (\"list\" in str(type(index_c))) or \\\n",
    "                (\"ndarray\" in str(type(index_c))), \"index_c should be a vector\"\n",
    "        assert len(index_c) == x.shape[0], \"length of index_c should be equal to len(x)\"\n",
    "    if fun is None:\n",
    "        fun = _ssqrt\n",
    "        \n",
    "    def _cols_(cols_l, cols_r=None, prefix=None):\n",
    "        '''\n",
    "        x : left/intra data \n",
    "        y : right data\n",
    "        cols_l: left/intra column-names\n",
    "        cols_r: right column-names\n",
    "        fun : pairwise combination function\n",
    "        '''\n",
    "        prefix = prefix+\"_\" if prefix is not None else \"\"\n",
    "        ms = len(cols_l)+1 if cols_r==None else 0\n",
    "        cols_r = cols_l if cols_r is None else cols_r\n",
    "        return [prefix+cols_l[jl]+\"_\"+cols_l[jr] \n",
    "                for jl in range(0, len(cols_l)) \n",
    "                for jr in range(np.min([jl+1, ms]), len(cols_r))]\n",
    "    \n",
    "    ecols = _cols_(cols_l, cols_r)\n",
    "    ex = pairwise_expander(x, y, fun=fun)\n",
    "    \n",
    "    new = pd.DataFrame(data=ex, columns=ecols, index=index_c)\n",
    "    old = pd.DataFrame(data=x, columns=cols_l, index=index_c)\n",
    "    return old.join(new, how='inner')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_filter = True # filter out weak univariates for classification\n",
    "x_transform = MinMaxScaler() # MinMaxScaler, QuantileTransformer\n",
    "x_reducer = None # PCA(n_components=10)\n",
    "rem_coll = True\n",
    "outlier_replacer = True\n",
    "min_var_init = 1e-2\n",
    "min_var_quantile = 0.99 # to filter after the feature combiner\n",
    "feature_combiner = True\n",
    "combiner_function = _ssqrt\n",
    "\n",
    "viz_reduced = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gene list\n",
    "genes = pd.read_csv('../../_meta_data/gene_types/selected_genes.csv', sep='\\t')\n",
    "genes.Symbol = genes.Symbol.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/media/bramiozo/DATA-FAST/genetic_expression/hackathon_2/Lung/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with RNA expression, methylation and DNA mutations separately.\n",
    "\n",
    "The differentiation targets are the treatment response, the cancer types and the tissue type.\n",
    "\n",
    "We use :\n",
    "\n",
    "* statistical difference\n",
    "* geometric difference\n",
    "* multivariate classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNA \n",
    "# Methylation\n",
    "# mutation\n",
    "# Load meta data: immunoresponse, age, gender\n",
    "\n",
    "dd = {}\n",
    "dd['RNAex'] = vx.open('Lung_GeneExpression.hdf5')\n",
    "dd['mutation'] = vx.open('Lung_Mutation.hdf5')\n",
    "dd['methylation'] = vx.open('Lung_Methylation.hdf5')\n",
    "dd['methylation_meta'] = vx.open('HumanMethylation450_meta.hdf5')\n",
    "dd['CNV'] = vx.open('Lung_CNV.hdf5')\n",
    "\n",
    "miRNA = pd.read_csv('Lung_miRNA.txt', sep=\"\\t\")\n",
    "mimamap = miRNA[['MIMATID', 'Name', 'Chr', 'Start', 'Stop', 'Strand']]\n",
    "miRNA.drop(['Name', 'Chr', 'Start', 'Stop', 'Strand'], axis=1, inplace=True)\n",
    "miRNA = miRNA.set_index('MIMATID').transpose()\n",
    "\n",
    "proteome = pd.read_csv(\"Lung_Proteome.txt\", sep=\"\\t\")\n",
    "proteome = proteome.set_index('sample').transpose()\n",
    "\n",
    "meta = pd.read_csv('Lung_Phenotype_Metadata.txt', sep='\\t')\n",
    "meta = meta.loc[~pd.isna(meta.Diagnosis)]\n",
    "meta = meta[~meta.SampleID.duplicated(keep='last')]\n",
    "meta['SampleID']  = meta.SampleID.str.replace(\"\\-\", \"_\")\n",
    "meta.set_index('SampleID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "granzyms = list(set(dd['RNAex'][dd['RNAex'].Gene.str.contains('(GZM)|(SECT)|(PRF)')].Gene.tolist()))\n",
    "tmp = pd.DataFrame([{'Symbol':_int, 'gene_class': 'granzyms'} for _int in granzyms])\n",
    "genes = pd.concat([genes, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interferons = list(set(dd['RNAex'][dd['RNAex'].Gene.str.contains('IFN')].Gene.tolist()))\n",
    "tmp = pd.DataFrame([{'Symbol':_int, 'gene_class': 'interferon'} for _int in interferons])\n",
    "genes = pd.concat([genes, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = ['Gender', 'Diagnosis', 'Age At Diagnosis (Years)', \n",
    "             'Overall Survival Status', 'Pack Years', 'Smoking Status',\n",
    "             'Time To Overall Survival (Days)']\n",
    "meta_cols = meta_cols + ['New Tumor Event', 'Radiation Therapy', 'Reponse To Therapy', 'Drug Therapy Type']\n",
    "\n",
    "stage_map = {'stage i': 'stage 1', 'stage ia': 'stage 1', 'stage ib': 'stage 1',\n",
    "             'stage ii': 'stage 2', 'stage iia': 'stage 2', 'stage iib': 'stage 2',\n",
    "             'stage iii': 'stage 3','stage iiia': 'stage 3', 'stage iiib': 'stage 3',\n",
    "             'stage iv' : 'stage 4'}\n",
    "meta['Stage'] = meta['Tumor Stage'].map(stage_map)\n",
    "########\n",
    "smoke_map = {'Current Reformed Smoker < or = 15 yrs': 'reformed', \n",
    "             'Current Reformed Smoker for > 15 yrs': 'reformed',\n",
    "             'Current Reformed Smoker, Duration Not Specified': 'reformed',\n",
    "             'Current Smoker': 'current',\n",
    "             'Lifelong Non-Smoker': 'non-smoker'}\n",
    "meta['Smoking'] = meta['Smoking Status'].map(smoke_map)\n",
    "########\n",
    "response_map = {'Progressive Disease': 0,\n",
    "                'Complete Remission/Response': 1,\n",
    "                'Stable Disease': 0,\n",
    "                'Partial Remission/Response': 1}\n",
    "meta['Response'] = meta['Reponse To Therapy'].map(response_map)\n",
    "\n",
    "meta['Overall Survival Status'] =  meta['Overall Survival Status'].astype(int)\n",
    "\n",
    "gender_map = {'male': 0, 'female': 1}\n",
    "meta['Gender'] = meta['Gender'].map(gender_map)\n",
    "\n",
    "meta_cols = list(set(meta_cols + ['Stage', 'Smoking', 'Response', 'Sample Type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3462"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['RNAex'].Gene = dd['RNAex'].Gene.str.upper()\n",
    "RNAex = dd['RNAex'].to_pandas_df()\n",
    "RNAex = RNAex.loc[RNAex.Gene.isin(genes.Symbol)]\n",
    "RNAex.sort_values(by='Gene', inplace=True)\n",
    "RNAex.Start = RNAex.Start.astype(str)\n",
    "RNAex.Stop = RNAex.Stop.astype(str)\n",
    "RNAex['rnaID'] = RNAex[['Gene', 'Chr', 'Start', 'Stop', 'Strand']].apply(lambda x: \"_\".join(x), axis=1)\n",
    "rnamap = RNAex[['rnaID', 'Gene', 'Chr', 'Start', 'Stop', 'Strand']]\n",
    "rnamap['rnaID'].reset_index(drop=True, inplace=True)\n",
    "RNAex.drop(['Gene', 'Chr', 'Start', 'Stop', 'Strand'], axis=1, inplace=True)\n",
    "RNAex = RNAex.set_index('rnaID').transpose()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_var_list = RNAex.loc[:, rnamap['rnaID']].columns[RNAex.loc[:, rnamap['rnaID']].var()<=min_var_init].tolist()\n",
    "rnaID_include = list(set(rnamap.rnaID) - set(low_var_list))\n",
    "cols = rnaID_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['mutation'].Gene = dd['mutation'].Gene.str.upper()\n",
    "mutation = dd['mutation'].to_pandas_df()\n",
    "mutation = mutation.loc[mutation.Gene.isin(genes.Symbol)]\n",
    "mutation.set_index('Sample_ID', inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['methylation'].Gene = dd['methylation'].Gene.str.upper()\n",
    "methylation = dd['methylation'].to_pandas_df()\n",
    "methylation = methylation.loc[methylation.Gene.isin(genes.Symbol)]\n",
    "probemap = methylation[['probeID', 'Chr', 'Start', 'Stop', 'Strand', 'Gene', 'Relation_CpG_Island']]\n",
    "probemap.reset_index(drop=True, inplace=True)\n",
    "methylation.drop(['Chr', 'Start', 'Stop', 'Strand', 'Gene', 'Relation_CpG_Island'], axis=1, inplace=True)\n",
    "methylation = methylation.set_index('probeID').transpose()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methylation_meta = dd['methylation_meta'].to_pandas_df()\n",
    "methylation_meta = methylation_meta.loc[methylation_meta.IlmnID.isin(methylation.columns)]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['CNV'].Gene = dd['CNV'].Gene.str.upper()\n",
    "CNV = dd['CNV'].to_pandas_df()\n",
    "CNV = CNV.loc[CNV.Gene.isin(genes.Symbol)]\n",
    "CNV.Start = CNV.Start.astype(str)\n",
    "CNV.Stop = CNV.Stop.astype(str)\n",
    "CNV.Start.fillna(\"nan\", inplace=True)\n",
    "CNV.Stop.fillna(\"nan\", inplace=True)\n",
    "CNV.Strand.fillna(\"nan\", inplace=True)\n",
    "CNV.Chr.fillna(\"nan\", inplace=True)\n",
    "CNV['cnvID'] = CNV[['Gene', 'Chr', 'Start', 'Stop', 'Strand']].apply(lambda x: \"_\".join(x), axis=1)\n",
    "cnvmap = CNV[['cnvID', 'Gene', 'Chr', 'Start', 'Stop', 'Strand']]\n",
    "cnvmap.reset_index(drop=True, inplace=True)\n",
    "CNV.drop(['Gene', 'Chr', 'Start', 'Stop', 'Strand'], axis=1, inplace=True)\n",
    "CNV = CNV.set_index('cnvID').transpose()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with with multicollinearity\n",
    "# 1. replace collinear cliques\n",
    "# 2. only keep exemplar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc = hlp.association_ruler(num_bins=7, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating item/order array\n"
     ]
    }
   ],
   "source": [
    "assoc_pairs = assoc.association_rules(RNAex, min_support=0.25, pre_ordered=False, \n",
    "                                      sample_column=None, debug=True) \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 class problem: LA+primary tumor, LSCC+primary tumor, normal tissue\n",
    "def make_target(x):\n",
    "    if (x[0]=='Lung Squamous Cell Carcinoma') and (x[1]=='Primary Tumor'):\n",
    "        return 'LSCC-tumor'\n",
    "    elif (x[0]=='Lung Adenocarcinoma') and (x[1]=='Primary Tumor'):\n",
    "        return 'LA-tumor'\n",
    "    elif (x[1]=='Solid Tissue Normal'):\n",
    "        return 'normal-tissue'\n",
    "        \n",
    "RNAex['y'] = RNAex[['Diagnosis', 'Sample Type']].apply(lambda x: make_target(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short exploratory\n",
    "# 1. dimension reduction\n",
    "# 2. plot the differential targets\n",
    "if viz_reduced:\n",
    "    red = 'UMAP'\n",
    "    reduceR = UMAP(n_components=3, random_state=323, n_neighbors=50) #UMAP(n_components=3)\n",
    "    reduceR.fit(RNAex.loc[:, rnaID_include])\n",
    "\n",
    "\n",
    "    fig = go.Figure(layout={'title': 'RNA expression:'+red+' components'})\n",
    "    # Configure the trace.\n",
    "    reduceD = reduceR.transform(RNAex.loc[(RNAex['Diagnosis']=='Lung Adenocarcinoma') & \n",
    "                                          (RNAex['Sample Type']=='Primary Tumor'), rnaID_include])\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0], \n",
    "        y=reduceD[:,1],\n",
    "        z=reduceD[:,2], \n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='Lung Adenocarcinoma, primary tumor'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    reduceD = reduceR.transform(RNAex.loc[(RNAex['Diagnosis']=='Lung Squamous Cell Carcinoma') & \n",
    "                                          (RNAex['Sample Type']=='Primary Tumor'), rnaID_include])\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0], \n",
    "        y=reduceD[:,1],\n",
    "        z=reduceD[:,2], \n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='Lung Squamous Cell Carcinoma, primary tumor'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    reduceD = reduceR.transform(RNAex.loc[(RNAex['Sample Type']=='Solid Tissue Normal'), rnaID_include])\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0], \n",
    "        y=reduceD[:,1], \n",
    "        z=reduceD[:,2], \n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='normal tissue'\n",
    "    )\n",
    "    )\n",
    "\n",
    "\n",
    "    # Configure the layout.\n",
    "    layout = go.Layout(\n",
    "        margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    "    )\n",
    "    plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short exploratory\n",
    "# 1. dimension reduction\n",
    "# 2. plot the differential targets\n",
    "if viz_reduced:\n",
    "    red = 'TSNE'\n",
    "    reduceR = TSNE(n_components=3, random_state=123, perplexity=40) #UMAP(n_components=3)\n",
    "    #reduceR = PCA(n_components=3)\n",
    "    reduceDat = pd.DataFrame(data=reduceR.fit_transform(RNAex.loc[:, rnaID_include]),\n",
    "                           columns=['red1', 'red2', 'red3'],\n",
    "                           index=RNAex.index)\n",
    "\n",
    "    fig = go.Figure(layout={'title': 'RNA expression:'+red+' components'})\n",
    "    # Configure the trace.\n",
    "    reduceD = reduceDat.loc[(RNAex['Diagnosis']=='Lung Adenocarcinoma') & \n",
    "                          (RNAex['Sample Type']=='Primary Tumor'), :].values\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0],  \n",
    "        y=reduceD[:,1],  \n",
    "        z=reduceD[:,2],  \n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='Lung Adenocarcinoma, primary tumor'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    reduceD = reduceDat.loc[(RNAex['Diagnosis']=='Lung Squamous Cell Carcinoma') & \n",
    "                            (RNAex['Sample Type']=='Primary Tumor'), :].values\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0],  \n",
    "        y=reduceD[:,1],  \n",
    "        z=reduceD[:,2], \n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='Lung Squamous Cell Carcinoma, primary tumor'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    reduceD = reduceDat.loc[(RNAex['Sample Type']=='Solid Tissue Normal'), :].values\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0], \n",
    "        y=reduceD[:,1],  \n",
    "        z=reduceD[:,2],  \n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='normal tissue'\n",
    "    )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Configure the layout.\n",
    "    layout = go.Layout(\n",
    "        margin={'l': 0, 'r': 0, 'b': 0}\n",
    "    )\n",
    "    plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UMAP and t-SNE visualisation suggest that the cancer types are **clearly** separable using RNA expression data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ugtm import eGTM,eGTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtm = eGTM(m=4).fit(RNAex.loc[RNAex.y!='normal-tissue', rnaID_include])\n",
    "gtm_means = gtm.transform(RNAex.loc[RNAex.y!='normal-tissue', rnaID_include], model=\"means\")\n",
    "gtm_modes = gtm.transform(RNAex.loc[RNAex.y!='normal-tissue', rnaID_include], model=\"modes\")\n",
    "\n",
    "dgtm_modes = pd.DataFrame(gtm_modes, columns=[\"x1\", \"x2\"], index=RNAex.loc[RNAex.y!='normal-tissue'].index)\n",
    "dgtm_modes[\"label\"] = RNAex.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4b3ab95780>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.scatterplot(data=dgtm_modes.loc[dgtm_modes.label.isin(['LA-tumor','LSCC-tumor'])], \n",
    "                x='x1', y='x2', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmother = RNAex.dropna(subset=['y'])\n",
    "X = Xmother.loc[:, rnaID_include].values\n",
    "Y = Xmother['y'].values # pd.Categorical(RNAex['y'])\n",
    "Y = LabelEncoder().fit(Y).transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out non-informative features\n",
    "\n",
    "Ingredients:\n",
    "* KS-score: ```ks_scores(X,y)```.\n",
    "* Wasserstein distance: ```wass1_scores(X,y)```.\n",
    "* quantile entropy: N-quantiles, average target value per quantile, with/without threshold\n",
    "\n",
    "#### Entropy of pdf\n",
    "\n",
    "To remove low-variant features is actually to remove low entropy features. As the idea of low variance is completely dependent on the scaling of the feature low entropy will give a \n",
    "more robust feature filter. For continuous variables we have to use *differential entropy*, described as; \n",
    "$$H = - \\int \\rho(x) \\ln \\rho(x) dx $$\n",
    "\n",
    "The best we can do is to approximate this using the histograms of the features;\n",
    "\n",
    "$$H \\approx - \\sum \\rho(b_i) \\ln \\rho(b_i) \\Delta x_i$$\n",
    "\n",
    "where $b_i$ represent the bins. To bound it between zero and one we should write\n",
    "\n",
    "$$H \\approx - \\frac{\\sum \\rho(b_i) \\ln \\rho(b_i) \\Delta x_i}{\\sum \\Delta x_i}$$\n",
    "\n",
    "which is implemented in ```diff_entropy_scores(X)```.\n",
    "\n",
    "Note that the correct differential entropy is actually written as \n",
    "$$H = - \\int \\rho(x) \\ln \\frac{\\rho(x)}{m(x)} dx $$\n",
    "\n",
    "where $m(x)$ is the invariant measure.\n",
    "\n",
    "\n",
    "#### Entropy of ordered target vector\n",
    "If we order the target vector by the continuous values of the feature vector and subsequently\n",
    "reward successive target values and penalize differing target values we get the following ansatz for the series entropy\n",
    "$$\\frac{1}{2^{N+1-2C}} \\prod^N_{i=1} \\delta_{i,i-1}\\, 2^{\\delta_{i,i-1}} + (1-\\delta_{i,i-1})\\,2^{\\delta_{i,i-1}-1}$$\n",
    "\n",
    "where $\\delta_{i,j}=0$ if $y_i\\neq y_j$ and $\\delta_{i,j}=1$ if $y_i= y_j$.\n",
    "\n",
    "If we do not penalize disorder and only reward order we get the following\n",
    "$$\\frac{1}{2^{N+1-2C}} \\prod^N_{i=1}  2^{\\delta_{i,i-1}}= 2^{\\left[\\sum \\delta_{i,i-1}\\right]-(N+1-2C)}$$.\n",
    "\n",
    "Which is intractable due to the $2^{-(N+1-2C)}$ term an approximation is given by\n",
    "$$\\frac{1}{N+1-2C}\\sum^N_{i=1} \\delta_{i,i-1}\\label{eq1}\\tag{1}$$\n",
    "with $\\delta_{i,j}=-1$ if $y_i\\neq y_j$. Equation \\ref{eq1} is implemented in the function\n",
    "```seq_entropy_scores(X,y)```.\n",
    "\n",
    "The benefit of this approach is that there is no dependency on monotonous/linear relationships between the \n",
    "feature values and the target value. A down side is that there is no relationship with the actual feature value, other than the order.\n",
    "\n",
    "One way to obtain information about larger sequence lengths without using recursion is the application of binning. Suppose we have a binomial problem with $Q$ bins we get the following recursion ansatz\n",
    "$$\\prod^Q_{i=1} \\frac{1}{\\Delta c_{max}}\\left\\vert \\overline{c}_i-c_r\\right\\vert\\label{eq2}\\tag{2}$$\n",
    "where $c_r$ is the a priori class ratio and $\\overline{c}_i$ is the class ratio for bin $i$, where\n",
    "$\\Delta c_{max}=\\max{\\left(1-c_r,c_r\\right)}$. We might apply a sliding window/rolling bin to avoid discontinuities. The downside of \\ref{eq2} is that it is exactly zero if only one bin has exactly the same target mean as the overall target vector. An easy fix is to change the product to a sum\n",
    "$$\\frac{1}{Q \\, \\Delta c_{max}}\\sum^Q_{i=1}\\left\\vert \\overline{c}_i-c_r\\right\\vert\\label{eq3}\\tag{3}$$\n",
    "\n",
    "Equations  \\ref{eq2} and \\ref{eq3} are implemented in ```qseq_entropy_scores(X,y)``` with the \n",
    "function parameter ```q_type``` is ```prod```and ```sum```respectively.\n",
    "\n",
    "To improve we might want to involve the actual probability that a sequence of target values belongs to \n",
    "a group of a certain length $g$. This involves the binomial distribution, where\n",
    "\n",
    "$$Pr(X=k) = \\left(\\begin{array}{c} n \\\\ k \\end{array}\\right) p^k(1-p)^{n-k}$$\n",
    "\n",
    "where $$\\left(\\begin{array}{c} n \\\\ k \\end{array}\\right) = \\frac{n!}{k!(n-k)!},$$\n",
    "\n",
    "this represents the probability that we have a set of $k$ successes (value $1$ in our case) in $n$ trials, given some prior probability $p$. However, this description is not tractable due to the huge requirement on the number accuracy. Rather, we take samples from the binomial distribution to approximate this number, with ```sum(np.random.binomial(k, p, num_samples)==1)/num_samples```.\n",
    "#### Entropy change of quantiles \n",
    "\n",
    "We extract $N$ quantiles for the parts of the feature vector associated with the different target values, \n",
    "this gives $C$ vectors of size $N$ representing the different quantiles. For each quantile we can extract descriptive information. Firstly we are interesting in the degree entropy change between the quantiles, we express this in \n",
    "* Kullback-Leibler divergence\n",
    "* Shannon entropy change\n",
    "* Cross-entropy\n",
    "\n",
    "This functionality is implemented in the function ```ec_scores(X,y, num_bins=10)``` with the function parameter ```ent_type``` being ```kl```, ```shannon``` or ```cross```.\n",
    "\n",
    "\n",
    "#### Entropy of ordered target vector with different sequence lengths\n",
    "\n",
    "We order the target array by the feature vector values and subsequently extract the occurrences\n",
    "of varying sequence lengths. We attribute a different probability to each sequence length and we combine these in a combined unprobability. A high unprobability implies a high non-randomness in the target vector ordering and vice versa. \n",
    "\n",
    "We use a convolution operation to approximately count the number of target sequences, we perform\n",
    "a weighted sum of these counts, where the weights are given by the probability mass function of the binomial distribution. \n",
    "\n",
    "\n",
    "Implemented in ```seqX_entropy_scores(X,Y)```.\n",
    "\n",
    "#### Cumulative distribution differences \n",
    "\n",
    "EMD, CvM\n",
    "\n",
    "\n",
    "### Model based\n",
    "\n",
    "Accuracy scores relative to the base accuracy by just applying a quantile split.\n",
    "\n",
    "$$D_{qmod} = \\frac{score_{k}}{c_r}$$\n",
    "\n",
    "Implemented in ```medacc_scores(X,y)```.\n",
    "\n",
    "### Variance based\n",
    "\n",
    "Change in variance relative to overall variance. We define the variance score as\n",
    "$$D_{var} = \\Delta \\sigma^2/\\overline{\\sigma}$$\n",
    "where the $\\Delta$ represents the difference over the two class vectors.\n",
    "\n",
    "Implemented in ```var_dist(X,y)```.\n",
    "\n",
    "### Delta of quantiles\n",
    "\n",
    "Change of quantile values relative to standard deviation.\n",
    "\n",
    "$$D_{quantile}= \\frac{\\Delta q_k}{\\min{\\left(\\sigma_0, \\sigma_1\\right)}}$$\n",
    "\n",
    "Implemented in ```q_dists(X,y,q=0.5)```.\n",
    "\n",
    "### Chi2 of quantile binned target vectors\n",
    "\n",
    "Implemented in ```chi2_scores(X,y, bins)```.\n",
    "\n",
    "\n",
    "### Probability mass of non-overlapping distribution functions (not implemented)\n",
    "\n",
    "$$m = \\int^{x_{max_2}}_{x_{max_1}} \\Delta\\rho(x)dx + \\int^{x_{min_2}}_{x_{min_1}} \\Delta\\rho(x)dx$$. \n",
    "\n",
    "\n",
    "### Probability of exceedence\n",
    "\n",
    "$$\n",
    "\\rho_{L,R} \\approx  \\frac{1}{2}+ \\frac{\\left(\\int^\\mathbf{x} \\rho(x)dx, \\rho(x)\\right)_L - \\left(\\int^\\mathbf{x} \\rho(x)dx, \\rho(x)\\right)_R}{\\frac{1}{2}\\sum\\left(\\mathbf{x}_L + \\mathbf{x}_R\\right)}\n",
    "$$\n",
    "\n",
    "Implemented in ```prob_exceed_scores(X,y)```.\n",
    "\n",
    "\n",
    "### Cross entropy of feature-sorted and unsorted target vectors\n",
    "\n",
    "Implemented in ```ecs_scores(X,y, num_bins, ent_type, num_sample_rounds)```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'set' and 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b4853b1fabaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhigh_var_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_var_quantile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrnaID_include\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnaID_include\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_var_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X has shape {}, {} features have sufficient variance\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnaID_include\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'set' and 'set'"
     ]
    }
   ],
   "source": [
    "if feature_combiner:\n",
    "    X = pairwise_bulk_expansion(x=X, cols_l=rnaID_include, fun=combiner_function, \n",
    "                                index_c=Xmother.index.tolist())\n",
    "    \n",
    "    high_var_list = X.columns[X.var()>X.var().quantile(min_var_quantile)].tolist()\n",
    "    rnaID_include = list(set(rnaID_include).union(set(high_var_list)))\n",
    "    print(\"X has shape {}, {} features have sufficient variance\".format(X.shape, len(rnaID_include)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is now transformed\n",
      "Basic outlier removal has been performed\n"
     ]
    }
   ],
   "source": [
    "if x_transform is not None:\n",
    "    X = pd.DataFrame(data=x_transform.fit_transform(X.loc[:,rnaID_include]), \n",
    "                     columns=rnaID_include, index=Xmother.index)\n",
    "    print(\"X is now transformed\")\n",
    "if outlier_replacer:\n",
    "    X = pd.DataFrame(data=hlp.featurewise_outlier_replacer(X.loc[:,rnaID_include], q=(0.01, 0.99)),\n",
    "                     columns=rnaID_include, index=Xmother.index) \n",
    "    print(\"Basic outlier removal has been performed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:Processing diffentropy, variance scores, spca/pca/fa/ica/nmf/dl importances...\n",
      "INFO:MainThread:root:diff entropy..\n",
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"_diff_entropy\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of Function(<function histogram at 0x7f4d04110a60>) with argument(s) of type(s): (array(float64, 1d, A), bins=int64, density=bool)\n",
      " * parameterized\n",
      "\u001b[1mIn definition 0:\u001b[0m\n",
      "\u001b[1m    TypeError: np_histogram() got an unexpected keyword argument 'density'\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typing/templates.py:539\n",
      "\u001b[1mIn definition 1:\u001b[0m\n",
      "\u001b[1m    TypeError: np_histogram() got an unexpected keyword argument 'density'\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typing/templates.py:539\n",
      "\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: Function(<function histogram at 0x7f4d04110a60>)\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at /media/koekiemonster/home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py (371)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 371:\u001b[0m\n",
      "\u001b[1mdef q_acc_scores(X,y, q=0.5):\n",
      "    <source elided>\n",
      "        X = X.values\n",
      "\u001b[1m    scores = np.zeros((X.shape[1],))\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "INFO:MainThread:numba.transforms:finding looplift candidates\n",
      "\u001b[1mFunction \"_diff_entropy\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 369:\u001b[0m\n",
      "\u001b[1mdef q_acc_scores(X,y, q=0.5):\n",
      "    <source elided>\n",
      "    cr = np.mean(y)\n",
      "\u001b[1m    if \"DataFrame\" in str(type(X)):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 369:\u001b[0m\n",
      "\u001b[1mdef q_acc_scores(X,y, q=0.5):\n",
      "    <source elided>\n",
      "    cr = np.mean(y)\n",
      "\u001b[1m    if \"DataFrame\" in str(type(X)):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "INFO:MainThread:root:variance..\n",
      "INFO:MainThread:root:pca_imp..\n",
      "INFO:MainThread:root:spca_imp..\n",
      "INFO:MainThread:root:fa_imp..\n",
      "INFO:MainThread:root:ica_imp..\n",
      "FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "INFO:MainThread:root:nmf_imp..\n",
      "INFO:MainThread:root:dl_imp..\n",
      "INFO:MainThread:root:minf..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing minf/fscore/wass1/wass2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:fscore..\n",
      "INFO:MainThread:root:Wass1..\n",
      "INFO:MainThread:root:Wass2..\n",
      "INFO:MainThread:root:spearman..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spearman and ks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:ks..\n",
      "INFO:MainThread:root:seqentropy..\n",
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"seq_entropy\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of BoundFunction(array.astype for array(bool, 1d, C)) with parameters (Function(<class 'float'>))\n",
      " * parameterized\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: BoundFunction(array.astype for array(bool, 1d, C))\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at /media/koekiemonster/home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py (569)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 569:\u001b[0m\n",
      "\u001b[1m\n",
      "\u001b[1mdef qseq_entropy(x, bins=20, q_type='sum'):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "INFO:MainThread:numba.transforms:finding looplift candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing seq entropies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mFunction \"seq_entropy\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 569:\u001b[0m\n",
      "\u001b[1m\n",
      "\u001b[1mdef qseq_entropy(x, bins=20, q_type='sum'):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 569:\u001b[0m\n",
      "\u001b[1m\n",
      "\u001b[1mdef qseq_entropy(x, bins=20, q_type='sum'):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "INFO:MainThread:root:qseqentropy_prod..\n",
      "INFO:MainThread:root:qseqentropy_sum..\n",
      "INFO:MainThread:root:seqentropyX..\n",
      "INFO:MainThread:root:cdf_1..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CDF scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"_cdf\" failed type inference due to: \u001b[1m\u001b[1mCannot unify array(float64, 2d, C) and array(float64, 1d, C) for '_res', defined at /media/koekiemonster/home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py (428)\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 428:\u001b[0m\n",
      "\u001b[1m@jit\n",
      "\u001b[1mdef _cdfcoeff(x, bin_size=5):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of assignment at /media/koekiemonster/home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py (431)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 431:\u001b[0m\n",
      "\u001b[1mdef _cdfcoeff(x, bin_size=5):\n",
      "    <source elided>\n",
      "    lt = _cdf(x, bin_size=bin_size)\n",
      "\u001b[1m    xmm = x.max() - x.min()\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "INFO:MainThread:numba.transforms:finding looplift candidates\n",
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"_cdf\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 429:\u001b[0m\n",
      "\u001b[1mdef _cdfcoeff(x, bin_size=5):\n",
      "\u001b[1m    # better to take out the creation of the cdf and split the function (_cdf, _cdfcoeff)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[1mFunction \"_cdf\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 426:\u001b[0m\n",
      "\u001b[1mdef ecdf(x):\n",
      "    <source elided>\n",
      "# np.asarray([[i/c, np.median(x[(i-bin_size):i])] for i in range(bin_size, c) if i%bin_size==0])\n",
      "\u001b[1m\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 426:\u001b[0m\n",
      "\u001b[1mdef ecdf(x):\n",
      "    <source elided>\n",
      "# np.asarray([[i/c, np.median(x[(i-bin_size):i])] for i in range(bin_size, c) if i%bin_size==0])\n",
      "\u001b[1m\n",
      "\u001b[0m \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"_cdf\" failed type inference due to: \u001b[1m\u001b[1m\u001b[1mInvalid use of Function(<function append at 0x7f4d0411d7b8>) with argument(s) of type(s): (array(float64, 2d, C), list(array(float64, 1d, C)), axis=Literal[int](0))\n",
      " * parameterized\n",
      "\u001b[1mIn definition 0:\u001b[0m\n",
      "\u001b[1m    TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mInvalid use of Function(<function concatenate at 0x7f4d0422d400>) with argument(s) of type(s): (Tuple(array(float64, 2d, C), list(array(float64, 1d, C))), axis=Literal[int](0))\n",
      " * parameterized\n",
      "\u001b[1mIn definition 0:\u001b[0m\n",
      "\u001b[1m    TypeError: np.concatenate(): expecting a non-empty tuple of arrays, got Tuple(array(float64, 2d, C), list(array(float64, 1d, C)))\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typing/npydecl.py:793\n",
      "\u001b[1mIn definition 1:\u001b[0m\n",
      "\u001b[1m    TypeError: np.concatenate(): expecting a non-empty tuple of arrays, got Tuple(array(float64, 2d, C), list(array(float64, 1d, C)))\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typing/npydecl.py:793\n",
      "\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: Function(<function concatenate at 0x7f4d0422d400>)\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at /usr/local/lib/python3.6/dist-packages/numba/targets/arrayobj.py (1718)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../../../usr/local/lib/python3.6/dist-packages/numba/targets/arrayobj.py\", line 1718:\u001b[0m\n",
      "\u001b[1m        def impl(arr, values, axis=None):\n",
      "\u001b[1m            return np.concatenate((arr, values), axis=axis)\n",
      "\u001b[0m            \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typeinfer.py:985\n",
      "\u001b[1mIn definition 1:\u001b[0m\n",
      "\u001b[1m    TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mInvalid use of Function(<function concatenate at 0x7f4d0422d400>) with argument(s) of type(s): (Tuple(array(float64, 2d, C), list(array(float64, 1d, C))), axis=int64)\n",
      " * parameterized\n",
      "\u001b[1mIn definition 0:\u001b[0m\n",
      "\u001b[1m    TypeError: np.concatenate(): expecting a non-empty tuple of arrays, got Tuple(array(float64, 2d, C), list(array(float64, 1d, C)))\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typing/npydecl.py:793\n",
      "\u001b[1mIn definition 1:\u001b[0m\n",
      "\u001b[1m    TypeError: np.concatenate(): expecting a non-empty tuple of arrays, got Tuple(array(float64, 2d, C), list(array(float64, 1d, C)))\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typing/npydecl.py:793\n",
      "\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: Function(<function concatenate at 0x7f4d0422d400>)\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at /usr/local/lib/python3.6/dist-packages/numba/targets/arrayobj.py (1718)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../../../usr/local/lib/python3.6/dist-packages/numba/targets/arrayobj.py\", line 1718:\u001b[0m\n",
      "\u001b[1m        def impl(arr, values, axis=None):\n",
      "\u001b[1m            return np.concatenate((arr, values), axis=axis)\n",
      "\u001b[0m            \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "    raised from /usr/local/lib/python3.6/dist-packages/numba/typeinfer.py:985\n",
      "\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: resolving callee type: Function(<function append at 0x7f4d0411d7b8>)\u001b[0m\n",
      "\u001b[0m\u001b[1m[2] During: typing of call at /media/koekiemonster/home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py (432)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 432:\u001b[0m\n",
      "\u001b[1mdef _cdfcoeff(x, bin_size=5):\n",
      "    <source elided>\n",
      "    xmm = x.max() - x.min()\n",
      "\u001b[1m    diff1 = np.diff(lt[:, 1]) * lt[:-1, 0]\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mFunction \"_cdf\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 429:\u001b[0m\n",
      "\u001b[1mdef _cdfcoeff(x, bin_size=5):\n",
      "\u001b[1m    # better to take out the creation of the cdf and split the function (_cdf, _cdfcoeff)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 429:\u001b[0m\n",
      "\u001b[1mdef _cdfcoeff(x, bin_size=5):\n",
      "\u001b[1m    # better to take out the creation of the cdf and split the function (_cdf, _cdfcoeff)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "INFO:MainThread:root:cdf_2..\n",
      "INFO:MainThread:root:cdf_3..\n",
      "INFO:MainThread:root:cdf_4..\n",
      "INFO:MainThread:root:cdf_5..\n",
      "INFO:MainThread:root:cdf_6..\n",
      "INFO:MainThread:root:med_dist..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing q distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:q25_dist..\n",
      "INFO:MainThread:root:q75_dist..\n",
      "INFO:MainThread:root:var_dist..\n",
      "INFO:MainThread:root:q5_acc..\n",
      "INFO:MainThread:root:q75_acc..\n",
      "INFO:MainThread:root:prob_exc..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting conditional probability of exceedence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:KL..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cross entropies over class-seperated features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:Shan..\n",
      "INFO:MainThread:root:Cross..\n",
      "INFO:MainThread:root:KL_sort..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cross entropies over random selects of feature-sorted and non-feature-sorted target vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:MainThread:root:Shan_sort..\n",
      "INFO:MainThread:root:Cross_sort..\n",
      "INFO:MainThread:root:Chi2..\n",
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"chi2_score\" failed type inference due to: \u001b[1mUntyped global name 'chisquare':\u001b[0m \u001b[1m\u001b[1mcannot determine Numba type of <class 'function'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 422:\u001b[0m\n",
      "\u001b[1mdef ecdf(x):\n",
      "    <source elided>\n",
      "    x = np.sort(x)\n",
      "\u001b[1m    y = np.arange(1, n+1)/n\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "INFO:MainThread:numba.transforms:finding looplift candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Chi2 and Epps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"chi2_score\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 417:\u001b[0m\n",
      "\u001b[1mdef _cdf(x, bin_size=5):\n",
      "    <source elided>\n",
      "from numba import float32 as nbfloat32, float64 as nbfloat64, int32 as nbint32\n",
      "\u001b[1mfrom numba import typeof\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[1mFunction \"chi2_score\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 416:\u001b[0m\n",
      "\u001b[1mdef _cdf(x, bin_size=5):\n",
      "    <source elided>\n",
      "from numba.types import UniTuple, Tuple\n",
      "\u001b[1mfrom numba import float32 as nbfloat32, float64 as nbfloat64, int32 as nbint32\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"../../../../home/bramiozo/DEV/GIT/RexR/_hackathon2018/omic_helper_lib/omic_helpers/omic_helpers.py\", line 416:\u001b[0m\n",
      "\u001b[1mdef _cdf(x, bin_size=5):\n",
      "    <source elided>\n",
      "from numba.types import UniTuple, Tuple\n",
      "\u001b[1mfrom numba import float32 as nbfloat32, float64 as nbfloat64, int32 as nbint32\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "invalid value encountered in true_divide\n",
      "divide by zero encountered in true_divide\n",
      "INFO:MainThread:root:epps..\n",
      "divide by zero encountered in true_divide\n",
      "invalid value encountered in multiply\n",
      "invalid value encountered in cos\n",
      "invalid value encountered in sin\n",
      "invalid value encountered in multiply\n",
      "invalid value encountered in cos\n",
      "invalid value encountered in sin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449 features remain after feature selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    }
   ],
   "source": [
    "if pre_filter:\n",
    "    stat_dist_df = hlp.get_statdist_dataframe_binomial(X.loc[:,rnaID_include].values,Y, \n",
    "                                                       features=rnaID_include);\n",
    "    features = hlp._feature_selector(stat_dist_df, topN=25, overlap='union', score_list=None)\n",
    "    X = Xmother.loc[:, features]\n",
    "    rnaID_include = X.columns.tolist()\n",
    "    print(\"{} features remain after feature selection\".format(len(rnaID_include)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if x_reducer is not None:\n",
    "    x_reducer.fit(X)\n",
    "    ncomp = x_reducer.n_components_\n",
    "    X = pd.DataFrame(data=x_reducer.transform(X), columns=['pc_'+str(j) for j in range(0,ncomp)], \n",
    "                     index=Xmother.index)\n",
    "    print(\"Dimension reduction to {} components has been applied\".format(ncomp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9e0da16b0f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0maff_prop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffinityPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0maff_prop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maff_prop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mexemplars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maff_prop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_indices_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/_affinity_propagation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffinity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"precomputed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffinity_matrix_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "if rem_coll:\n",
    "    '''\n",
    "    try:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        corr = np.corrcoef(X.T) # 1 - cdist(X.T, X.T, metric='cosine')\n",
    "        corr_linkage = hierarchy.ward(corr)\n",
    "        dendro = hierarchy.dendrogram(corr_linkage, labels= rnamap['rnaID'].tolist(), ax=ax1,\n",
    "                                      leaf_rotation=90)\n",
    "        dendro_idx = np.arange(0, len(dendro['ivl']))\n",
    "\n",
    "        ax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\n",
    "        ax2.set_xticks(dendro_idx)\n",
    "        ax2.set_yticks(dendro_idx)\n",
    "        ax2.set_xticklabels(dendro['ivl'], rotation='vertical')\n",
    "        ax2.set_yticklabels(dendro['ivl'])\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Some problemo: \", e)\n",
    "    '''\n",
    "    \n",
    "    aff_prop = AffinityPropagation(preference=-1000)\n",
    "    aff_prop.fit(X.T)\n",
    "    clusters = aff_prop.predict(X.T)\n",
    "    exemplars = aff_prop.cluster_centers_indices_\n",
    "    X = X.iloc[:,exemplars]\n",
    "    col_arr = np.array(rnaID_include)\n",
    "    cols = col_arr[exemplars]\n",
    "    \n",
    "    print('Affinity propagation leads to {} exemplars:'.format(len(cols)))\n",
    "    clust_list = defaultdict(list)\n",
    "    for idx, _exemplar_id in enumerate(aff_prop.cluster_centers_indices_):\n",
    "        _exemplar_column = col_arr[_exemplar_id]\n",
    "        for jdx, _idx in enumerate(aff_prop.labels_):\n",
    "            if idx == _idx: \n",
    "                clust_list[_exemplar_column].append(col_arr[jdx])\n",
    "            \n",
    "else:\n",
    "    col_arr = X.columns.tolist()\n",
    "    cols= col_arr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1235, stratify=Y, train_size=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RF': RF(n_estimators=300), \n",
    "          'XGB': xgb(n_estimators=300), \n",
    "          'HB': HB(max_bins=200),\n",
    "          'ET': ET(n_estimators=600),\n",
    "          'LGBM': lgbm(n_estimators=300),\n",
    "          'LR': LR(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "          'SVM': SVC(C=0.8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting RF')\n",
    "models['RF'].fit(X_train, y_train)\n",
    "print('Fitting XGB')\n",
    "models['XGB'].fit(X_train, y_train)\n",
    "print('Fitting HB')\n",
    "models['HB'].fit(X_train, y_train)\n",
    "print('Fitting ET')\n",
    "models['ET'].fit(X_train, y_train)\n",
    "print('Fitting LGBM')\n",
    "models['LGBM'].fit(X_train, y_train)\n",
    "print('Fitting LR')\n",
    "models['LR'].fit(X_train, y_train)\n",
    "print('Fitting SVM')\n",
    "models['SVM'].fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm_result = dict()\n",
    "\n",
    "perm_result['RF'] = permutation_importance(models['RF'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['XGB'] = permutation_importance(models['XGB'], X_test, y_test, \n",
    "                                            n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['HB'] = permutation_importance(models['HB'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['ET'] = permutation_importance(models['ET'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['LGBM'] = permutation_importance(models['LGBM'], X_test, y_test,\n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['LR'] = permutation_importance(models['LR'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['SVM'] = permutation_importance(models['SVM'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.vstack([perm_result['RF'].importances_mean,\n",
    "           perm_result['XGB'].importances_mean,\n",
    "           perm_result['HB'].importances_mean,\n",
    "           perm_result['ET'].importances_mean,      \n",
    "           perm_result['LGBM'].importances_mean,\n",
    "           perm_result['LR'].importances_mean,\n",
    "           perm_result['SVM'].importances_mean]).transpose()\n",
    "perm_df = pd.DataFrame(data=tmp, index=cols, columns=['RF', 'XGB', 'HB', 'ET', 'LGBM', 'LR', 'SVM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df.max(axis=1).plot.kde(figsize=(10, 7))\n",
    "plt.title(\"Distribution of maximum permutation importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permutation analysis suggests that **none** of the features are important in permuting the prediction. This contradicts the clear separation of the classes in the embedded space. \n",
    "\n",
    "It could be that all the features are highly correlated with at least one other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.abs(perm_result['XGB'].importances).mean(axis=1).argsort()[-10:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(perm_result['XGB'].importances[sorted_idx].T,\n",
    "           vert=False, labels=np.array(cols)[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = TreeExplainer(models['XGB'])\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "summary_plot(shap_values, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16, 8))\n",
    "RNAex.loc[RNAex.y=='LA-tumor', 'TGFBR2_chr3_30606502_30694142_+'].plot.kde(label='LA', ax=ax[0])\n",
    "RNAex.loc[RNAex.y=='LSCC-tumor', 'TGFBR2_chr3_30606502_30694142_+'].plot.kde(label='LSCC', ax=ax[0])\n",
    "RNAex.loc[RNAex.y=='normal-tissue', 'TGFBR2_chr3_30606502_30694142_+'].plot.kde(label='normal-tissue', ax=ax[0])\n",
    "ax[0].legend()\n",
    "ax[0].set_title('TGFBR2 expression')\n",
    "\n",
    "RNAex.loc[RNAex.y=='LA-tumor', 'RORC_chr1_151806071_151831872_-'].plot.kde(label='LA', ax=ax[1])\n",
    "RNAex.loc[RNAex.y=='LSCC-tumor', 'RORC_chr1_151806071_151831872_-'].plot.kde(label='LSCC', ax=ax[1])\n",
    "RNAex.loc[RNAex.y=='normal-tissue', 'RORC_chr1_151806071_151831872_-'].plot.kde(label='normal-tissue', ax=ax[1])\n",
    "ax[1].legend()\n",
    "ax[1].set_title('RORC expression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _m in models.keys():\n",
    "    y_pred = models[_m].predict(X_test)\n",
    "    print(\"Model:{}, Accuracy:{}\".format(_m, balanced_accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining RNA with Methylation\n",
    "\n",
    "We add the methylation data, transformed to $\\beta$-values. We apply the following link-function for the expression values $\\alpha$;\n",
    "$$\\alpha_{new} = (1-\\beta)\\,\\alpha$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNAex Methylation\n",
    "probemap.dropna(subset=['Start'], inplace=True)\n",
    "probemap.Start = probemap.Start.astype(int).astype(str)\n",
    "probemap.Stop = probemap.Stop.astype(int).astype(str)\n",
    "probemap['geneid'] = probemap[['Gene', 'Chr', 'Start', 'Stop', 'Strand']].apply(lambda x: \"_\".join(x), axis=1)\n",
    "#RNAex.loc[:, rnaID_include]\n",
    "rnamap.set_index('rnaID', inplace=True)\n",
    "probemap.set_index('probeID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} unique probe id's\".format(probemap.geneid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methylation.replace([np.inf, -np.inf], np.nan)\n",
    "methylation_new = 1-methylation.loc[:, probemap.index].dropna(axis=1).T\n",
    "RNA_new  = RNAex.loc[:, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first filter out the samples that are not overlapping\n",
    "overlapping_samples = list(set(methylation_new.columns).intersection(set(RNA_new.columns)))\n",
    "methylation_new = methylation_new.loc[:, overlapping_samples]\n",
    "RNA_new = RNA_new.loc[:, overlapping_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now outer-join on the gene\n",
    "RNA_new = RNA_new.join(rnamap[['Gene']])\n",
    "methylation_new = methylation_new.join(probemap[['Gene', 'geneid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_index = methylation_new.index\n",
    "res = methylation_new.merge(RNA_new, left_on='Gene', right_on='Gene', \n",
    "                            suffixes=['_methyl', '_expression'], how='inner')\n",
    "res.set_index('geneid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mo = res.copy()\n",
    "for _col in overlapping_samples:\n",
    "    methyl_col = _col+\"_methyl\"\n",
    "    rnaexp_col = _col+\"_expression\"\n",
    "    res_mo[_col] = res_mo[methyl_col]*res_mo[rnaexp_col]\n",
    "res_mo.drop(res.columns.tolist(), axis=1, inplace=True)\n",
    "RNA_Meth = res_mo.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_Meth = RNA_Meth.loc[:, RNA_Meth.columns[RNA_Meth.var()!=0]]\n",
    "collist = RNA_Meth.columns.tolist()\n",
    "print(\"There are : {} duplicate column names, GO FIX\".format(len(collist) \n",
    "                                                             - len(set(collist))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what column names are duplicated\n",
    "coldict = defaultdict(int)\n",
    "multiple = []\n",
    "new_list = []\n",
    "for _col in collist:\n",
    "    coldict[_col] += 1\n",
    "    if coldict[_col]>1:\n",
    "        multiple.append(_col)\n",
    "        new_list.append(_col+\"_\"+str(coldict[_col]))\n",
    "    else:\n",
    "        new_list.append(_col)\n",
    "# add suffix to duplicate column names.\n",
    "multiple = list(set(multiple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_Meth.columns = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change types to float\n",
    "no_float = []\n",
    "for _col in tqdm(RNA_Meth.columns):\n",
    "    if RNA_Meth[_col].dtype==object:\n",
    "        try:\n",
    "            RNA_Meth[_col] = RNA_Meth[_col].astype(float)\n",
    "        except Exception as e:\n",
    "            no_float.append((_col, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_Meth = RNA_Meth.iloc[:, :12699]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_Meth_joined = RNA_Meth.join(meta[meta_cols])\n",
    "assert RNA_Meth.isna().sum().sum()==0, \"GO FIX NaN's!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_Meth_joined.shape, RNA_Meth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "Do we see a qualitative change with respect to the RNA only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnameth_cols = RNA_Meth.columns.tolist()\n",
    "RNA_Meth_joined.dropna(axis=1, inplace=True)\n",
    "RNA_Meth_joined['y'] = RNA_Meth_joined[['Diagnosis', 'Sample Type']].apply(lambda x: make_target(x), axis=1)\n",
    "rnameth_cols = list((set(RNA_Meth_joined.columns)-set(meta_cols)).intersection(set(rnameth_cols)))\n",
    "rnameth_cols = RNA_Meth_joined.loc[:, rnameth_cols].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short exploratory\n",
    "# 1. dimension reduction\n",
    "# 2. plot the differential targets\n",
    "\n",
    "if viz_reduced:\n",
    "    red = 'UMAP'\n",
    "    reduceR = UMAP(n_components=3, random_state=323, n_neighbors=50) #UMAP(n_components=3)\n",
    "    reduceR.fit(RNA_Meth_joined.loc[:, rnameth_cols])\n",
    "\n",
    "\n",
    "    fig = go.Figure(layout={'title': 'Methylation-RNA combined expression:'+red+' components'})\n",
    "    # Configure the trace.\n",
    "    reduceD = reduceR.transform(RNA_Meth_joined.loc[(RNA_Meth_joined['Diagnosis']=='Lung Adenocarcinoma') & \n",
    "                                          (RNA_Meth_joined['Sample Type']=='Primary Tumor'), rnameth_cols])\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0],  # <-- Put your data instead\n",
    "        y=reduceD[:,1],  # <-- Put your data instead\n",
    "        z=reduceD[:,2],  # <-- Put your data instead\n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='Lung Adenocarcinoma, primary tumor'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    reduceD = reduceR.transform(RNA_Meth_joined.loc[(RNA_Meth_joined['Diagnosis']=='Lung Squamous Cell Carcinoma') & \n",
    "                                          (RNA_Meth_joined['Sample Type']=='Primary Tumor'), rnameth_cols])\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0],  # <-- Put your data instead\n",
    "        y=reduceD[:,1],  # <-- Put your data instead\n",
    "        z=reduceD[:,2],  # <-- Put your data instead\n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='Lung Squamous Cell Carcinoma, primary tumor'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    reduceD = reduceR.transform(RNA_Meth_joined.loc[(RNA_Meth_joined['Sample Type']=='Solid Tissue Normal'), rnameth_cols])\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=reduceD[:,0],  # <-- Put your data instead\n",
    "        y=reduceD[:,1],  # <-- Put your data instead\n",
    "        z=reduceD[:,2],  # <-- Put your data instead\n",
    "        mode='markers',\n",
    "        marker={\n",
    "            'size': 3,\n",
    "            'opacity': 0.5,\n",
    "        },\n",
    "        name='normal tissue'\n",
    "    )\n",
    "    )\n",
    "\n",
    "    # Configure the layout.\n",
    "    layout = go.Layout(\n",
    "        margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    "    )\n",
    "    plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_momic_mother = RNA_Meth_joined.dropna(subset=['y'])\n",
    "X = X_momic_mother.loc[:, rnameth_cols].values\n",
    "Y = X_momic_mother['y'].values\n",
    "Y = LabelEncoder().fit(Y).transform(Y)\n",
    "print(X.shape, len(rnameth_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_filter:\n",
    "    stat_dist_multiomic_df = hlp.get_statdist_dataframe_binomial(X,Y, features=rnameth_cols)\n",
    "    gc.collect()\n",
    "    features = hlp._feature_selector(stat_dist_multiomic_df, topN=10, overlap='union')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA - Methylation classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by\n",
    "if pre_filter:\n",
    "    #include = stat_dist_df.sort_values(by='seqentropyX_wass1', ascending=False)[:50].index.tolist()\n",
    "    X = X_momic_mother.loc[:, features]\n",
    "    rnameth_cols = X.columns.tolist()\n",
    "    \n",
    "if outlier_replacer:\n",
    "    X = hlp.featurewise_outlier_replacer(X, q=(0.01, 0.99))\n",
    "    \n",
    "if x_transform is not None:\n",
    "    X = pd.DataFrame(data=x_transform.fit_transform(X), columns=rnameth_cols, index=X_momic_mother.index)\n",
    "\n",
    "if x_reducer is not None:\n",
    "    x_reducer.fit(X)\n",
    "    ncomp = x_reducer.n_components_\n",
    "    X = pd.DataFrame(data=x_reducer.transform(X), columns=['pc_'+str(j) for j in range(0,ncomp)], \n",
    "                     index=X_momic_mother.index)\n",
    "    \n",
    "if rem_coll:\n",
    "    '''\n",
    "    try:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        corr = np.corrcoef(X.T) # 1 - cdist(X.T, X.T, metric='cosine')\n",
    "        corr_linkage = hierarchy.ward(corr)\n",
    "        dendro = hierarchy.dendrogram(corr_linkage, labels= rnameth_cols, ax=ax1,\n",
    "                                      leaf_rotation=90)\n",
    "        dendro_idx = np.arange(0, len(dendro['ivl']))\n",
    "\n",
    "        ax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\n",
    "        ax2.set_xticks(dendro_idx)\n",
    "        ax2.set_yticks(dendro_idx)\n",
    "        ax2.set_xticklabels(dendro['ivl'], rotation='vertical')\n",
    "        ax2.set_yticklabels(dendro['ivl'])\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Some problemo: \", e)\n",
    "    '''\n",
    "    \n",
    "    aff_prop = AffinityPropagation(preference=-1000)\n",
    "    aff_prop.fit(X.T)\n",
    "    clusters = aff_prop.predict(X.T)\n",
    "    exemplars = aff_prop.cluster_centers_indices_\n",
    "    X = X.iloc[:,exemplars]\n",
    "    col_arr = np.array(rnameth_cols)\n",
    "    cols = col_arr[exemplars]\n",
    "    \n",
    "    print('Number of exemplars:', len(cols))\n",
    "    clust_list = defaultdict(list)\n",
    "    for idx, _exemplar_id in enumerate(aff_prop.cluster_centers_indices_):\n",
    "        _exemplar_column = col_arr[_exemplar_id]\n",
    "        for jdx, _idx in enumerate(aff_prop.labels_):\n",
    "            if idx == _idx: \n",
    "                clust_list[_exemplar_column].append(col_arr[jdx])\n",
    "            \n",
    "else:\n",
    "    col_arr = X.columns.tolist()\n",
    "    cols= col_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1235, stratify=Y, train_size=0.90)\n",
    "models = {'RF': RF(n_estimators=500), \n",
    "          'XGB': xgb(n_estimators=500), \n",
    "          'HB': HB(max_bins=200),\n",
    "          'ET': ET(n_estimators=1000),\n",
    "          'LGBM': lgbm(n_estimators=500),\n",
    "          'LR': LR(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "          'SVM': SVC(C=0.8)}\n",
    "\n",
    "print('Fitting RF')\n",
    "models['RF'].fit(X_train, y_train)\n",
    "print('Fitting XGB')\n",
    "models['XGB'].fit(X_train, y_train)\n",
    "print('Fitting HB')\n",
    "models['HB'].fit(X_train, y_train)\n",
    "print('Fitting ET')\n",
    "models['ET'].fit(X_train, y_train)\n",
    "print('Fitting LGBM')\n",
    "models['LGBM'].fit(X_train, y_train)\n",
    "print('Fitting LR')\n",
    "models['LR'].fit(X_train, y_train)\n",
    "print('Fitting SVM')\n",
    "models['SVM'].fit(X_train, y_train)\n",
    "\n",
    "\n",
    "perm_result = dict()\n",
    "\n",
    "perm_result['RF'] = permutation_importance(models['RF'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['XGB'] = permutation_importance(models['XGB'], X_test, y_test, \n",
    "                                            n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['HB'] = permutation_importance(models['HB'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['ET'] = permutation_importance(models['ET'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['LGBM'] = permutation_importance(models['LGBM'], X_test, y_test,\n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['LR'] = permutation_importance(models['LR'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "\n",
    "perm_result['SVM'] = permutation_importance(models['SVM'], X_test, y_test, \n",
    "                                           n_repeats=10, random_state=422, n_jobs=8)\n",
    "gc.collect()\n",
    "\n",
    "tmp = np.vstack([perm_result['RF'].importances_mean,\n",
    "           perm_result['XGB'].importances_mean,\n",
    "           perm_result['HB'].importances_mean,\n",
    "           perm_result['ET'].importances_mean,      \n",
    "           perm_result['LGBM'].importances_mean,\n",
    "           perm_result['LR'].importances_mean,\n",
    "           perm_result['SVM'].importances_mean]).transpose()\n",
    "perm_df = pd.DataFrame(data=tmp, index=cols, columns=['RF', 'XGB', 'HB', 'ET', 'LGBM', 'LR', 'SVM'])\n",
    "\n",
    "perm_df.max(axis=1).plot.kde(figsize=(10, 7))\n",
    "plt.title(\"Distribution of maximum permutation importances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.abs(perm_result['XGB'].importances).mean(axis=1).argsort()[-10:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(perm_result['XGB'].importances[sorted_idx].T,\n",
    "           vert=False, labels=np.array(cols)[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = TreeExplainer(models['XGB'])\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "summary_plot(shap_values, X_train, max_display=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _m in models.keys():\n",
    "    y_pred = models[_m].predict(X_test)\n",
    "    print(\"Model:{}, Accuracy:{}\".format(_m, balanced_accuracy_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (multi_omics)",
   "language": "python",
   "name": "multi_omics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
