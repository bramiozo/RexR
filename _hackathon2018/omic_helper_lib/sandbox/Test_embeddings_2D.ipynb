{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings to test\n",
    "\n",
    "* UMAP\n",
    "* t-SNE\n",
    "* Parametric UMAP (part of UMAP)\n",
    "* DenseMap (part of UMAP)\n",
    "* [PacMap](https://github.com/YingfanWang/PaCMAP)\n",
    "* [TriMap](https://github.com/eamid/trimap)\n",
    "* PCA\n",
    "* Laplacian eigenmaps\n",
    "* MDS\n",
    "* Isomap\n",
    "* [MDE](https://github.com/cvxgrp/pymde)\n",
    "* [PHATE](https://github.com/KrishnaswamyLab/PHATE)\n",
    "* ForceAtlas2\n",
    "* dbMAP\n",
    "\n",
    "\n",
    "# Experiments\n",
    "\n",
    "* distance/distance-rank preservation with varying ```n_neighbors```, ```n_components``` and ```min_dist```, measured with Pearson's corr.\n",
    "* hierarchical embedding: original -> 1000d -> 100d -> 2d\n",
    "* negative test: does it magically create clusters? Test using a high dimensional Gaussian\n",
    "\n",
    "\n",
    "Metrics:\n",
    "* Spearman rank correlation between samples\n",
    "* Pearson correlation of distances\n",
    "* Distance correlation of distances\n",
    "* Average Jaccard distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport omic_helpers\n",
    "%matplotlib inline\n",
    "\n",
    "from omic_helpers import graph_clustering\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, RobustScaler, MinMaxScaler, FunctionTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare, chi2_contingency, pearsonr\n",
    "from scipy.stats import kendalltau,spearmanr, weightedtau, theilslopes, wilcoxon, ttest_rel\n",
    "from scipy.spatial import distance\n",
    "import dcor\n",
    "\n",
    "import umap\n",
    "import pacmap\n",
    "import trimap\n",
    "import pymde\n",
    "import dbmap\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA, NMF, FactorAnalysis\n",
    "from sklearn.manifold import Isomap, MDS, SpectralEmbedding\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE, TSNE, smacof, trustworthiness\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intra_sample_distances(X, how='euclidean'):\n",
    "    if how == 'euclidean':  \n",
    "        return distance.pdist(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x] Sammon mapping: https://arxiv.org/pdf/2009.08136.pdf\n",
    "# [x] landmark maximum variance unfolding \n",
    "# [x] Landmark MDS\n",
    "# [x] GSOM: https://github.com/CDAC-lab/pygsom/tree/master/gsom -> never mind this is a clustering method..\n",
    "# [x] SMACOF\n",
    "\n",
    "# IVIS: https://github.com/beringresearch/ivis, https://www.nature.com/articles/s41598-019-45301-0\n",
    "# RankVisu\n",
    "# diffeomorphic dimensionality reduction Diffeomap\n",
    "# FastMap MDS: https://github.com/shawn-davis/FastMapy\n",
    "# FactorizedEmbeddings: https://github.com/TrofimovAssya/FactorizedEmbeddings, https://academic.oup.com/bioinformatics/article/36/Supplement_1/i417/5870511\n",
    "# MetricMap\n",
    "# SparseMap: https://github.com/vene/sparsemap\n",
    "# growing curvilinear component analysis\n",
    "# curvilinear distance analysis\n",
    "# autoencoder NeuroScale\n",
    "# PHATE\n",
    "# GPLVM\n",
    "# FA\n",
    "# Nonlinear PCA\n",
    "# SDNE \n",
    "# GCN\n",
    "# Graph Factorisation\n",
    "# HOPE\n",
    "# opt-SNE: https://github.com/omiq-ai/Multicore-opt-SNE\n",
    "#  Poincare embedding : https://github.com/facebookresearch/poincare-embeddings\n",
    "# NN-graph/Parametric UMAP -> GraphSage/Node2Vec/etc.. see NetworkX and karateclub!\n",
    "# https://github.com/benedekrozemberczki/karateclub\n",
    "# https://github.com/palash1992/GEM-Benchmark, https://github.com/palash1992/GEM\n",
    "\n",
    "# https://www.sciencedirect.com/science/article/pii/S0950705118301540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_test_sets(**kwargs):\n",
    "    \n",
    "    transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "    \n",
    "    sets = []\n",
    "    mcluster = datasets.make_blobs(n_samples=1000, n_features=3, centers=[(-10,-10), (10,10), (-10,10), (10,-10)], \n",
    "                                cluster_std=[1., 0.5, 1.5, 0.75])[0]\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    mcluster = datasets.make_circles(n_samples=1000, factor=.4, noise=.01)[0]\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    mcluster = datasets.make_s_curve(n_samples=1000, noise=.2)[0]\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,[0,2]]    \n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    mcluster = datasets.make_moons(n_samples=1000, noise=.1)[0]\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    mcluster = datasets.make_blobs(n_samples=1000, centers=3, cluster_std=[1., 0.5, 1.5])[0]\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    mcluster = datasets.make_blobs(n_samples=1000, centers=4, cluster_std=[1., 0.5, 1.5, 0.75])[0]\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    x = np.random.lognormal(mean=0., sigma=0.25, size=1000)\n",
    "    y = -0.3/x**3 + np.random.normal(loc=0., scale=0.1, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    x = np.random.lognormal(mean=0., sigma=0.25, size=1000)\n",
    "    y = 2*x**2 + np.random.normal(loc=0., scale=0.1, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    x = np.random.normal(loc=0, scale=0.5, size=1000)\n",
    "    y = 2*x**2 + np.random.uniform(low=-0.5, high=0.5, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    x = np.random.normal(loc=0, scale=0.5, size=1000)\n",
    "    y = -2*x**2 + np.random.uniform(low=-0.5, high=0.5, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    sets.append(mcluster)\n",
    "    #####################\n",
    "    x = np.random.normal(loc=0.1, scale=0.5, size=1000)\n",
    "    y = 2*x**2 + np.random.uniform(low=-0.5, high=0.5, size=1000)\n",
    "    mcluster1 = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster1 = StandardScaler().fit_transform(mcluster1)[:,:]\n",
    "\n",
    "    x = np.random.normal(loc=-0.1, scale=0.5, size=1000)\n",
    "    y = -2*x**2 + np.random.uniform(low=-0.5, high=0.5, size=1000)\n",
    "    mcluster2 = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster2 = StandardScaler().fit_transform(mcluster2)[:,:]\n",
    "    mcluster = np.vstack([mcluster1, mcluster2])\n",
    "    sets.append(mcluster)\n",
    "    ####################\n",
    "    x = np.random.normal(loc=0.1, scale=0.4, size=1000)\n",
    "    y = np.sin(4*x) + np.random.uniform(low=-0.2, high=0.2, size=1000)\n",
    "    mcluster1 = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster1 = StandardScaler().fit_transform(mcluster1)[:,:]\n",
    "\n",
    "    x = np.random.normal(loc=-0.1, scale=0.4, size=1000)\n",
    "    y = np.sin(4*x) + np.random.uniform(low=-0.2, high=0.2, size=1000)\n",
    "    mcluster2 = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster2 = StandardScaler().fit_transform(mcluster2)[:,:]\n",
    "    mcluster = np.vstack([mcluster1, mcluster2])\n",
    "    sets.append(mcluster)\n",
    "    ####################\n",
    "    x = np.random.normal(loc=0.1, scale=0.4, size=1000)\n",
    "    y = -np.sin(4*x) + np.random.uniform(low=-0.3, high=0.3, size=1000)\n",
    "    mcluster1 = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster1 = StandardScaler().fit_transform(mcluster1)[:,:]\n",
    "\n",
    "    x = np.random.normal(loc=-0.1, scale=0.4, size=1000)\n",
    "    y = np.sin(4*x) + np.random.uniform(low=-0.3, high=0.3, size=1000)\n",
    "    mcluster2 = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster2 = StandardScaler().fit_transform(mcluster2)[:,:]\n",
    "\n",
    "    mcluster = np.vstack([mcluster1, mcluster2])\n",
    "    sets.append(mcluster)\n",
    "    ###################\n",
    "    x = np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    y = x*np.sin(8*x+1) + np.random.uniform(low=-0.4, high=0.4, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)    \n",
    "    ###################\n",
    "    x = np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    y = np.random.normal(loc=0.0, scale=0.5, size=1000) # np.random.uniform(low=-0.4, high=0.4, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    sets.append(mcluster)\n",
    "    ###################\n",
    "    transformation = [[0.95, -0.33667341], [-0.30887718, 0.95]]\n",
    "    x = np.random.uniform(low=-0.4, high=0.4, size=1000) # np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    y = np.random.uniform(low=-0.4, high=0.4, size=1000) # np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    ###################\n",
    "    transformation = [[0.95, 0.33667341], [0.30887718, 0.95]]\n",
    "    x = np.random.uniform(low=-0.4, high=0.4, size=1000) # np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    y = np.random.uniform(low=-0.4, high=0.4, size=1000) # np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    ###################\n",
    "    transformation = [[0.7071, -0.7071], [0.7071, 0.7071]]\n",
    "\n",
    "    x = np.random.uniform(low=-0.4, high=0.4, size=1000) # np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    y = np.random.uniform(low=-0.4, high=0.4, size=1000) # np.random.normal(loc=0.0, scale=0.5, size=1000)\n",
    "    mcluster = np.hstack([x.reshape(-1,1), y.reshape(-1,1)])\n",
    "    mcluster = StandardScaler().fit_transform(mcluster)[:,:]\n",
    "\n",
    "    mcluster = np.dot(mcluster[:,[0,1]], transformation)\n",
    "    sets.append(mcluster)\n",
    "    \n",
    "    return sets, kwargs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = generate_2d_test_sets()[0]\n",
    "fig, ax = plt.subplots(ncols=3, nrows=6, figsize=(22,25))\n",
    "for k, ds in enumerate(test_sets):\n",
    "    j=k%3 \n",
    "    i=int(k/3)\n",
    "    ax[i,j].scatter(x=ds[:,0], y=ds[:,1])\n",
    "    ax[i,j].set_title(f'Image:{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spread_factor = 0.1\n",
    "#dimensions = 2\n",
    "#ts = 4\n",
    "#gsom_test = graph_clustering.GSOM(spread_factor,\n",
    "#                                  dimensions,\n",
    "#                                  smooth_iterations=100,\n",
    "#                                  training_iterations=250,\n",
    "#                                  learning_rate=.1)\n",
    "#gsom_test.fit(test_sets[ts])\n",
    "#assignments = gsom_test.train_assignments- gsom_test.train_assignments.min()\n",
    "\n",
    "#plot_df = pd.DataFrame(data=test_sets[ts], columns=['d1', 'd2'])\n",
    "#plot_df['cluster'] = assignments.astype(str)\n",
    "#sns.scatterplot(data=plot_df, x='d1', y='d2', hue='cluster', alpha=0.5)\n",
    "#print(plot_df.cluster.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_type = 'Sammon'\n",
    "\n",
    "n_n = 7\n",
    "\n",
    "embedder = {}\n",
    "embedder['umap'] = umap.UMAP(n_components=2, densmap=True, metric='l2', n_neighbors=n_n, min_dist=0.)\n",
    "embedder['trimap'] = trimap.TRIMAP(n_dims=2, n_iters=750)\n",
    "embedder['pacmap'] = pacmap.PaCMAP(n_dims=2, n_neighbors=n_n)\n",
    "embedder['SpectralEmbedding'] = SpectralEmbedding(n_components=2, n_neighbors=n_n)\n",
    "embedder['Isomap'] = Isomap(n_components=2)\n",
    "embedder['MDS'] = MDS(n_components=2)\n",
    "embedder['KernelPCA'] = KernelPCA(n_components=2)\n",
    "embedder['PCA'] = PCA(n_components=2)\n",
    "embedder['dbmap'] = dbmap.diffusion.Diffusor(n_components=120, ann_dist='euclidean')\n",
    "embedder['LLE'] = LLE(n_components=2, n_neighbors=n_n, method='hessian') # ltsa\n",
    "embedder['NMF'] = NMF(n_components=2, max_iter=2000) \n",
    "embedder['Sammon'] = graph_clustering.Sammon(n_components=2, n_neighbors=n_n,\n",
    "                                            max_iterations=250, learning_rate=0.1, init_type='PCA')\n",
    "embedder['MVU'] = graph_clustering.MaximumVarianceUnfolding(n_components=2, n_neighbors=n_n)\n",
    "embedder['LMVU'] = graph_clustering.LandmarkMaximumVarianceUnfolding(n_components=2, n_landmarks=50,\n",
    "                                                                     n_neighbors=n_n)\n",
    "\n",
    "test_sets_embedded = []\n",
    "if embedder_type == 'dbmap':\n",
    "    for ts in tqdm(test_sets):\n",
    "        tts = embedder['dbmap'].fit_transform(ts)\n",
    "        test_sets_embedded.append(np.array(embedder['umap'].fit_transform(tts)))\n",
    "elif embedder_type == 'NMF':\n",
    "    for ts in tqdm(test_sets):\n",
    "        tts = ts + np.abs(np.min(ts, axis=0))\n",
    "        test_sets_embedded.append(np.array(embedder['NMF'].fit_transform(tts)))    \n",
    "else:\n",
    "    for ts in tqdm(test_sets):\n",
    "        test_sets_embedded.append(np.array(embedder[embedder_type].fit_transform(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=6, figsize=(22,25))\n",
    "for k, ds in enumerate(test_sets_embedded):\n",
    "    j=k%3 \n",
    "    i=int(k/3)\n",
    "    ax[i,j].scatter(x=ds[:,0], y=ds[:,1], color='black')\n",
    "    ax[i,j].set_title(f'Image:{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "sample_size = 100\n",
    "\n",
    "dist_preservation = []\n",
    "dists = []\n",
    "for num in tqdm(range(0,18)):\n",
    "    sample_selection = np.random.randint(0,num_samples, sample_size)\n",
    "    dist_or = get_intra_sample_distances(test_sets[num][sample_selection,:])\n",
    "    dist_emb = get_intra_sample_distances(test_sets_embedded[num][sample_selection,:])\n",
    "\n",
    "    dists.append({'d_or': dist_or, 'd_emb': dist_emb})\n",
    "    dist_preservation.append({'dataset': num , 'corr':spearmanr(dist_or, dist_emb)[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=6, figsize=(22,25))\n",
    "for k, ds in enumerate(dists):\n",
    "    j=k%3 \n",
    "    i=int(k/3)\n",
    "    ax[i,j].scatter(x=ds['d_or'], y=ds['d_emb'], color='black', alpha=0.3)\n",
    "    ax[i,j].set_title(f'Image:{k}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5a14f050fb035b48dd4bb6477ca57635d0b683745fc30c1f601aba2e74fe38c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('long')",
   "language": "python",
   "name": "python385jvsc74a57bd0d5a14f050fb035b48dd4bb6477ca57635d0b683745fc30c1f601aba2e74fe38c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
