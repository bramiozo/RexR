{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings to test\n",
    "\n",
    "* UMAP\n",
    "* t-SNE\n",
    "* Parametric UMAP (part of UMAP)\n",
    "* DenseMap (part of UMAP)\n",
    "* [PacMap](https://github.com/YingfanWang/PaCMAP)\n",
    "* [TriMap](https://github.com/eamid/trimap)\n",
    "* PCA\n",
    "* Laplacian eigenmaps\n",
    "* MDS\n",
    "* Isomap\n",
    "* [MDE](https://github.com/cvxgrp/pymde)\n",
    "* [PHATE](https://github.com/KrishnaswamyLab/PHATE)\n",
    "* ForceAtlas2\n",
    "* dbMAP\n",
    "\n",
    "\n",
    "# Experiments\n",
    "\n",
    "* distance/distance-rank preservation with varying ```n_neighbors```, ```n_components``` and ```min_dist```, measured with Pearson's corr.\n",
    "* hierarchical embedding: original -> 1000d -> 100d -> 2d\n",
    "* negative test: does it magically create clusters? Test using a high dimensional Gaussian\n",
    "\n",
    "\n",
    "Metrics:\n",
    "* Spearman rank correlation between samples\n",
    "* Pearson correlation of distances\n",
    "* Distance correlation of distances\n",
    "* Average Jaccard distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport omic_helpers\n",
    "%matplotlib inline\n",
    "\n",
    "from omic_helpers import graph_clustering\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, RobustScaler, MinMaxScaler, FunctionTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import chisquare, chi2_contingency, pearsonr\n",
    "from scipy.stats import kendalltau,spearmanr, weightedtau, theilslopes, wilcoxon, ttest_rel\n",
    "from scipy.spatial import distance\n",
    "import dcor\n",
    "\n",
    "import umap\n",
    "import pacmap\n",
    "import trimap\n",
    "import pymde\n",
    "import dbmap\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA, NMF, FactorAnalysis\n",
    "from sklearn.manifold import Isomap, MDS, SpectralEmbedding\n",
    "from sklearn.manifold import LocallyLinearEmbedding as LLE, TSNE, smacof, trustworthiness\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intra_sample_distances(X, how='euclidean'):\n",
    "    if how == 'euclidean':  \n",
    "        return distance.pdist(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x] Sammon mapping: https://arxiv.org/pdf/2009.08136.pdf\n",
    "# [x] landmark maximum variance unfolding \n",
    "# [x] Landmark MDS\n",
    "# [x] GSOM: https://github.com/CDAC-lab/pygsom/tree/master/gsom -> never mind this is a clustering method..\n",
    "# [x] SMACOF\n",
    "\n",
    "# IVIS: https://github.com/beringresearch/ivis, https://www.nature.com/articles/s41598-019-45301-0\n",
    "# RankVisu\n",
    "# diffeomorphic dimensionality reduction Diffeomap\n",
    "# FastMap MDS: https://github.com/shawn-davis/FastMapy\n",
    "# FactorizedEmbeddings: https://github.com/TrofimovAssya/FactorizedEmbeddings, https://academic.oup.com/bioinformatics/article/36/Supplement_1/i417/5870511\n",
    "# MetricMap\n",
    "# SparseMap: https://github.com/vene/sparsemap\n",
    "# growing curvilinear component analysis\n",
    "# curvilinear distance analysis\n",
    "# autoencoder NeuroScale\n",
    "# PHATE\n",
    "# GPLVM\n",
    "# FA\n",
    "# Nonlinear PCA\n",
    "# SDNE \n",
    "# GCN\n",
    "# Graph Factorisation\n",
    "# HOPE\n",
    "# opt-SNE: https://github.com/omiq-ai/Multicore-opt-SNE\n",
    "#  Poincare embedding : https://github.com/facebookresearch/poincare-embeddings\n",
    "# NN-graph/Parametric UMAP -> GraphSage/Node2Vec/etc.. see NetworkX and karateclub!\n",
    "# https://github.com/benedekrozemberczki/karateclub\n",
    "# https://github.com/palash1992/GEM-Benchmark, https://github.com/palash1992/GEM\n",
    "\n",
    "# https://www.sciencedirect.com/science/article/pii/S0950705118301540"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-dimensional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 80\n",
    "num_blobs = 5\n",
    "test_data_multidim = []\n",
    "\n",
    "rnd_perturbation = np.random.normal(0, 1, (1000,dimensionality))\n",
    "test_data_multidim.append(('blobs1', datasets.make_blobs(n_samples=1000, \n",
    "                                                         n_features=dimensionality, \n",
    "                                                         centers=num_blobs)[0]+rnd_perturbation))\n",
    "test_data_multidim.append(('blobs2', datasets.make_blobs(n_samples=1000, \n",
    "                                                         n_features=dimensionality, \n",
    "                                                         centers=2*num_blobs)[0]+rnd_perturbation))\n",
    "test_data_multidim.append(('Class1', datasets.make_classification(n_samples=1000, \n",
    "                                                                  n_features=dimensionality, \n",
    "                                                                  n_informative=20, \n",
    "                                                                  n_redundant=0)[0]+rnd_perturbation))\n",
    "test_data_multidim.append(('Class2', datasets.make_classification(n_samples=1000, \n",
    "                                                                  n_features=dimensionality, \n",
    "                                                                  n_informative=5, \n",
    "                                                                  n_redundant=0)[0]+rnd_perturbation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "sample_size = 250\n",
    "sample_selection = np.random.randint(0,num_samples, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_type = 'Sammon'\n",
    "n_n = 77\n",
    "reduce_dim = 11\n",
    "scaler = StandardScaler() # QuantileTransformer(output_distribution='normal')#  QuantileTransformer(output_distribution='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedder = {}\n",
    "embedder['umap'] = umap.UMAP(n_components=reduce_dim, densmap=True, metric='euclidean',\n",
    "                             n_neighbors=n_n, min_dist=0.25, disconnection_distance=15.)\n",
    "embedder['trimap'] = trimap.TRIMAP(n_dims=reduce_dim, n_iters=2500);\n",
    "embedder['pacmap'] = pacmap.PaCMAP(n_dims=reduce_dim, n_neighbors=n_n)\n",
    "embedder['SpectralEmbedding'] = SpectralEmbedding(n_components=reduce_dim, n_neighbors=n_n)\n",
    "embedder['Isomap'] = Isomap(n_components=reduce_dim)\n",
    "embedder['MDS'] = MDS(n_components=reduce_dim, metric='euclidean')\n",
    "embedder['KernelPCA'] = KernelPCA(n_components=reduce_dim, kernel='sigmoid')\n",
    "embedder['PCA'] = PCA(n_components=reduce_dim)\n",
    "embedder['FA'] = FactorAnalysis(n_components=reduce_dim, max_iter=1000)\n",
    "embedder['dbmap'] = dbmap.diffusion.Diffusor(n_components=120, ann_dist='euclidean')\n",
    "embedder['LLE'] = LLE(n_components=reduce_dim, n_neighbors=n_n, method='ltsa')\n",
    "embedder['NMF'] = NMF(n_components=reduce_dim, max_iter=10000)\n",
    "embedder['TSNE'] = TSNE(n_components=3, perplexity=50)\n",
    "embedder['Sammon'] = graph_clustering.Sammon(n_components=reduce_dim, n_neighbors=n_n,\n",
    "                                            max_iterations=250, learning_rate=0.05, init_type='PCA')\n",
    "embedder['MVU'] = graph_clustering.MaximumVarianceUnfolding(n_components=2, n_neighbors=n_n)\n",
    "embedder['LMVU'] = graph_clustering.LandmarkMaximumVarianceUnfolding(n_components=reduce_dim, \n",
    "                                                                     n_neighbors=n_n, \n",
    "                                                                     n_landmarks=n_landmarks)\n",
    "embedder['LMDS'] = graph_clustering.LandmarkMultiDimensionalScaling(n_components=reduce_dim,\n",
    "                                                                     n_landmarks=n_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedder['MVU']\n",
    "#embedder['GSOM']\n",
    "#embedder['MetricMap']\n",
    "#embedder['SparseMap']\n",
    "\n",
    "test_sets_embedded = []\n",
    "if embedder_type == 'dbmap':\n",
    "    pipe = Pipeline([('scaler', scaler), \n",
    "                     ('prepmap', embedder['dbmap']), \n",
    "                     ('reducer', embedder['umap'])])\n",
    "    for _, ts in tqdm(test_data_multidim):\n",
    "        tts = embedder['dbmap'].fit_transform(ts)\n",
    "        test_sets_embedded.append(np.array(pipe.fit_transform(tts)))\n",
    "elif embedder_type == 'NMF':    \n",
    "    for _, ts in tqdm(test_data_multidim):\n",
    "        nonnegger = lambda x: x + 2*np.abs(np.min(x, axis=0))\n",
    "        nonnegger_F = FunctionTransformer(func=nonnegger)\n",
    "\n",
    "        pipe = Pipeline([('scaler', scaler), \n",
    "                         ('nngr', nonnegger_F), \n",
    "                         ('reducer', embedder['NMF'])])\n",
    "        test_sets_embedded.append(pipe.fit_transform(ts)) \n",
    "else:\n",
    "    pipe = Pipeline([('scaler', scaler), \n",
    "                     ('reducer', embedder[embedder_type])])\n",
    "    for _, ts in tqdm(test_data_multidim):\n",
    "        test_sets_embedded.append(pipe.fit_transform(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(22,25))\n",
    "#for k, ds in enumerate(test_sets_embedded):\n",
    "#    j=k%2 \n",
    "#    i=int(k/2)\n",
    "#    ax[i,j].scatter(x=ds[:,0], y=ds[:,1], color='black')\n",
    "#    ax[i,j].set_title(f'Image:{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_preservation_overall = []\n",
    "dists = []\n",
    "for num in tqdm(range(0,4)):\n",
    "    dist_or = get_intra_sample_distances(test_data_multidim[num][1][sample_selection,:])\n",
    "    dist_emb = get_intra_sample_distances(test_sets_embedded[num][sample_selection,:])\n",
    "\n",
    "    dists.append({'d_or': dist_or, 'd_emb': dist_emb})\n",
    "    dist_preservation_overall.append({'dataset': test_data_multidim[num][0], \n",
    "                              'corr':dcor.distance_correlation(dist_or, dist_emb)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(22,25))\n",
    "for k, ds in enumerate(dists):\n",
    "    j=k%2 \n",
    "    i=int(k/2)\n",
    "    ax[i,j].scatter(x=ds['d_or'], y=ds['d_emb'], color='black', alpha=0.01)\n",
    "    mx,my = max(ds['d_or']), max(ds['d_emb'])\n",
    "    ax[i,j].plot([0,mx], [0, my], ls='--', c='blue')\n",
    "    ax[i,j].set_title(f'Image:{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_preservation_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, roughly: *a better metric approximation is co-related with a worse cluster separation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised UMAP\n",
    "\n",
    "The main flavor is to add labels for the different clusters we know we want to \n",
    "see. This can be based on a clustering on a sample set of the original data (perhaps also a selection of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric UMAP\n",
    "\n",
    "* Create nearest-neighbor graph with fuzzy simplicials\n",
    "* Apply graph embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchored embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance preserving embedding\n",
    "\n",
    "* Siamese twins networks\n",
    "* distance as outcome\n",
    "* pairs as input\n",
    "\n",
    "The method IVIS seems to use this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking based embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-patch UMAP\n",
    "\n",
    "The core assumption of UMAP is that all points lie on the same manifold. What if we split our data in dense patches prior to the creation of the fuzzy simplicials? \n",
    "\n",
    "To make this tractable this split should be computationally in-expensive. One way to go about is to treat overlapping regions with a sufficient number of samples as patches. The embeddings associated with these patches can later be combined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-sample UMAP\n",
    "\n",
    "\n",
    "* $N$ sampled UMAP embedders with/without minimal perturbations\n",
    "* aligned using Procrustes\n",
    "* uniform scaling\n",
    "* concensus distance determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarkbased embeddings coupled to sparse exemplar finders\n",
    "\n",
    "Instead of random landmarks we can use exemplars based on \n",
    "* points closest to centroids\n",
    "* exemplars based on e.g. affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5a14f050fb035b48dd4bb6477ca57635d0b683745fc30c1f601aba2e74fe38c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('long')",
   "language": "python",
   "name": "python385jvsc74a57bd0d5a14f050fb035b48dd4bb6477ca57635d0b683745fc30c1f601aba2e74fe38c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
