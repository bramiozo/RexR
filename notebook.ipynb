{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ Firing up RexR! ++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from RexR import *\n",
    "import _helpers\n",
    "Rocket = RexR(datalocation = None, #'_data/genomic_data/data.pkl', \n",
    "              seed = 3123, \n",
    "              debug = False, \n",
    "              write_out=True,\n",
    "              set_name = 'ALL_10') # data to read in ALL_10, or MELA\n",
    "Rocket.load_probeset_data();\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD_LIST = ['RF','XGB', 'LGBM', 'ExtraTrees','SVM', 'LR', 'MLNN', 'RVM', 'DNN', 'CNN'] #, 'NaiveBayes','MLNN', 'XGB'] # \n",
    "Runs = []\n",
    "nruns = 1\n",
    "SCALER = \"standard\" # minmax, standard, normaliser\n",
    "GROUPING = \"mean\"\n",
    "FEAT_SELECTOR = \"low_variance\" # \"low_variance\" None\n",
    "SELECTOR_METHOD = \"FDR\" # mannwhitney, FDR\n",
    "SELECTOR_ALPHA = 0.05 # see this as the maximum p-value to classify \n",
    "DIM_TYPE =  None #\"PCA\" #\"LDA\", \"PCA\", \"PLS\" \n",
    "DIM_NUM = 1000\n",
    "Results = None\n",
    "ACC = pd.DataFrame()\n",
    "Rocket.VIZ = False\n",
    "Rocket.DATA_merged_processed = None\n",
    "PREPROC_DICT = {\"patient_grouping\": GROUPING, \"bias_removal\": False, \"noise\": True}\n",
    "FSELECT_DICT = {\"type\": FEAT_SELECTOR, \"pvalue\": SELECTOR_ALPHA, \"method\": SELECTOR_METHOD}\n",
    "DIMRED_DICT = {\"type\": DIM_TYPE, \"n_comp\": DIM_NUM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Prepping data, this may take a while..\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Grouping probesets\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Selecting features using a low_variance filter\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  Kept 230 of 54613 features using FDR with p = 0.05\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "MODEL: RF accuracy:  0.8103448275862071 +/-: 0.02176341296436421\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: XGB accuracy:  0.7241379310344827 +/-: 0.03244243865528051\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: LGBM accuracy:  0.8103448275862067 +/-: 0.01653875256729002\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "MODEL: ET accuracy:  0.793103448275862 +/-: 0.03700050444996938\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "MODEL: SVM accuracy:  0.8275862068965517 +/-: 0.014683097322811953\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "MODEL: LR accuracy:  0.8620689655172412 +/-: 0.013237307678449179\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "MODEL: MLNN accuracy:  0.8965517241379309 +/-: 0.01542175620653623\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "Initial alpha = [[0.08480218]]\n",
      "   1 - L=-530.8974440 - Gamma= 1.9999608 (M=   2) - s=0.0100\n",
      "   2 - L=-420.0469737 - Gamma= 2.9998561 (M=   3) - s=0.0100\n",
      "   3 - L=-351.0562616 - Gamma= 3.9996949 (M=   4) - s=0.0100\n",
      "   4 - L=-290.8579012 - Gamma= 4.9994783 (M=   5) - s=0.0100\n",
      "   5 - L=-259.3288128 - Gamma= 5.9990850 (M=   6) - s=0.0100\n",
      "   6 - L=-205.5954630 - Gamma= 6.9987885 (M=   7) - s=0.0100\n",
      "   7 - L=-173.9295968 - Gamma= 7.9983583 (M=   8) - s=0.0100\n",
      "   8 - L=-129.4495037 - Gamma= 8.9979133 (M=   9) - s=0.0100\n",
      "   9 - L=-112.8366329 - Gamma= 9.9971464 (M=  10) - s=0.0100\n",
      "  10 - L=-92.8263150 - Gamma=10.9963896 (M=  11) - s=0.0100\n",
      "  11 - L=-79.0685733 - Gamma=11.9952137 (M=  12) - s=0.0100\n",
      "  12 - L=-70.9392824 - Gamma=12.9932922 (M=  13) - s=0.0100\n",
      "  13 - L=-60.0521510 - Gamma=13.9919110 (M=  14) - s=0.0100\n",
      "  14 - L=-43.4271325 - Gamma=14.9906545 (M=  15) - s=0.0100\n",
      "  15 - L=-34.1759636 - Gamma=15.9885361 (M=  16) - s=0.0100\n",
      "  16 - L=-28.0805229 - Gamma=16.9863721 (M=  17) - s=0.0100\n",
      "  17 - L=-20.0013244 - Gamma=17.9841614 (M=  18) - s=0.0100\n",
      "  18 - L=-15.8489456 - Gamma=18.9809304 (M=  19) - s=0.0100\n",
      "  19 - L=-12.2458217 - Gamma=19.9768288 (M=  20) - s=0.0100\n",
      "  20 - L=-9.2187965 - Gamma=20.9719984 (M=  21) - s=0.0100\n",
      "  21 - L=-6.7352055 - Gamma=21.9661042 (M=  22) - s=0.0100\n",
      "  22 - L=-4.8430065 - Gamma=22.9430420 (M=  23) - s=0.0100\n",
      "  23 - L=-2.9886298 - Gamma=23.9104525 (M=  24) - s=0.0100\n",
      "  24 - L=-1.6929034 - Gamma=24.8849326 (M=  25) - s=0.0100\n",
      "  25 - L=-0.8957827 - Gamma=25.8626593 (M=  26) - s=0.0100\n",
      "  26 - L=-0.1147099 - Gamma=26.8421497 (M=  27) - s=0.0100\n",
      "  27 - L= 0.5970551 - Gamma=26.9016427 (M=  27) - s=0.0100\n",
      "  28 - L= 0.9287355 - Gamma=27.8701969 (M=  28) - s=0.0100\n",
      "  29 - L= 1.1096945 - Gamma=28.8199023 (M=  29) - s=0.0100\n",
      "  30 - L= 1.2006743 - Gamma=29.7263042 (M=  30) - s=0.0100\n",
      "  31 - L= 1.3332271 - Gamma=30.6583752 (M=  31) - s=0.0100\n",
      "  32 - L= 1.4121512 - Gamma=30.6607084 (M=  31) - s=0.0100\n",
      "  33 - L= 1.4607051 - Gamma=30.5636163 (M=  31) - s=0.0100\n",
      "  34 - L= 1.4982330 - Gamma=30.5662478 (M=  31) - s=0.0100\n",
      "  35 - L= 1.5339030 - Gamma=31.3827952 (M=  32) - s=0.0100\n",
      "  36 - L= 1.5674244 - Gamma=31.3856126 (M=  32) - s=0.0100\n",
      "  37 - L= 1.5973390 - Gamma=32.1903481 (M=  33) - s=0.0100\n",
      "  38 - L= 1.6254498 - Gamma=32.1911453 (M=  33) - s=0.0100\n",
      "  39 - L= 1.6417273 - Gamma=32.1925859 (M=  33) - s=0.0100\n",
      "  40 - L= 1.6574470 - Gamma=32.1995665 (M=  33) - s=0.0100\n",
      "  41 - L= 1.6681694 - Gamma=32.2017098 (M=  33) - s=0.0100\n",
      "  42 - L= 1.6786690 - Gamma=32.2022566 (M=  33) - s=0.0100\n",
      "  43 - L= 1.6862870 - Gamma=32.2086373 (M=  33) - s=0.0100\n",
      "  44 - L= 1.6929017 - Gamma=32.2102751 (M=  33) - s=0.0100\n",
      "  45 - L= 1.6994018 - Gamma=32.2140804 (M=  33) - s=0.0100\n",
      "  46 - L= 1.7051269 - Gamma=32.7732667 (M=  34) - s=0.0100\n",
      "  47 - L= 1.7101851 - Gamma=32.7741412 (M=  34) - s=0.0100\n",
      "  48 - L= 1.7142506 - Gamma=32.7777248 (M=  34) - s=0.0100\n",
      "  49 - L= 1.7167920 - Gamma=32.7782820 (M=  34) - s=0.0100\n",
      "  50 - L= 1.7193119 - Gamma=32.7811718 (M=  34) - s=0.0100\n",
      "  51 - L= 1.7217805 - Gamma=32.7839206 (M=  34) - s=0.0100\n",
      "  52 - L= 1.7240141 - Gamma=32.7873400 (M=  34) - s=0.0100\n",
      "  53 - L= 1.7258255 - Gamma=32.7856010 (M=  34) - s=0.0100\n",
      "  54 - L= 1.7275391 - Gamma=32.7895040 (M=  34) - s=0.0100\n",
      "  55 - L= 1.7288824 - Gamma=32.8144525 (M=  34) - s=0.0100\n",
      "  56 - L= 1.7302401 - Gamma=32.8257184 (M=  34) - s=0.0100\n",
      "  57 - L= 1.7313562 - Gamma=33.1414034 (M=  35) - s=0.0100\n",
      "  58 - L= 1.7321718 - Gamma=33.0898503 (M=  35) - s=0.0100\n",
      "  59 - L= 1.7328696 - Gamma=33.0833798 (M=  35) - s=0.0100\n",
      "  60 - L= 1.7333406 - Gamma=33.0963287 (M=  35) - s=0.0100\n",
      "  61 - L= 1.7337022 - Gamma=33.2875087 (M=  36) - s=0.0100\n",
      "  62 - L= 1.7340386 - Gamma=33.3221166 (M=  36) - s=0.0100\n",
      "  63 - L= 1.7343275 - Gamma=33.3226750 (M=  36) - s=0.0100\n",
      "  64 - L= 1.7345924 - Gamma=33.4369226 (M=  36) - s=0.0100\n",
      "  65 - L= 1.7347213 - Gamma=33.5519793 (M=  37) - s=0.0100\n",
      "  66 - L= 1.7348455 - Gamma=33.5263450 (M=  37) - s=0.0100\n",
      "  67 - L= 1.7349440 - Gamma=33.5262656 (M=  37) - s=0.0100\n",
      "  68 - L= 1.7350096 - Gamma=33.5724845 (M=  38) - s=0.0100\n",
      "  69 - L= 1.7350901 - Gamma=33.5482181 (M=  38) - s=0.0100\n",
      "  70 - L= 1.7351555 - Gamma=33.5887189 (M=  38) - s=0.0100\n",
      "  71 - L= 1.7352222 - Gamma=33.5644031 (M=  38) - s=0.0100\n",
      "  72 - L= 1.7352794 - Gamma=33.6321325 (M=  38) - s=0.0100\n",
      "  73 - L= 1.7353837 - Gamma=33.6761279 (M=  38) - s=0.0100\n",
      "  74 - L= 1.7354813 - Gamma=33.6426538 (M=  38) - s=0.0100\n",
      "  75 - L= 1.7355598 - Gamma=33.6756233 (M=  38) - s=0.0100\n",
      "  76 - L= 1.7356406 - Gamma=33.6405947 (M=  38) - s=0.0100\n",
      "  77 - L= 1.7357064 - Gamma=33.6673015 (M=  38) - s=0.0100\n",
      "  78 - L= 1.7357758 - Gamma=33.7343097 (M=  38) - s=0.0100\n",
      "  79 - L= 1.7358448 - Gamma=33.6973195 (M=  38) - s=0.0100\n",
      "  80 - L= 1.7359390 - Gamma=33.7256985 (M=  38) - s=0.0100\n",
      "  81 - L= 1.7360373 - Gamma=33.6743223 (M=  38) - s=0.0100\n",
      "  82 - L= 1.7361418 - Gamma=33.6148280 (M=  38) - s=0.0100\n",
      "  83 - L= 1.7362729 - Gamma=33.5015519 (M=  38) - s=0.0100\n",
      "  84 - L= 1.7364779 - Gamma=33.6562058 (M=  39) - s=0.0100\n",
      "  85 - L= 1.7365299 - Gamma=33.5928160 (M=  38) - s=0.0100\n",
      "  86 - L= 1.7367318 - Gamma=33.4914395 (M=  38) - s=0.0100\n",
      "  87 - L= 1.7368791 - Gamma=33.5742382 (M=  38) - s=0.0100\n",
      "  88 - L= 1.7370891 - Gamma=33.6112932 (M=  38) - s=0.0100\n",
      "  89 - L= 1.7374267 - Gamma=33.5492433 (M=  37) - s=0.0100\n",
      "  90 - L= 1.7375485 - Gamma=33.5545203 (M=  37) - s=0.0100\n",
      "  91 - L= 1.7376654 - Gamma=33.5326912 (M=  37) - s=0.0100\n",
      "  92 - L= 1.7377498 - Gamma=33.4559409 (M=  37) - s=0.0100\n",
      "  93 - L= 1.7378776 - Gamma=33.5218563 (M=  37) - s=0.0100\n",
      "  94 - L= 1.7379532 - Gamma=33.5425240 (M=  37) - s=0.0100\n",
      "  95 - L= 1.7380349 - Gamma=33.4771968 (M=  37) - s=0.0100\n",
      "  96 - L= 1.7381077 - Gamma=33.4875458 (M=  37) - s=0.0100\n",
      "  97 - L= 1.7381572 - Gamma=33.4217017 (M=  37) - s=0.0100\n",
      "  98 - L= 1.7382196 - Gamma=33.4009341 (M=  37) - s=0.0100\n",
      "  99 - L= 1.7382785 - Gamma=33.4854118 (M=  38) - s=0.0100\n",
      " 100 - L= 1.7383296 - Gamma=33.4690164 (M=  38) - s=0.0100\n",
      " 101 - L= 1.7383858 - Gamma=33.3939209 (M=  38) - s=0.0100\n",
      " 102 - L= 1.7384660 - Gamma=33.4402028 (M=  38) - s=0.0100\n",
      " 103 - L= 1.7385055 - Gamma=33.3720789 (M=  38) - s=0.0100\n",
      " 104 - L= 1.7385415 - Gamma=33.4340548 (M=  38) - s=0.0100\n",
      " 105 - L= 1.7386416 - Gamma=33.3450644 (M=  37) - s=0.0100\n",
      " 106 - L= 1.7386943 - Gamma=33.3794972 (M=  37) - s=0.0100\n",
      " 107 - L= 1.7387434 - Gamma=33.3810672 (M=  37) - s=0.0100\n",
      " 108 - L= 1.7387950 - Gamma=33.3623761 (M=  37) - s=0.0100\n",
      " 109 - L= 1.7388459 - Gamma=33.4319439 (M=  37) - s=0.0100\n",
      " 110 - L= 1.7388818 - Gamma=33.4322692 (M=  37) - s=0.0100\n",
      " 111 - L= 1.7389141 - Gamma=33.4306187 (M=  37) - s=0.0100\n",
      " 112 - L= 1.7389453 - Gamma=33.4145126 (M=  37) - s=0.0100\n",
      " 113 - L= 1.7389751 - Gamma=33.4142990 (M=  37) - s=0.0100\n",
      " 114 - L= 1.7390019 - Gamma=33.4020044 (M=  37) - s=0.0100\n",
      " 115 - L= 1.7390270 - Gamma=33.3957618 (M=  37) - s=0.0100\n",
      " 116 - L= 1.7390504 - Gamma=33.3955158 (M=  37) - s=0.0100\n",
      " 117 - L= 1.7390735 - Gamma=33.3954373 (M=  37) - s=0.0100\n",
      " 118 - L= 1.7390925 - Gamma=33.3955623 (M=  37) - s=0.0100\n",
      " 119 - L= 1.7391069 - Gamma=33.3956114 (M=  37) - s=0.0100\n",
      " 120 - L= 1.7391212 - Gamma=33.3958047 (M=  37) - s=0.0100\n",
      " 121 - L= 1.7391338 - Gamma=33.3957988 (M=  37) - s=0.0100\n",
      " 122 - L= 1.7391461 - Gamma=33.3958270 (M=  37) - s=0.0100\n",
      " 123 - L= 1.7391575 - Gamma=33.3956679 (M=  37) - s=0.0100\n",
      " 124 - L= 1.7391690 - Gamma=33.3956973 (M=  37) - s=0.0100\n",
      " 125 - L= 1.7391799 - Gamma=33.3971198 (M=  37) - s=0.0100\n",
      " 126 - L= 1.7391902 - Gamma=33.3971288 (M=  37) - s=0.0100\n",
      " 127 - L= 1.7391974 - Gamma=33.3972075 (M=  37) - s=0.0100\n",
      " 128 - L= 1.7392046 - Gamma=33.4091070 (M=  37) - s=0.0100\n",
      " 129 - L= 1.7392104 - Gamma=33.4089909 (M=  37) - s=0.0100\n",
      " 130 - L= 1.7392167 - Gamma=33.4319315 (M=  37) - s=0.0100\n",
      " 131 - L= 1.7392223 - Gamma=33.4319629 (M=  37) - s=0.0100\n",
      " 132 - L= 1.7392279 - Gamma=33.4254708 (M=  37) - s=0.0100\n",
      " 133 - L= 1.7392325 - Gamma=33.4009243 (M=  37) - s=0.0100\n",
      " 134 - L= 1.7392368 - Gamma=33.4009051 (M=  37) - s=0.0100\n",
      " 135 - L= 1.7392398 - Gamma=33.4013559 (M=  37) - s=0.0100\n",
      " 136 - L= 1.7392427 - Gamma=33.4014239 (M=  37) - s=0.0100\n",
      " 137 - L= 1.7392451 - Gamma=33.4017564 (M=  37) - s=0.0100\n",
      " 138 - L= 1.7392474 - Gamma=33.4016331 (M=  37) - s=0.0100\n",
      " 139 - L= 1.7392499 - Gamma=33.4131972 (M=  37) - s=0.0100\n",
      " 140 - L= 1.7392528 - Gamma=33.4206244 (M=  37) - s=0.0100\n",
      " 141 - L= 1.7392552 - Gamma=33.4186108 (M=  37) - s=0.0100\n",
      " 142 - L= 1.7392574 - Gamma=33.4177393 (M=  37) - s=0.0100\n",
      " 143 - L= 1.7392587 - Gamma=33.4177351 (M=  37) - s=0.0100\n",
      " 144 - L= 1.7392597 - Gamma=33.4147805 (M=  37) - s=0.0100\n",
      " 145 - L= 1.7392606 - Gamma=33.4148133 (M=  37) - s=0.0100\n",
      " 146 - L= 1.7392613 - Gamma=33.4211436 (M=  37) - s=0.0100\n",
      " 147 - L= 1.7392619 - Gamma=33.4192383 (M=  37) - s=0.0100\n",
      " 148 - L= 1.7392626 - Gamma=33.4265203 (M=  37) - s=0.0100\n",
      " 149 - L= 1.7392629 - Gamma=33.4201784 (M=  37) - s=0.0100\n",
      " 150 - L= 1.7392632 - Gamma=33.4225339 (M=  37) - s=0.0100\n",
      " 151 - L= 1.7392634 - Gamma=33.4218772 (M=  37) - s=0.0100\n",
      " 152 - L= 1.7392637 - Gamma=33.4254672 (M=  37) - s=0.0100\n",
      " 153 - L= 1.7392639 - Gamma=33.4254709 (M=  37) - s=0.0100\n",
      " 154 - L= 1.7392640 - Gamma=33.4254720 (M=  37) - s=0.0100\n",
      " 155 - L= 1.7392642 - Gamma=33.4209470 (M=  37) - s=0.0100\n",
      " 156 - L= 1.7392643 - Gamma=33.4224814 (M=  37) - s=0.0100\n",
      " 157 - L= 1.7392644 - Gamma=33.4224680 (M=  37) - s=0.0100\n",
      " 158 - L= 1.7392645 - Gamma=33.4223830 (M=  37) - s=0.0100\n",
      " 159 - L= 1.7392646 - Gamma=33.4223688 (M=  37) - s=0.0100\n",
      " 160 - L= 1.7392647 - Gamma=33.4223711 (M=  37) - s=0.0100\n",
      " 161 - L= 1.7392648 - Gamma=33.4223791 (M=  37) - s=0.0100\n",
      " 162 - L= 1.7392648 - Gamma=33.4217061 (M=  37) - s=0.0100\n",
      " 163 - L= 1.7392649 - Gamma=33.4189921 (M=  37) - s=0.0100\n",
      " 164 - L= 1.7392649 - Gamma=33.4189884 (M=  37) - s=0.0100\n",
      " 165 - L= 1.7392650 - Gamma=33.4199188 (M=  37) - s=0.0100\n",
      " 166 - L= 1.7392650 - Gamma=33.4214153 (M=  37) - s=0.0100\n",
      " 167 - L= 1.7392651 - Gamma=33.4210918 (M=  37) - s=0.0100\n",
      " 168 - L= 1.7392651 - Gamma=33.4210735 (M=  37) - s=0.0100\n",
      " 169 - L= 1.7392652 - Gamma=33.4204048 (M=  37) - s=0.0100\n",
      " 170 - L= 1.7392652 - Gamma=33.4204151 (M=  37) - s=0.0100\n",
      " 171 - L= 1.7392653 - Gamma=33.4204061 (M=  37) - s=0.0100\n",
      " 172 - L= 1.7392653 - Gamma=33.4199986 (M=  37) - s=0.0100\n",
      " 173 - L= 1.7392653 - Gamma=33.4211845 (M=  37) - s=0.0100\n",
      " 174 - L= 1.7392653 - Gamma=33.4211922 (M=  37) - s=0.0100\n",
      " 175 - L= 1.7392654 - Gamma=33.4211288 (M=  37) - s=0.0100\n",
      " 176 - L= 1.7392654 - Gamma=33.4211278 (M=  37) - s=0.0100\n",
      " 177 - L= 1.7392654 - Gamma=33.4216107 (M=  37) - s=0.0100\n",
      " 178 - L= 1.7392654 - Gamma=33.4197010 (M=  37) - s=0.0100\n",
      " 179 - L= 1.7392654 - Gamma=33.4196774 (M=  37) - s=0.0100\n",
      " 180 - L= 1.7392654 - Gamma=33.4200361 (M=  37) - s=0.0100\n",
      " 181 - L= 1.7392654 - Gamma=33.4200334 (M=  37) - s=0.0100\n",
      " 182 - L= 1.7392655 - Gamma=33.4199275 (M=  37) - s=0.0100\n",
      " 183 - L= 1.7392655 - Gamma=33.4206523 (M=  37) - s=0.0100\n",
      " 184 - L= 1.7392655 - Gamma=33.4211730 (M=  37) - s=0.0100\n",
      " 185 - L= 1.7392655 - Gamma=33.4211730 (M=  37) - s=0.0100\n",
      "Stopping at iteration 185 - max_delta_ml=2.4029860437439245e-07\n",
      "L=1.7392654668820864 - Gamma=33.42117297519535 (M=37) - s=0.01\n",
      "Initial alpha = [[0.0850453]]\n",
      "   1 - L=-601.5542178 - Gamma= 1.9999517 (M=   2) - s=0.0100\n",
      "   2 - L=-419.9204198 - Gamma= 2.9998886 (M=   3) - s=0.0100\n",
      "   3 - L=-334.0015891 - Gamma= 3.9997498 (M=   4) - s=0.0100\n",
      "   4 - L=-283.4061264 - Gamma= 4.9995112 (M=   5) - s=0.0100\n",
      "   5 - L=-238.0272655 - Gamma= 5.9991933 (M=   6) - s=0.0100\n",
      "   6 - L=-211.0931586 - Gamma= 6.9987064 (M=   7) - s=0.0100\n",
      "   7 - L=-179.0867414 - Gamma= 7.9983418 (M=   8) - s=0.0100\n",
      "   8 - L=-136.9096151 - Gamma= 8.9980368 (M=   9) - s=0.0100\n",
      "   9 - L=-116.1455609 - Gamma= 9.9974252 (M=  10) - s=0.0100\n",
      "  10 - L=-97.7886079 - Gamma=10.9967279 (M=  11) - s=0.0100\n",
      "  11 - L=-81.5441358 - Gamma=11.9959502 (M=  12) - s=0.0100\n",
      "  12 - L=-63.4200938 - Gamma=12.9950491 (M=  13) - s=0.0100\n",
      "  13 - L=-54.3925228 - Gamma=13.9927437 (M=  14) - s=0.0100\n",
      "  14 - L=-47.7124546 - Gamma=14.9909105 (M=  15) - s=0.0100\n",
      "  15 - L=-39.4938960 - Gamma=15.9887276 (M=  16) - s=0.0100\n",
      "  16 - L=-31.9553523 - Gamma=16.9862927 (M=  17) - s=0.0100\n",
      "  17 - L=-24.3641158 - Gamma=17.9835635 (M=  18) - s=0.0100\n",
      "  18 - L=-17.5178010 - Gamma=18.9814931 (M=  19) - s=0.0100\n",
      "  19 - L=-13.1149704 - Gamma=19.9782959 (M=  20) - s=0.0100\n",
      "  20 - L=-8.7087104 - Gamma=20.9741345 (M=  21) - s=0.0100\n",
      "  21 - L=-6.2000646 - Gamma=21.9695924 (M=  22) - s=0.0100\n",
      "  22 - L=-3.8830335 - Gamma=22.9634789 (M=  23) - s=0.0100\n",
      "  23 - L=-1.8092951 - Gamma=23.9548445 (M=  24) - s=0.0100\n",
      "  24 - L=-1.7275248 - Gamma=22.9576511 (M=  23) - s=0.0100\n",
      "  25 - L=-0.5996392 - Gamma=23.9468611 (M=  24) - s=0.0100\n",
      "  26 - L= 0.1925786 - Gamma=24.9324429 (M=  25) - s=0.0100\n",
      "  27 - L= 0.5472755 - Gamma=25.8997589 (M=  26) - s=0.0100\n",
      "  28 - L= 0.9018573 - Gamma=26.8588641 (M=  27) - s=0.0100\n",
      "  29 - L= 1.1605144 - Gamma=27.8188470 (M=  28) - s=0.0100\n",
      "  30 - L= 1.3303583 - Gamma=28.7506253 (M=  29) - s=0.0100\n",
      "  31 - L= 1.4915781 - Gamma=29.6852783 (M=  30) - s=0.0100\n",
      "  32 - L= 1.5597878 - Gamma=29.6868365 (M=  30) - s=0.0100\n",
      "  33 - L= 1.6251531 - Gamma=29.6881140 (M=  30) - s=0.0100\n",
      "  34 - L= 1.6603203 - Gamma=30.5112272 (M=  31) - s=0.0100\n",
      "  35 - L= 1.6855043 - Gamma=31.2291178 (M=  32) - s=0.0100\n",
      "  36 - L= 1.7057694 - Gamma=31.2307351 (M=  32) - s=0.0100\n",
      "  37 - L= 1.7249726 - Gamma=31.2389119 (M=  32) - s=0.0100\n",
      "  38 - L= 1.7427331 - Gamma=31.2381400 (M=  32) - s=0.0100\n",
      "  39 - L= 1.7590912 - Gamma=31.2395776 (M=  32) - s=0.0100\n",
      "  40 - L= 1.7741452 - Gamma=31.2437081 (M=  32) - s=0.0100\n",
      "  41 - L= 1.7884278 - Gamma=31.2454501 (M=  32) - s=0.0100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bramiozo/DEV/GIT/RexR/rvm.py:289: RuntimeWarning: invalid value encountered in log\n",
      "  change = np.abs(np.log(newAlpha) - np.log(self.Alpha[j]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  42 - L= 1.8013935 - Gamma=31.2527516 (M=  32) - s=0.0100\n",
      "  43 - L= 1.8125567 - Gamma=31.2552713 (M=  32) - s=0.0100\n",
      "  44 - L= 1.8229514 - Gamma=31.2561623 (M=  32) - s=0.0100\n",
      "  45 - L= 1.8320160 - Gamma=31.2682677 (M=  32) - s=0.0100\n",
      "  46 - L= 1.8389406 - Gamma=31.8630569 (M=  33) - s=0.0100\n",
      "  47 - L= 1.8446765 - Gamma=31.8544958 (M=  33) - s=0.0100\n",
      "  48 - L= 1.8502033 - Gamma=31.8726044 (M=  33) - s=0.0100\n",
      "  49 - L= 1.8553945 - Gamma=31.8768507 (M=  33) - s=0.0100\n",
      "  50 - L= 1.8602539 - Gamma=31.8674155 (M=  33) - s=0.0100\n",
      "  51 - L= 1.8648532 - Gamma=31.8708066 (M=  33) - s=0.0100\n",
      "  52 - L= 1.8688645 - Gamma=31.8666367 (M=  33) - s=0.0100\n",
      "  53 - L= 1.8719845 - Gamma=32.3360421 (M=  34) - s=0.0100\n",
      "  54 - L= 1.8752764 - Gamma=32.3375245 (M=  34) - s=0.0100\n",
      "  55 - L= 1.8776316 - Gamma=32.3560868 (M=  34) - s=0.0100\n",
      "  56 - L= 1.8797426 - Gamma=32.1601446 (M=  34) - s=0.0100\n",
      "  57 - L= 1.8811825 - Gamma=32.1597469 (M=  34) - s=0.0100\n",
      "  58 - L= 1.8823534 - Gamma=32.1619775 (M=  34) - s=0.0100\n",
      "  59 - L= 1.8834223 - Gamma=32.1816348 (M=  34) - s=0.0100\n",
      "  60 - L= 1.8844311 - Gamma=32.4228527 (M=  35) - s=0.0100\n",
      "  61 - L= 1.8858548 - Gamma=32.4469190 (M=  35) - s=0.0100\n",
      "  62 - L= 1.8873326 - Gamma=32.4104212 (M=  35) - s=0.0100\n",
      "  63 - L= 1.8883022 - Gamma=32.1972220 (M=  35) - s=0.0100\n",
      "  64 - L= 1.8896756 - Gamma=32.3840822 (M=  35) - s=0.0100\n",
      "  65 - L= 1.8902870 - Gamma=32.2007001 (M=  34) - s=0.0100\n",
      "  66 - L= 1.8919457 - Gamma=32.3443258 (M=  34) - s=0.0100\n",
      "  67 - L= 1.8926980 - Gamma=32.3438613 (M=  34) - s=0.0100\n",
      "  68 - L= 1.8933243 - Gamma=32.3107990 (M=  34) - s=0.0100\n",
      "  69 - L= 1.8938661 - Gamma=32.3006425 (M=  34) - s=0.0100\n",
      "  70 - L= 1.8943355 - Gamma=32.1245899 (M=  34) - s=0.0100\n",
      "  71 - L= 1.8950237 - Gamma=32.2157649 (M=  34) - s=0.0100\n",
      "  72 - L= 1.8954614 - Gamma=32.2156862 (M=  34) - s=0.0100\n",
      "  73 - L= 1.8958351 - Gamma=32.2043876 (M=  34) - s=0.0100\n",
      "  74 - L= 1.8961431 - Gamma=32.2141878 (M=  34) - s=0.0100\n",
      "  75 - L= 1.8963750 - Gamma=32.2157218 (M=  34) - s=0.0100\n",
      "  76 - L= 1.8965265 - Gamma=32.2133233 (M=  34) - s=0.0100\n",
      "  77 - L= 1.8966438 - Gamma=32.1041321 (M=  34) - s=0.0100\n",
      "  78 - L= 1.8967672 - Gamma=32.1095690 (M=  34) - s=0.0100\n",
      "  79 - L= 1.8968688 - Gamma=32.1108543 (M=  34) - s=0.0100\n",
      "  80 - L= 1.8969659 - Gamma=32.1389883 (M=  34) - s=0.0100\n",
      "  81 - L= 1.8970693 - Gamma=32.1332037 (M=  34) - s=0.0100\n",
      "  82 - L= 1.8971567 - Gamma=32.2437414 (M=  35) - s=0.0100\n",
      "  83 - L= 1.8972913 - Gamma=32.1276634 (M=  34) - s=0.0100\n",
      "  84 - L= 1.8973739 - Gamma=32.1266641 (M=  34) - s=0.0100\n",
      "  85 - L= 1.8974527 - Gamma=32.1270983 (M=  34) - s=0.0100\n",
      "  86 - L= 1.8975052 - Gamma=32.1272692 (M=  34) - s=0.0100\n",
      "  87 - L= 1.8975570 - Gamma=32.1268083 (M=  34) - s=0.0100\n",
      "  88 - L= 1.8976060 - Gamma=32.1448693 (M=  34) - s=0.0100\n",
      "  89 - L= 1.8976455 - Gamma=32.1449395 (M=  34) - s=0.0100\n",
      "  90 - L= 1.8976856 - Gamma=32.2141523 (M=  34) - s=0.0100\n",
      "  91 - L= 1.8977196 - Gamma=32.2171840 (M=  34) - s=0.0100\n",
      "  92 - L= 1.8977533 - Gamma=32.2079979 (M=  34) - s=0.0100\n",
      "  93 - L= 1.8977850 - Gamma=32.2078165 (M=  34) - s=0.0100\n",
      "  94 - L= 1.8978117 - Gamma=32.2073590 (M=  34) - s=0.0100\n",
      "  95 - L= 1.8978325 - Gamma=32.2072633 (M=  34) - s=0.0100\n",
      "  96 - L= 1.8978525 - Gamma=32.2072129 (M=  34) - s=0.0100\n",
      "  97 - L= 1.8978679 - Gamma=32.1981507 (M=  34) - s=0.0100\n",
      "  98 - L= 1.8978810 - Gamma=32.1981165 (M=  34) - s=0.0100\n",
      "  99 - L= 1.8978915 - Gamma=32.1961080 (M=  34) - s=0.0100\n",
      " 100 - L= 1.8979023 - Gamma=32.1939332 (M=  34) - s=0.0100\n",
      " 101 - L= 1.8979121 - Gamma=32.1945553 (M=  34) - s=0.0100\n",
      " 102 - L= 1.8979208 - Gamma=32.1945760 (M=  34) - s=0.0100\n",
      " 103 - L= 1.8979269 - Gamma=32.1943372 (M=  34) - s=0.0100\n",
      " 104 - L= 1.8979327 - Gamma=32.2002230 (M=  34) - s=0.0100\n",
      " 105 - L= 1.8979375 - Gamma=32.2002685 (M=  34) - s=0.0100\n",
      " 106 - L= 1.8979417 - Gamma=32.2217118 (M=  34) - s=0.0100\n",
      " 107 - L= 1.8979433 - Gamma=32.2217146 (M=  34) - s=0.0100\n",
      " 108 - L= 1.8979445 - Gamma=32.2216703 (M=  34) - s=0.0100\n",
      " 109 - L= 1.8979456 - Gamma=32.2217209 (M=  34) - s=0.0100\n",
      " 110 - L= 1.8979467 - Gamma=32.2215961 (M=  34) - s=0.0100\n",
      " 111 - L= 1.8979476 - Gamma=32.2215890 (M=  34) - s=0.0100\n",
      " 112 - L= 1.8979485 - Gamma=32.2214812 (M=  34) - s=0.0100\n",
      " 113 - L= 1.8979493 - Gamma=32.2219300 (M=  34) - s=0.0100\n",
      " 114 - L= 1.8979497 - Gamma=32.2214707 (M=  34) - s=0.0100\n",
      " 115 - L= 1.8979501 - Gamma=32.2214668 (M=  34) - s=0.0100\n",
      " 116 - L= 1.8979505 - Gamma=32.2210650 (M=  34) - s=0.0100\n",
      " 117 - L= 1.8979509 - Gamma=32.2200392 (M=  34) - s=0.0100\n",
      " 118 - L= 1.8979513 - Gamma=32.2200342 (M=  34) - s=0.0100\n",
      " 119 - L= 1.8979516 - Gamma=32.2200455 (M=  34) - s=0.0100\n",
      " 120 - L= 1.8979518 - Gamma=32.2211234 (M=  34) - s=0.0100\n",
      " 121 - L= 1.8979519 - Gamma=32.2201226 (M=  34) - s=0.0100\n",
      " 122 - L= 1.8979521 - Gamma=32.2200951 (M=  34) - s=0.0100\n",
      " 123 - L= 1.8979522 - Gamma=32.2202707 (M=  34) - s=0.0100\n",
      " 124 - L= 1.8979523 - Gamma=32.2202717 (M=  34) - s=0.0100\n",
      " 125 - L= 1.8979524 - Gamma=32.2202706 (M=  34) - s=0.0100\n",
      " 126 - L= 1.8979525 - Gamma=32.2229547 (M=  34) - s=0.0100\n",
      " 127 - L= 1.8979525 - Gamma=32.2229573 (M=  34) - s=0.0100\n",
      " 128 - L= 1.8979526 - Gamma=32.2229523 (M=  34) - s=0.0100\n",
      " 129 - L= 1.8979526 - Gamma=32.2229317 (M=  34) - s=0.0100\n",
      " 130 - L= 1.8979527 - Gamma=32.2230113 (M=  34) - s=0.0100\n",
      " 131 - L= 1.8979527 - Gamma=32.2229058 (M=  34) - s=0.0100\n",
      " 132 - L= 1.8979527 - Gamma=32.2229046 (M=  34) - s=0.0100\n",
      " 133 - L= 1.8979527 - Gamma=32.2229159 (M=  34) - s=0.0100\n",
      " 134 - L= 1.8979527 - Gamma=32.2229343 (M=  34) - s=0.0100\n",
      " 135 - L= 1.8979527 - Gamma=32.2226560 (M=  34) - s=0.0100\n",
      " 136 - L= 1.8979527 - Gamma=32.2225963 (M=  34) - s=0.0100\n",
      " 137 - L= 1.8979528 - Gamma=32.2224402 (M=  34) - s=0.0100\n",
      " 138 - L= 1.8979528 - Gamma=32.2226759 (M=  34) - s=0.0100\n",
      " 139 - L= 1.8979528 - Gamma=32.2226801 (M=  34) - s=0.0100\n",
      " 140 - L= 1.8979528 - Gamma=32.2226801 (M=  34) - s=0.0100\n",
      "Stopping at iteration 140 - max_delta_ml=2.2273215188390498e-07\n",
      "L=1.8979527727262548 - Gamma=32.222680077304844 (M=34) - s=0.01\n",
      "Initial alpha = [[0.08682708]]\n",
      "   1 - L=-524.3037736 - Gamma= 1.9999637 (M=   2) - s=0.0100\n",
      "   2 - L=-440.0073398 - Gamma= 2.9998220 (M=   3) - s=0.0100\n",
      "   3 - L=-386.0725289 - Gamma= 3.9995853 (M=   4) - s=0.0100\n",
      "   4 - L=-340.8017037 - Gamma= 4.9992351 (M=   5) - s=0.0100\n",
      "   5 - L=-298.8544933 - Gamma= 5.9989089 (M=   6) - s=0.0100\n",
      "   6 - L=-258.1444456 - Gamma= 6.9986180 (M=   7) - s=0.0100\n",
      "   7 - L=-220.1094011 - Gamma= 7.9983065 (M=   8) - s=0.0100\n",
      "   8 - L=-188.6276961 - Gamma= 8.9978591 (M=   9) - s=0.0100\n",
      "   9 - L=-163.6569220 - Gamma= 9.9973235 (M=  10) - s=0.0100\n",
      "  10 - L=-138.5041275 - Gamma=10.9968252 (M=  11) - s=0.0100\n",
      "  11 - L=-119.8599986 - Gamma=11.9960349 (M=  12) - s=0.0100\n",
      "  12 - L=-99.2813229 - Gamma=12.9951303 (M=  13) - s=0.0100\n",
      "  13 - L=-84.0204397 - Gamma=13.9942764 (M=  14) - s=0.0100\n",
      "  14 - L=-69.9932164 - Gamma=14.9931276 (M=  15) - s=0.0100\n",
      "  15 - L=-61.1388673 - Gamma=15.9916922 (M=  16) - s=0.0100\n",
      "  16 - L=-45.3855271 - Gamma=16.9902797 (M=  17) - s=0.0100\n",
      "  17 - L=-36.9571508 - Gamma=17.9880873 (M=  18) - s=0.0100\n",
      "  18 - L=-27.4946745 - Gamma=18.9865674 (M=  19) - s=0.0100\n",
      "  19 - L=-16.6178222 - Gamma=19.9850828 (M=  20) - s=0.0100\n",
      "  20 - L=-16.5369754 - Gamma=18.9863422 (M=  19) - s=0.0100\n",
      "  21 - L=-12.4506825 - Gamma=19.9831786 (M=  20) - s=0.0100\n",
      "  22 - L=-9.2918353 - Gamma=20.9793813 (M=  21) - s=0.0100\n",
      "  23 - L=-7.5675072 - Gamma=21.9723343 (M=  22) - s=0.0100\n",
      "  24 - L=-5.5283588 - Gamma=22.9660974 (M=  23) - s=0.0100\n",
      "  25 - L=-4.2465138 - Gamma=23.9562193 (M=  24) - s=0.0100\n",
      "  26 - L=-3.3196714 - Gamma=24.9432712 (M=  25) - s=0.0100\n",
      "  27 - L=-2.3097335 - Gamma=25.9297598 (M=  26) - s=0.0100\n",
      "  28 - L=-1.0173944 - Gamma=26.9167287 (M=  27) - s=0.0100\n",
      "  29 - L= 0.0083226 - Gamma=27.8976209 (M=  28) - s=0.0100\n",
      "  30 - L= 0.5154876 - Gamma=28.8749148 (M=  29) - s=0.0100\n",
      "  31 - L= 0.8583735 - Gamma=29.8174403 (M=  30) - s=0.0100\n",
      "  32 - L= 1.0343540 - Gamma=30.7650925 (M=  31) - s=0.0100\n",
      "  33 - L= 1.1375059 - Gamma=31.6843051 (M=  32) - s=0.0100\n",
      "  34 - L= 1.2290324 - Gamma=32.5840123 (M=  33) - s=0.0100\n",
      "  35 - L= 1.2928794 - Gamma=32.5857278 (M=  33) - s=0.0100\n",
      "  36 - L= 1.3556946 - Gamma=32.5882716 (M=  33) - s=0.0100\n",
      "  37 - L= 1.4088620 - Gamma=32.5891616 (M=  33) - s=0.0100\n",
      "  38 - L= 1.4572300 - Gamma=32.6006202 (M=  33) - s=0.0100\n",
      "  39 - L= 1.5008193 - Gamma=33.4472820 (M=  34) - s=0.0100\n",
      "  40 - L= 1.5350156 - Gamma=34.2493580 (M=  35) - s=0.0100\n",
      "  41 - L= 1.5635548 - Gamma=34.2505931 (M=  35) - s=0.0100\n",
      "  42 - L= 1.5892405 - Gamma=34.2283474 (M=  35) - s=0.0100\n",
      "  43 - L= 1.6031728 - Gamma=34.2306691 (M=  35) - s=0.0100\n",
      "  44 - L= 1.6135931 - Gamma=34.2324162 (M=  35) - s=0.0100\n",
      "  45 - L= 1.6213318 - Gamma=34.2474351 (M=  35) - s=0.0100\n",
      "  46 - L= 1.6282894 - Gamma=34.2478532 (M=  35) - s=0.0100\n",
      "  47 - L= 1.6350141 - Gamma=34.2495080 (M=  35) - s=0.0100\n",
      "  48 - L= 1.6394238 - Gamma=34.2476011 (M=  35) - s=0.0100\n",
      "  49 - L= 1.6430436 - Gamma=34.2457696 (M=  35) - s=0.0100\n",
      "  50 - L= 1.6460895 - Gamma=34.2620785 (M=  35) - s=0.0100\n",
      "  51 - L= 1.6491344 - Gamma=34.2628055 (M=  35) - s=0.0100\n",
      "  52 - L= 1.6517720 - Gamma=34.2676588 (M=  35) - s=0.0100\n",
      "  53 - L= 1.6545186 - Gamma=34.7196934 (M=  36) - s=0.0100\n",
      "  54 - L= 1.6570862 - Gamma=34.7195033 (M=  36) - s=0.0100\n",
      "  55 - L= 1.6594190 - Gamma=34.7160152 (M=  36) - s=0.0100\n",
      "  56 - L= 1.6615532 - Gamma=34.7140381 (M=  36) - s=0.0100\n",
      "  57 - L= 1.6632381 - Gamma=34.7508357 (M=  36) - s=0.0100\n",
      "  58 - L= 1.6648555 - Gamma=34.7512428 (M=  36) - s=0.0100\n",
      "  59 - L= 1.6663096 - Gamma=34.7476999 (M=  36) - s=0.0100\n",
      "  60 - L= 1.6673561 - Gamma=34.7492128 (M=  36) - s=0.0100\n",
      "  61 - L= 1.6683422 - Gamma=34.7668330 (M=  36) - s=0.0100\n",
      "  62 - L= 1.6690892 - Gamma=34.7733351 (M=  36) - s=0.0100\n",
      "  63 - L= 1.6697623 - Gamma=35.0459407 (M=  37) - s=0.0100\n",
      "  64 - L= 1.6703801 - Gamma=35.0874813 (M=  37) - s=0.0100\n",
      "  65 - L= 1.6708657 - Gamma=35.1078443 (M=  37) - s=0.0100\n",
      "  66 - L= 1.6711399 - Gamma=35.1136947 (M=  37) - s=0.0100\n",
      "  67 - L= 1.6714377 - Gamma=34.9717167 (M=  37) - s=0.0100\n",
      "  68 - L= 1.6716308 - Gamma=34.9718923 (M=  37) - s=0.0100\n",
      "  69 - L= 1.6717537 - Gamma=35.0328748 (M=  38) - s=0.0100\n",
      "  70 - L= 1.6718268 - Gamma=35.0311012 (M=  38) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  71 - L= 1.6718869 - Gamma=35.0311236 (M=  38) - s=0.0100\n",
      "  72 - L= 1.6719274 - Gamma=35.0356351 (M=  38) - s=0.0100\n",
      "  73 - L= 1.6719657 - Gamma=35.0368473 (M=  38) - s=0.0100\n",
      "  74 - L= 1.6720008 - Gamma=35.0898395 (M=  38) - s=0.0100\n",
      "  75 - L= 1.6720297 - Gamma=35.0884900 (M=  38) - s=0.0100\n",
      "  76 - L= 1.6720564 - Gamma=35.0958099 (M=  38) - s=0.0100\n",
      "  77 - L= 1.6720843 - Gamma=35.0968780 (M=  38) - s=0.0100\n",
      "  78 - L= 1.6721082 - Gamma=35.0970019 (M=  38) - s=0.0100\n",
      "  79 - L= 1.6721285 - Gamma=35.0964801 (M=  38) - s=0.0100\n",
      "  80 - L= 1.6721470 - Gamma=35.0964033 (M=  38) - s=0.0100\n",
      "  81 - L= 1.6721588 - Gamma=35.0966728 (M=  38) - s=0.0100\n",
      "  82 - L= 1.6721689 - Gamma=35.0968007 (M=  38) - s=0.0100\n",
      "  83 - L= 1.6721769 - Gamma=35.0958859 (M=  38) - s=0.0100\n",
      "  84 - L= 1.6721823 - Gamma=35.0957850 (M=  38) - s=0.0100\n",
      "  85 - L= 1.6721873 - Gamma=35.0955064 (M=  38) - s=0.0100\n",
      "  86 - L= 1.6721913 - Gamma=35.1128005 (M=  38) - s=0.0100\n",
      "  87 - L= 1.6721940 - Gamma=35.1128053 (M=  38) - s=0.0100\n",
      "  88 - L= 1.6721960 - Gamma=35.1127265 (M=  38) - s=0.0100\n",
      "  89 - L= 1.6721976 - Gamma=35.1133333 (M=  38) - s=0.0100\n",
      "  90 - L= 1.6721993 - Gamma=35.1200675 (M=  38) - s=0.0100\n",
      "  91 - L= 1.6722020 - Gamma=35.1165201 (M=  38) - s=0.0100\n",
      "  92 - L= 1.6722045 - Gamma=35.1244168 (M=  38) - s=0.0100\n",
      "  93 - L= 1.6722068 - Gamma=35.1211424 (M=  38) - s=0.0100\n",
      "  94 - L= 1.6722088 - Gamma=35.1281295 (M=  38) - s=0.0100\n",
      "  95 - L= 1.6722107 - Gamma=35.1269619 (M=  38) - s=0.0100\n",
      "  96 - L= 1.6722126 - Gamma=35.1239378 (M=  38) - s=0.0100\n",
      "  97 - L= 1.6722146 - Gamma=35.1308327 (M=  38) - s=0.0100\n",
      "  98 - L= 1.6722172 - Gamma=35.1447604 (M=  38) - s=0.0100\n",
      "  99 - L= 1.6722195 - Gamma=35.1412842 (M=  38) - s=0.0100\n",
      " 100 - L= 1.6722232 - Gamma=35.1501150 (M=  38) - s=0.0100\n",
      " 101 - L= 1.6722265 - Gamma=35.1458974 (M=  38) - s=0.0100\n",
      " 102 - L= 1.6722295 - Gamma=35.1536657 (M=  38) - s=0.0100\n",
      " 103 - L= 1.6722322 - Gamma=35.1497497 (M=  38) - s=0.0100\n",
      " 104 - L= 1.6722347 - Gamma=35.1566027 (M=  38) - s=0.0100\n",
      " 105 - L= 1.6722384 - Gamma=35.1729632 (M=  38) - s=0.0100\n",
      " 106 - L= 1.6722414 - Gamma=35.1687731 (M=  38) - s=0.0100\n",
      " 107 - L= 1.6722459 - Gamma=35.1777363 (M=  38) - s=0.0100\n",
      " 108 - L= 1.6722501 - Gamma=35.1726500 (M=  38) - s=0.0100\n",
      " 109 - L= 1.6722538 - Gamma=35.1805059 (M=  38) - s=0.0100\n",
      " 110 - L= 1.6722572 - Gamma=35.1757552 (M=  38) - s=0.0100\n",
      " 111 - L= 1.6722603 - Gamma=35.1826633 (M=  38) - s=0.0100\n",
      " 112 - L= 1.6722647 - Gamma=35.1999716 (M=  38) - s=0.0100\n",
      " 113 - L= 1.6722686 - Gamma=35.1947719 (M=  38) - s=0.0100\n",
      " 114 - L= 1.6722740 - Gamma=35.2036186 (M=  38) - s=0.0100\n",
      " 115 - L= 1.6722790 - Gamma=35.1974803 (M=  38) - s=0.0100\n",
      " 116 - L= 1.6722835 - Gamma=35.2052106 (M=  38) - s=0.0100\n",
      " 117 - L= 1.6722886 - Gamma=35.2062863 (M=  38) - s=0.0100\n",
      " 118 - L= 1.6722936 - Gamma=35.2043310 (M=  38) - s=0.0100\n",
      " 119 - L= 1.6722981 - Gamma=35.1983245 (M=  38) - s=0.0100\n",
      " 120 - L= 1.6723037 - Gamma=35.2066662 (M=  38) - s=0.0100\n",
      " 121 - L= 1.6723104 - Gamma=35.2273567 (M=  38) - s=0.0100\n",
      " 122 - L= 1.6723177 - Gamma=35.2193801 (M=  38) - s=0.0100\n",
      " 123 - L= 1.6723270 - Gamma=35.2295327 (M=  38) - s=0.0100\n",
      " 124 - L= 1.6723357 - Gamma=35.2204015 (M=  38) - s=0.0100\n",
      " 125 - L= 1.6723437 - Gamma=35.1957630 (M=  38) - s=0.0100\n",
      " 126 - L= 1.6723542 - Gamma=35.1911390 (M=  38) - s=0.0100\n",
      " 127 - L= 1.6723609 - Gamma=35.2110880 (M=  38) - s=0.0100\n",
      " 128 - L= 1.6723687 - Gamma=35.2199664 (M=  38) - s=0.0100\n",
      " 129 - L= 1.6723844 - Gamma=35.2069743 (M=  38) - s=0.0100\n",
      " 130 - L= 1.6723980 - Gamma=35.2179910 (M=  38) - s=0.0100\n",
      " 131 - L= 1.6724109 - Gamma=35.2053963 (M=  38) - s=0.0100\n",
      " 132 - L= 1.6724222 - Gamma=35.2148661 (M=  38) - s=0.0100\n",
      " 133 - L= 1.6724329 - Gamma=35.2027171 (M=  38) - s=0.0100\n",
      " 134 - L= 1.6724492 - Gamma=35.2517595 (M=  39) - s=0.0100\n",
      " 135 - L= 1.6724582 - Gamma=35.2496527 (M=  39) - s=0.0100\n",
      " 136 - L= 1.6724676 - Gamma=35.2725406 (M=  39) - s=0.0100\n",
      " 137 - L= 1.6724783 - Gamma=35.2429922 (M=  39) - s=0.0100\n",
      " 138 - L= 1.6724919 - Gamma=35.2288704 (M=  39) - s=0.0100\n",
      " 139 - L= 1.6725282 - Gamma=35.2445297 (M=  39) - s=0.0100\n",
      " 140 - L= 1.6725641 - Gamma=35.2193792 (M=  39) - s=0.0100\n",
      " 141 - L= 1.6725945 - Gamma=35.2326421 (M=  39) - s=0.0100\n",
      " 142 - L= 1.6726245 - Gamma=35.2073408 (M=  39) - s=0.0100\n",
      " 143 - L= 1.6726729 - Gamma=35.2861274 (M=  39) - s=0.0100\n",
      " 144 - L= 1.6727035 - Gamma=35.2777154 (M=  39) - s=0.0100\n",
      " 145 - L= 1.6727287 - Gamma=35.2767065 (M=  39) - s=0.0100\n",
      " 146 - L= 1.6727568 - Gamma=35.2511324 (M=  39) - s=0.0100\n",
      " 147 - L= 1.6728234 - Gamma=35.2694859 (M=  39) - s=0.0100\n",
      " 148 - L= 1.6728913 - Gamma=35.2295815 (M=  38) - s=0.0100\n",
      " 149 - L= 1.6729616 - Gamma=35.2871246 (M=  38) - s=0.0100\n",
      " 150 - L= 1.6730307 - Gamma=35.3753431 (M=  39) - s=0.0100\n",
      " 151 - L= 1.6732478 - Gamma=35.2259297 (M=  39) - s=0.0100\n",
      " 152 - L= 1.6733819 - Gamma=35.3395511 (M=  39) - s=0.0100\n",
      " 153 - L= 1.6734877 - Gamma=35.2383818 (M=  38) - s=0.0100\n",
      " 154 - L= 1.6736365 - Gamma=35.2168564 (M=  38) - s=0.0100\n",
      " 155 - L= 1.6737175 - Gamma=35.2978654 (M=  38) - s=0.0100\n",
      " 156 - L= 1.6737777 - Gamma=35.2919180 (M=  38) - s=0.0100\n",
      " 157 - L= 1.6738373 - Gamma=35.2844703 (M=  38) - s=0.0100\n",
      " 158 - L= 1.6738824 - Gamma=35.2988167 (M=  38) - s=0.0100\n",
      " 159 - L= 1.6739296 - Gamma=35.3414354 (M=  38) - s=0.0100\n",
      " 160 - L= 1.6739706 - Gamma=35.2674682 (M=  38) - s=0.0100\n",
      " 161 - L= 1.6740717 - Gamma=35.3874414 (M=  39) - s=0.0100\n",
      " 162 - L= 1.6741070 - Gamma=35.3361036 (M=  38) - s=0.0100\n",
      " 163 - L= 1.6741659 - Gamma=35.3986219 (M=  38) - s=0.0100\n",
      " 164 - L= 1.6742029 - Gamma=35.4013836 (M=  38) - s=0.0100\n",
      " 165 - L= 1.6742271 - Gamma=35.4296967 (M=  38) - s=0.0100\n",
      " 166 - L= 1.6742500 - Gamma=35.4286621 (M=  38) - s=0.0100\n",
      " 167 - L= 1.6742694 - Gamma=35.4249854 (M=  38) - s=0.0100\n",
      " 168 - L= 1.6742933 - Gamma=35.4787301 (M=  38) - s=0.0100\n",
      " 169 - L= 1.6743191 - Gamma=35.4686642 (M=  38) - s=0.0100\n",
      " 170 - L= 1.6743363 - Gamma=35.4678331 (M=  38) - s=0.0100\n",
      " 171 - L= 1.6743527 - Gamma=35.4682674 (M=  38) - s=0.0100\n",
      " 172 - L= 1.6743600 - Gamma=35.4830882 (M=  38) - s=0.0100\n",
      " 173 - L= 1.6743675 - Gamma=35.4828267 (M=  38) - s=0.0100\n",
      " 174 - L= 1.6743723 - Gamma=35.4872804 (M=  38) - s=0.0100\n",
      " 175 - L= 1.6743774 - Gamma=35.5048360 (M=  38) - s=0.0100\n",
      " 176 - L= 1.6743832 - Gamma=35.5023762 (M=  38) - s=0.0100\n",
      " 177 - L= 1.6743871 - Gamma=35.5028732 (M=  38) - s=0.0100\n",
      " 178 - L= 1.6743904 - Gamma=35.5030155 (M=  38) - s=0.0100\n",
      " 179 - L= 1.6743937 - Gamma=35.5030959 (M=  38) - s=0.0100\n",
      " 180 - L= 1.6743971 - Gamma=35.5031954 (M=  38) - s=0.0100\n",
      " 181 - L= 1.6744000 - Gamma=35.5016990 (M=  38) - s=0.0100\n",
      " 182 - L= 1.6744026 - Gamma=35.5187887 (M=  38) - s=0.0100\n",
      " 183 - L= 1.6744049 - Gamma=35.5187748 (M=  38) - s=0.0100\n",
      " 184 - L= 1.6744069 - Gamma=35.5187915 (M=  38) - s=0.0100\n",
      " 185 - L= 1.6744089 - Gamma=35.5187578 (M=  38) - s=0.0100\n",
      " 186 - L= 1.6744109 - Gamma=35.5185949 (M=  38) - s=0.0100\n",
      " 187 - L= 1.6744125 - Gamma=35.5185972 (M=  38) - s=0.0100\n",
      " 188 - L= 1.6744141 - Gamma=35.5159985 (M=  38) - s=0.0100\n",
      " 189 - L= 1.6744154 - Gamma=35.5162558 (M=  38) - s=0.0100\n",
      " 190 - L= 1.6744166 - Gamma=35.5220904 (M=  38) - s=0.0100\n",
      " 191 - L= 1.6744177 - Gamma=35.5220714 (M=  38) - s=0.0100\n",
      " 192 - L= 1.6744184 - Gamma=35.5282084 (M=  38) - s=0.0100\n",
      " 193 - L= 1.6744190 - Gamma=35.5284516 (M=  38) - s=0.0100\n",
      " 194 - L= 1.6744194 - Gamma=35.5284590 (M=  38) - s=0.0100\n",
      " 195 - L= 1.6744198 - Gamma=35.5284685 (M=  38) - s=0.0100\n",
      " 196 - L= 1.6744202 - Gamma=35.5283350 (M=  38) - s=0.0100\n",
      " 197 - L= 1.6744205 - Gamma=35.5283272 (M=  38) - s=0.0100\n",
      " 198 - L= 1.6744208 - Gamma=35.5341955 (M=  38) - s=0.0100\n",
      " 199 - L= 1.6744213 - Gamma=35.5336258 (M=  38) - s=0.0100\n",
      " 200 - L= 1.6744216 - Gamma=35.5334293 (M=  38) - s=0.0100\n",
      "Initial alpha = [[0.0827663]]\n",
      "   1 - L=-500.2151071 - Gamma= 1.9999602 (M=   2) - s=0.0100\n",
      "   2 - L=-409.1141939 - Gamma= 2.9998374 (M=   3) - s=0.0100\n",
      "   3 - L=-326.2258586 - Gamma= 3.9997042 (M=   4) - s=0.0100\n",
      "   4 - L=-246.8429101 - Gamma= 4.9995581 (M=   5) - s=0.0100\n",
      "   5 - L=-169.1757277 - Gamma= 5.9993996 (M=   6) - s=0.0100\n",
      "   6 - L=-153.6504095 - Gamma= 6.9986802 (M=   7) - s=0.0100\n",
      "   7 - L=-140.5929158 - Gamma= 7.9977931 (M=   8) - s=0.0100\n",
      "   8 - L=-123.7977954 - Gamma= 8.9971200 (M=   9) - s=0.0100\n",
      "   9 - L=-109.0491468 - Gamma= 9.9962917 (M=  10) - s=0.0100\n",
      "  10 - L=-90.2716528 - Gamma=10.9955212 (M=  11) - s=0.0100\n",
      "  11 - L=-76.2836309 - Gamma=11.9946804 (M=  12) - s=0.0100\n",
      "  12 - L=-58.5701701 - Gamma=12.9938757 (M=  13) - s=0.0100\n",
      "  13 - L=-48.9182939 - Gamma=13.9925957 (M=  14) - s=0.0100\n",
      "  14 - L=-40.8383559 - Gamma=14.9909497 (M=  15) - s=0.0100\n",
      "  15 - L=-32.6218823 - Gamma=15.9891440 (M=  16) - s=0.0100\n",
      "  16 - L=-27.2496190 - Gamma=16.9868212 (M=  17) - s=0.0100\n",
      "  17 - L=-22.2608997 - Gamma=17.9842246 (M=  18) - s=0.0100\n",
      "  18 - L=-17.4946884 - Gamma=18.9813460 (M=  19) - s=0.0100\n",
      "  19 - L=-10.4411705 - Gamma=19.9782526 (M=  20) - s=0.0100\n",
      "  20 - L=-7.9958808 - Gamma=20.9732802 (M=  21) - s=0.0100\n",
      "  21 - L=-5.5694085 - Gamma=21.9677115 (M=  22) - s=0.0100\n",
      "  22 - L=-2.5058608 - Gamma=22.9608845 (M=  23) - s=0.0100\n",
      "  23 - L=-1.0784554 - Gamma=23.9528241 (M=  24) - s=0.0100\n",
      "  24 - L=-0.1643066 - Gamma=24.9404104 (M=  25) - s=0.0100\n",
      "  25 - L= 0.3507421 - Gamma=25.9180810 (M=  26) - s=0.0100\n",
      "  26 - L= 0.8417688 - Gamma=26.8956240 (M=  27) - s=0.0100\n",
      "  27 - L= 1.2076510 - Gamma=27.8638565 (M=  28) - s=0.0100\n",
      "  28 - L= 1.3883582 - Gamma=28.7977718 (M=  29) - s=0.0100\n",
      "  29 - L= 1.5005737 - Gamma=29.7141471 (M=  30) - s=0.0100\n",
      "  30 - L= 1.6027464 - Gamma=30.6167972 (M=  31) - s=0.0100\n",
      "  31 - L= 1.7024397 - Gamma=30.6188137 (M=  31) - s=0.0100\n",
      "  32 - L= 1.7477332 - Gamma=31.4608475 (M=  32) - s=0.0100\n",
      "  33 - L= 1.7792774 - Gamma=31.4493077 (M=  32) - s=0.0100\n",
      "  34 - L= 1.8085330 - Gamma=30.5895963 (M=  32) - s=0.0100\n",
      "  35 - L= 1.8348018 - Gamma=30.5910777 (M=  32) - s=0.0100\n",
      "  36 - L= 1.8603935 - Gamma=30.5927684 (M=  32) - s=0.0100\n",
      "  37 - L= 1.8850220 - Gamma=30.5981140 (M=  32) - s=0.0100\n",
      "  38 - L= 1.9042218 - Gamma=31.3392593 (M=  33) - s=0.0100\n",
      "  39 - L= 1.9045031 - Gamma=31.2832759 (M=  32) - s=0.0100\n",
      "  40 - L= 1.9229694 - Gamma=31.9994344 (M=  33) - s=0.0100\n",
      "  41 - L= 1.9366683 - Gamma=32.0001664 (M=  33) - s=0.0100\n",
      "  42 - L= 1.9471238 - Gamma=32.0178524 (M=  33) - s=0.0100\n",
      "  43 - L= 1.9568575 - Gamma=32.0220471 (M=  33) - s=0.0100\n",
      "  44 - L= 1.9652824 - Gamma=32.0248398 (M=  33) - s=0.0100\n",
      "  45 - L= 1.9729242 - Gamma=32.0446183 (M=  33) - s=0.0100\n",
      "  46 - L= 1.9802414 - Gamma=31.9100529 (M=  33) - s=0.0100\n",
      "  47 - L= 1.9880958 - Gamma=32.5308181 (M=  34) - s=0.0100\n",
      "  48 - L= 1.9942192 - Gamma=32.5303557 (M=  34) - s=0.0100\n",
      "  49 - L= 1.9973102 - Gamma=32.5257062 (M=  34) - s=0.0100\n",
      "  50 - L= 1.9995798 - Gamma=32.5287693 (M=  34) - s=0.0100\n",
      "  51 - L= 2.0018292 - Gamma=32.5308736 (M=  34) - s=0.0100\n",
      "  52 - L= 2.0035055 - Gamma=32.5279479 (M=  34) - s=0.0100\n",
      "  53 - L= 2.0051779 - Gamma=32.5167312 (M=  34) - s=0.0100\n",
      "  54 - L= 2.0064241 - Gamma=32.8684297 (M=  35) - s=0.0100\n",
      "  55 - L= 2.0076296 - Gamma=32.8654174 (M=  35) - s=0.0100\n",
      "  56 - L= 2.0082748 - Gamma=32.8644324 (M=  35) - s=0.0100\n",
      "  57 - L= 2.0086555 - Gamma=32.8240705 (M=  35) - s=0.0100\n",
      "  58 - L= 2.0089447 - Gamma=32.8230281 (M=  35) - s=0.0100\n",
      "  59 - L= 2.0091671 - Gamma=32.8361574 (M=  35) - s=0.0100\n",
      "  60 - L= 2.0093200 - Gamma=32.8021398 (M=  35) - s=0.0100\n",
      "  61 - L= 2.0094918 - Gamma=32.8146481 (M=  35) - s=0.0100\n",
      "  62 - L= 2.0096422 - Gamma=32.8498538 (M=  35) - s=0.0100\n",
      "  63 - L= 2.0097870 - Gamma=32.8497676 (M=  35) - s=0.0100\n",
      "  64 - L= 2.0099316 - Gamma=32.8498268 (M=  35) - s=0.0100\n",
      "  65 - L= 2.0100622 - Gamma=32.8498783 (M=  35) - s=0.0100\n",
      "  66 - L= 2.0101830 - Gamma=32.8496107 (M=  35) - s=0.0100\n",
      "  67 - L= 2.0102728 - Gamma=32.8522628 (M=  35) - s=0.0100\n",
      "  68 - L= 2.0103587 - Gamma=32.8522265 (M=  35) - s=0.0100\n",
      "  69 - L= 2.0104368 - Gamma=32.8505437 (M=  35) - s=0.0100\n",
      "  70 - L= 2.0105123 - Gamma=32.8227164 (M=  35) - s=0.0100\n",
      "  71 - L= 2.0105795 - Gamma=32.8234775 (M=  35) - s=0.0100\n",
      "  72 - L= 2.0106430 - Gamma=32.9210926 (M=  36) - s=0.0100\n",
      "  73 - L= 2.0106944 - Gamma=32.9222116 (M=  36) - s=0.0100\n",
      "  74 - L= 2.0107371 - Gamma=32.9223029 (M=  36) - s=0.0100\n",
      "  75 - L= 2.0107596 - Gamma=32.9610800 (M=  36) - s=0.0100\n",
      "  76 - L= 2.0107860 - Gamma=32.9848459 (M=  36) - s=0.0100\n",
      "  77 - L= 2.0108163 - Gamma=32.9679144 (M=  36) - s=0.0100\n",
      "  78 - L= 2.0108310 - Gamma=32.9678337 (M=  36) - s=0.0100\n",
      "  79 - L= 2.0108427 - Gamma=32.9677257 (M=  36) - s=0.0100\n",
      "  80 - L= 2.0108515 - Gamma=32.9742030 (M=  36) - s=0.0100\n",
      "  81 - L= 2.0108605 - Gamma=32.9751294 (M=  36) - s=0.0100\n",
      "  82 - L= 2.0108713 - Gamma=32.9638053 (M=  36) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  83 - L= 2.0108876 - Gamma=33.0125858 (M=  37) - s=0.0100\n",
      "  84 - L= 2.0109020 - Gamma=33.0019551 (M=  37) - s=0.0100\n",
      "  85 - L= 2.0109108 - Gamma=33.0042926 (M=  37) - s=0.0100\n",
      "  86 - L= 2.0109182 - Gamma=32.9954702 (M=  37) - s=0.0100\n",
      "  87 - L= 2.0109241 - Gamma=32.9868513 (M=  37) - s=0.0100\n",
      "  88 - L= 2.0109315 - Gamma=32.9925065 (M=  37) - s=0.0100\n",
      "  89 - L= 2.0109400 - Gamma=33.0262917 (M=  37) - s=0.0100\n",
      "  90 - L= 2.0109477 - Gamma=33.0270901 (M=  37) - s=0.0100\n",
      "  91 - L= 2.0109549 - Gamma=33.0294312 (M=  37) - s=0.0100\n",
      "  92 - L= 2.0109617 - Gamma=33.0296850 (M=  37) - s=0.0100\n",
      "  93 - L= 2.0109674 - Gamma=33.0298853 (M=  37) - s=0.0100\n",
      "  94 - L= 2.0109722 - Gamma=33.0225827 (M=  37) - s=0.0100\n",
      "  95 - L= 2.0109777 - Gamma=33.0139550 (M=  37) - s=0.0100\n",
      "  96 - L= 2.0109834 - Gamma=33.0404330 (M=  37) - s=0.0100\n",
      "  97 - L= 2.0109881 - Gamma=33.0574300 (M=  37) - s=0.0100\n",
      "  98 - L= 2.0109930 - Gamma=33.0509339 (M=  37) - s=0.0100\n",
      "  99 - L= 2.0109988 - Gamma=33.0615376 (M=  37) - s=0.0100\n",
      " 100 - L= 2.0110035 - Gamma=33.0658976 (M=  37) - s=0.0100\n",
      " 101 - L= 2.0110075 - Gamma=33.0659084 (M=  37) - s=0.0100\n",
      " 102 - L= 2.0110113 - Gamma=33.0659909 (M=  37) - s=0.0100\n",
      " 103 - L= 2.0110148 - Gamma=33.0596070 (M=  37) - s=0.0100\n",
      " 104 - L= 2.0110198 - Gamma=33.0511198 (M=  37) - s=0.0100\n",
      " 105 - L= 2.0110234 - Gamma=33.0509960 (M=  37) - s=0.0100\n",
      " 106 - L= 2.0110270 - Gamma=33.0717014 (M=  37) - s=0.0100\n",
      " 107 - L= 2.0110315 - Gamma=33.0722785 (M=  37) - s=0.0100\n",
      " 108 - L= 2.0110353 - Gamma=33.0737500 (M=  37) - s=0.0100\n",
      " 109 - L= 2.0110385 - Gamma=33.0733872 (M=  37) - s=0.0100\n",
      " 110 - L= 2.0110410 - Gamma=33.0732127 (M=  37) - s=0.0100\n",
      " 111 - L= 2.0110430 - Gamma=33.0682207 (M=  37) - s=0.0100\n",
      " 112 - L= 2.0110454 - Gamma=33.0695290 (M=  37) - s=0.0100\n",
      " 113 - L= 2.0110479 - Gamma=33.0634896 (M=  37) - s=0.0100\n",
      " 114 - L= 2.0110504 - Gamma=33.0802525 (M=  37) - s=0.0100\n",
      " 115 - L= 2.0110541 - Gamma=33.0840201 (M=  37) - s=0.0100\n",
      " 116 - L= 2.0110578 - Gamma=33.0782512 (M=  37) - s=0.0100\n",
      " 117 - L= 2.0110597 - Gamma=33.0887044 (M=  37) - s=0.0100\n",
      " 118 - L= 2.0110614 - Gamma=33.0886359 (M=  37) - s=0.0100\n",
      " 119 - L= 2.0110631 - Gamma=33.0835841 (M=  37) - s=0.0100\n",
      " 120 - L= 2.0110655 - Gamma=33.0779822 (M=  37) - s=0.0100\n",
      " 121 - L= 2.0110677 - Gamma=33.0934076 (M=  37) - s=0.0100\n",
      " 122 - L= 2.0110698 - Gamma=33.0937872 (M=  37) - s=0.0100\n",
      " 123 - L= 2.0110714 - Gamma=33.0939063 (M=  37) - s=0.0100\n",
      " 124 - L= 2.0110725 - Gamma=33.0984074 (M=  37) - s=0.0100\n",
      " 125 - L= 2.0110736 - Gamma=33.0942018 (M=  37) - s=0.0100\n",
      " 126 - L= 2.0110749 - Gamma=33.0963846 (M=  37) - s=0.0100\n",
      " 127 - L= 2.0110761 - Gamma=33.0925171 (M=  37) - s=0.0100\n",
      " 128 - L= 2.0110773 - Gamma=33.0933233 (M=  37) - s=0.0100\n",
      " 129 - L= 2.0110786 - Gamma=33.1049186 (M=  37) - s=0.0100\n",
      " 130 - L= 2.0110804 - Gamma=33.1008126 (M=  37) - s=0.0100\n",
      " 131 - L= 2.0110815 - Gamma=33.0965390 (M=  37) - s=0.0100\n",
      " 132 - L= 2.0110825 - Gamma=33.0965366 (M=  37) - s=0.0100\n",
      " 133 - L= 2.0110833 - Gamma=33.0964775 (M=  37) - s=0.0100\n",
      " 134 - L= 2.0110840 - Gamma=33.0971881 (M=  37) - s=0.0100\n",
      " 135 - L= 2.0110847 - Gamma=33.0941746 (M=  37) - s=0.0100\n",
      " 136 - L= 2.0110855 - Gamma=33.1033058 (M=  37) - s=0.0100\n",
      " 137 - L= 2.0110865 - Gamma=33.1051159 (M=  37) - s=0.0100\n",
      " 138 - L= 2.0110875 - Gamma=33.1053710 (M=  37) - s=0.0100\n",
      " 139 - L= 2.0110882 - Gamma=33.1017604 (M=  37) - s=0.0100\n",
      " 140 - L= 2.0110891 - Gamma=33.1086189 (M=  37) - s=0.0100\n",
      " 141 - L= 2.0110897 - Gamma=33.1084494 (M=  37) - s=0.0100\n",
      " 142 - L= 2.0110904 - Gamma=33.1084323 (M=  37) - s=0.0100\n",
      " 143 - L= 2.0110910 - Gamma=33.1084264 (M=  37) - s=0.0100\n",
      " 144 - L= 2.0110916 - Gamma=33.1083064 (M=  37) - s=0.0100\n",
      " 145 - L= 2.0110922 - Gamma=33.1082390 (M=  37) - s=0.0100\n",
      " 146 - L= 2.0110927 - Gamma=33.1082359 (M=  37) - s=0.0100\n",
      " 147 - L= 2.0110933 - Gamma=33.1082965 (M=  37) - s=0.0100\n",
      " 148 - L= 2.0110938 - Gamma=33.1152922 (M=  37) - s=0.0100\n",
      " 149 - L= 2.0110945 - Gamma=33.1122216 (M=  37) - s=0.0100\n",
      " 150 - L= 2.0110952 - Gamma=33.1095908 (M=  37) - s=0.0100\n",
      " 151 - L= 2.0110958 - Gamma=33.1062993 (M=  37) - s=0.0100\n",
      " 152 - L= 2.0110963 - Gamma=33.1093879 (M=  37) - s=0.0100\n",
      " 153 - L= 2.0110968 - Gamma=33.1094174 (M=  37) - s=0.0100\n",
      " 154 - L= 2.0110973 - Gamma=33.1093930 (M=  37) - s=0.0100\n",
      " 155 - L= 2.0110977 - Gamma=33.1106131 (M=  37) - s=0.0100\n",
      " 156 - L= 2.0110982 - Gamma=33.1171379 (M=  37) - s=0.0100\n",
      " 157 - L= 2.0110986 - Gamma=33.1176315 (M=  37) - s=0.0100\n",
      " 158 - L= 2.0110991 - Gamma=33.1149689 (M=  37) - s=0.0100\n",
      " 159 - L= 2.0110996 - Gamma=33.1151483 (M=  37) - s=0.0100\n",
      " 160 - L= 2.0111001 - Gamma=33.1123009 (M=  37) - s=0.0100\n",
      " 161 - L= 2.0111006 - Gamma=33.1128687 (M=  37) - s=0.0100\n",
      " 162 - L= 2.0111010 - Gamma=33.1129267 (M=  37) - s=0.0100\n",
      " 163 - L= 2.0111014 - Gamma=33.1192830 (M=  37) - s=0.0100\n",
      " 164 - L= 2.0111019 - Gamma=33.1110959 (M=  37) - s=0.0100\n",
      " 165 - L= 2.0111023 - Gamma=33.1110876 (M=  37) - s=0.0100\n",
      " 166 - L= 2.0111026 - Gamma=33.1092544 (M=  37) - s=0.0100\n",
      " 167 - L= 2.0111029 - Gamma=33.1092496 (M=  37) - s=0.0100\n",
      " 168 - L= 2.0111031 - Gamma=33.1092473 (M=  37) - s=0.0100\n",
      " 169 - L= 2.0111034 - Gamma=33.1092992 (M=  37) - s=0.0100\n",
      " 170 - L= 2.0111036 - Gamma=33.1101434 (M=  37) - s=0.0100\n",
      " 171 - L= 2.0111038 - Gamma=33.1147017 (M=  37) - s=0.0100\n",
      " 172 - L= 2.0111040 - Gamma=33.1126624 (M=  37) - s=0.0100\n",
      " 173 - L= 2.0111044 - Gamma=33.1103911 (M=  37) - s=0.0100\n",
      " 174 - L= 2.0111047 - Gamma=33.1103571 (M=  37) - s=0.0100\n",
      " 175 - L= 2.0111049 - Gamma=33.1140155 (M=  37) - s=0.0100\n",
      " 176 - L= 2.0111051 - Gamma=33.1139618 (M=  37) - s=0.0100\n",
      " 177 - L= 2.0111054 - Gamma=33.1140807 (M=  37) - s=0.0100\n",
      " 178 - L= 2.0111055 - Gamma=33.1143868 (M=  37) - s=0.0100\n",
      " 179 - L= 2.0111057 - Gamma=33.1127544 (M=  37) - s=0.0100\n",
      " 180 - L= 2.0111059 - Gamma=33.1173842 (M=  37) - s=0.0100\n",
      " 181 - L= 2.0111061 - Gamma=33.1177552 (M=  37) - s=0.0100\n",
      " 182 - L= 2.0111063 - Gamma=33.1185302 (M=  37) - s=0.0100\n",
      " 183 - L= 2.0111065 - Gamma=33.1171346 (M=  37) - s=0.0100\n",
      " 184 - L= 2.0111067 - Gamma=33.1154664 (M=  37) - s=0.0100\n",
      " 185 - L= 2.0111069 - Gamma=33.1172334 (M=  37) - s=0.0100\n",
      " 186 - L= 2.0111070 - Gamma=33.1154207 (M=  37) - s=0.0100\n",
      " 187 - L= 2.0111072 - Gamma=33.1153335 (M=  37) - s=0.0100\n",
      " 188 - L= 2.0111074 - Gamma=33.1194996 (M=  37) - s=0.0100\n",
      " 189 - L= 2.0111075 - Gamma=33.1195879 (M=  37) - s=0.0100\n",
      " 190 - L= 2.0111077 - Gamma=33.1195860 (M=  37) - s=0.0100\n",
      " 191 - L= 2.0111078 - Gamma=33.1183472 (M=  37) - s=0.0100\n",
      " 192 - L= 2.0111079 - Gamma=33.1189308 (M=  37) - s=0.0100\n",
      " 193 - L= 2.0111080 - Gamma=33.1174987 (M=  37) - s=0.0100\n",
      " 194 - L= 2.0111081 - Gamma=33.1164940 (M=  37) - s=0.0100\n",
      " 195 - L= 2.0111082 - Gamma=33.1199304 (M=  37) - s=0.0100\n",
      " 196 - L= 2.0111083 - Gamma=33.1199621 (M=  37) - s=0.0100\n",
      " 197 - L= 2.0111084 - Gamma=33.1222416 (M=  37) - s=0.0100\n",
      " 198 - L= 2.0111085 - Gamma=33.1222514 (M=  37) - s=0.0100\n",
      " 199 - L= 2.0111086 - Gamma=33.1222530 (M=  37) - s=0.0100\n",
      " 200 - L= 2.0111087 - Gamma=33.1224557 (M=  37) - s=0.0100\n",
      "Initial alpha = [[0.08389369]]\n",
      "   1 - L=-503.2923765 - Gamma= 1.9999610 (M=   2) - s=0.0100\n",
      "   2 - L=-391.4748208 - Gamma= 2.9998616 (M=   3) - s=0.0100\n",
      "   3 - L=-320.6656629 - Gamma= 3.9996993 (M=   4) - s=0.0100\n",
      "   4 - L=-279.4727981 - Gamma= 4.9994307 (M=   5) - s=0.0100\n",
      "   5 - L=-227.0618258 - Gamma= 5.9991276 (M=   6) - s=0.0100\n",
      "   6 - L=-183.6814364 - Gamma= 6.9987627 (M=   7) - s=0.0100\n",
      "   7 - L=-155.9603522 - Gamma= 7.9982615 (M=   8) - s=0.0100\n",
      "   8 - L=-126.0962585 - Gamma= 8.9977962 (M=   9) - s=0.0100\n",
      "   9 - L=-109.6093459 - Gamma= 9.9971093 (M=  10) - s=0.0100\n",
      "  10 - L=-93.5596139 - Gamma=10.9963471 (M=  11) - s=0.0100\n",
      "  11 - L=-75.5833507 - Gamma=11.9955331 (M=  12) - s=0.0100\n",
      "  12 - L=-65.1021770 - Gamma=12.9943180 (M=  13) - s=0.0100\n",
      "  13 - L=-49.2617414 - Gamma=13.9932365 (M=  14) - s=0.0100\n",
      "  14 - L=-39.5108079 - Gamma=14.9918588 (M=  15) - s=0.0100\n",
      "  15 - L=-27.5471824 - Gamma=15.9906362 (M=  16) - s=0.0100\n",
      "  16 - L=-19.3021390 - Gamma=16.9890628 (M=  17) - s=0.0100\n",
      "  17 - L=-15.7242325 - Gamma=17.9854606 (M=  18) - s=0.0100\n",
      "  18 - L=-11.8670222 - Gamma=18.9815765 (M=  19) - s=0.0100\n",
      "  19 - L=-9.4150778 - Gamma=19.9756028 (M=  20) - s=0.0100\n",
      "  20 - L=-7.5084059 - Gamma=20.9671585 (M=  21) - s=0.0100\n",
      "  21 - L=-5.9111311 - Gamma=21.9597085 (M=  22) - s=0.0100\n",
      "  22 - L=-4.2763934 - Gamma=22.9514275 (M=  23) - s=0.0100\n",
      "  23 - L=-3.2521654 - Gamma=23.9410205 (M=  24) - s=0.0100\n",
      "  24 - L=-2.1319272 - Gamma=24.9288030 (M=  25) - s=0.0100\n",
      "  25 - L=-1.4307270 - Gamma=25.9087802 (M=  26) - s=0.0100\n",
      "  26 - L=-0.7691887 - Gamma=26.8876495 (M=  27) - s=0.0100\n",
      "  27 - L= 0.0241460 - Gamma=27.8683641 (M=  28) - s=0.0100\n",
      "  28 - L= 0.5859063 - Gamma=28.8444632 (M=  29) - s=0.0100\n",
      "  29 - L= 0.9231409 - Gamma=29.8120487 (M=  30) - s=0.0100\n",
      "  30 - L= 1.3073541 - Gamma=30.7764041 (M=  31) - s=0.0100\n",
      "  31 - L= 1.5064072 - Gamma=31.7232136 (M=  32) - s=0.0100\n",
      "  32 - L= 1.5590493 - Gamma=32.5872778 (M=  33) - s=0.0100\n",
      "  33 - L= 1.6072993 - Gamma=32.5946984 (M=  33) - s=0.0100\n",
      "  34 - L= 1.6445946 - Gamma=32.5983093 (M=  33) - s=0.0100\n",
      "  35 - L= 1.6805569 - Gamma=33.4266371 (M=  34) - s=0.0100\n",
      "  36 - L= 1.7029505 - Gamma=33.4339517 (M=  34) - s=0.0100\n",
      "  37 - L= 1.7227566 - Gamma=33.4346164 (M=  34) - s=0.0100\n",
      "  38 - L= 1.7389101 - Gamma=33.4314551 (M=  34) - s=0.0100\n",
      "  39 - L= 1.7520662 - Gamma=34.1156191 (M=  35) - s=0.0100\n",
      "  40 - L= 1.7657311 - Gamma=34.1234648 (M=  35) - s=0.0100\n",
      "  41 - L= 1.7761089 - Gamma=34.1325469 (M=  35) - s=0.0100\n",
      "  42 - L= 1.7853476 - Gamma=34.1345361 (M=  35) - s=0.0100\n",
      "  43 - L= 1.7944091 - Gamma=34.1338125 (M=  35) - s=0.0100\n",
      "  44 - L= 1.8029037 - Gamma=34.1358599 (M=  35) - s=0.0100\n",
      "  45 - L= 1.8103681 - Gamma=34.1366266 (M=  35) - s=0.0100\n",
      "  46 - L= 1.8168238 - Gamma=34.1331018 (M=  35) - s=0.0100\n",
      "  47 - L= 1.8230785 - Gamma=34.1466159 (M=  35) - s=0.0100\n",
      "  48 - L= 1.8282600 - Gamma=34.1462409 (M=  35) - s=0.0100\n",
      "  49 - L= 1.8333324 - Gamma=34.1605990 (M=  35) - s=0.0100\n",
      "  50 - L= 1.8379417 - Gamma=34.1303643 (M=  35) - s=0.0100\n",
      "  51 - L= 1.8415284 - Gamma=34.1245111 (M=  35) - s=0.0100\n",
      "  52 - L= 1.8449135 - Gamma=34.1428306 (M=  35) - s=0.0100\n",
      "  53 - L= 1.8477456 - Gamma=34.1515497 (M=  35) - s=0.0100\n",
      "  54 - L= 1.8501414 - Gamma=34.1657010 (M=  35) - s=0.0100\n",
      "  55 - L= 1.8522185 - Gamma=34.1666202 (M=  35) - s=0.0100\n",
      "  56 - L= 1.8538986 - Gamma=34.1668978 (M=  35) - s=0.0100\n",
      "  57 - L= 1.8551780 - Gamma=34.1677786 (M=  35) - s=0.0100\n",
      "  58 - L= 1.8563407 - Gamma=34.1600614 (M=  35) - s=0.0100\n",
      "  59 - L= 1.8570551 - Gamma=34.1596324 (M=  35) - s=0.0100\n",
      "  60 - L= 1.8577759 - Gamma=34.4402714 (M=  36) - s=0.0100\n",
      "  61 - L= 1.8584650 - Gamma=34.4399114 (M=  36) - s=0.0100\n",
      "  62 - L= 1.8590170 - Gamma=34.3750304 (M=  36) - s=0.0100\n",
      "  63 - L= 1.8595295 - Gamma=34.3765685 (M=  36) - s=0.0100\n",
      "  64 - L= 1.8599907 - Gamma=34.3785938 (M=  36) - s=0.0100\n",
      "  65 - L= 1.8604284 - Gamma=34.3622126 (M=  36) - s=0.0100\n",
      "  66 - L= 1.8608242 - Gamma=34.3619277 (M=  36) - s=0.0100\n",
      "  67 - L= 1.8609988 - Gamma=34.3821243 (M=  36) - s=0.0100\n",
      "  68 - L= 1.8611404 - Gamma=34.4217648 (M=  36) - s=0.0100\n",
      "  69 - L= 1.8612545 - Gamma=34.4214473 (M=  36) - s=0.0100\n",
      "  70 - L= 1.8612853 - Gamma=34.4200786 (M=  36) - s=0.0100\n",
      "  71 - L= 1.8613174 - Gamma=34.4857659 (M=  37) - s=0.0100\n",
      "  72 - L= 1.8613649 - Gamma=34.4180673 (M=  37) - s=0.0100\n",
      "  73 - L= 1.8613869 - Gamma=34.4321399 (M=  37) - s=0.0100\n",
      "  74 - L= 1.8614278 - Gamma=34.5012281 (M=  37) - s=0.0100\n",
      "  75 - L= 1.8614604 - Gamma=34.4973589 (M=  37) - s=0.0100\n",
      "  76 - L= 1.8614962 - Gamma=34.4778984 (M=  37) - s=0.0100\n",
      "  77 - L= 1.8615265 - Gamma=34.4198911 (M=  37) - s=0.0100\n",
      "  78 - L= 1.8615590 - Gamma=34.4356947 (M=  37) - s=0.0100\n",
      "  79 - L= 1.8616096 - Gamma=34.5065218 (M=  37) - s=0.0100\n",
      "  80 - L= 1.8616490 - Gamma=34.5007330 (M=  37) - s=0.0100\n",
      "  81 - L= 1.8616936 - Gamma=34.4247856 (M=  37) - s=0.0100\n",
      "  82 - L= 1.8617572 - Gamma=34.5189577 (M=  38) - s=0.0100\n",
      "  83 - L= 1.8618225 - Gamma=34.5921912 (M=  38) - s=0.0100\n",
      "  84 - L= 1.8618953 - Gamma=34.5357618 (M=  37) - s=0.0100\n",
      "  85 - L= 1.8619851 - Gamma=34.5594119 (M=  37) - s=0.0100\n",
      "  86 - L= 1.8620658 - Gamma=34.5268571 (M=  37) - s=0.0100\n",
      "  87 - L= 1.8621541 - Gamma=34.6256580 (M=  37) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  88 - L= 1.8622599 - Gamma=34.7087931 (M=  37) - s=0.0100\n",
      "  89 - L= 1.8623409 - Gamma=34.7062913 (M=  37) - s=0.0100\n",
      "  90 - L= 1.8623796 - Gamma=34.6814532 (M=  37) - s=0.0100\n",
      "  91 - L= 1.8624282 - Gamma=34.6762295 (M=  37) - s=0.0100\n",
      "  92 - L= 1.8624704 - Gamma=34.7376961 (M=  37) - s=0.0100\n",
      "  93 - L= 1.8625140 - Gamma=34.7359391 (M=  37) - s=0.0100\n",
      "  94 - L= 1.8625538 - Gamma=34.7501565 (M=  37) - s=0.0100\n",
      "  95 - L= 1.8625966 - Gamma=34.7434869 (M=  37) - s=0.0100\n",
      "  96 - L= 1.8626501 - Gamma=34.7962367 (M=  37) - s=0.0100\n",
      "  97 - L= 1.8626926 - Gamma=34.7952844 (M=  37) - s=0.0100\n",
      "  98 - L= 1.8627340 - Gamma=34.7672566 (M=  37) - s=0.0100\n",
      "  99 - L= 1.8627641 - Gamma=34.7680440 (M=  37) - s=0.0100\n",
      " 100 - L= 1.8627942 - Gamma=34.8158135 (M=  37) - s=0.0100\n",
      " 101 - L= 1.8628181 - Gamma=34.8160952 (M=  37) - s=0.0100\n",
      " 102 - L= 1.8628409 - Gamma=34.8173198 (M=  37) - s=0.0100\n",
      " 103 - L= 1.8628623 - Gamma=34.8174971 (M=  37) - s=0.0100\n",
      " 104 - L= 1.8628800 - Gamma=34.8175761 (M=  37) - s=0.0100\n",
      " 105 - L= 1.8628934 - Gamma=34.8253446 (M=  37) - s=0.0100\n",
      " 106 - L= 1.8629099 - Gamma=34.8527461 (M=  37) - s=0.0100\n",
      " 107 - L= 1.8629276 - Gamma=34.8493182 (M=  37) - s=0.0100\n",
      " 108 - L= 1.8629486 - Gamma=34.8277844 (M=  37) - s=0.0100\n",
      " 109 - L= 1.8629661 - Gamma=34.8265019 (M=  37) - s=0.0100\n",
      " 110 - L= 1.8629820 - Gamma=34.8590745 (M=  37) - s=0.0100\n",
      " 111 - L= 1.8629948 - Gamma=34.8593853 (M=  37) - s=0.0100\n",
      " 112 - L= 1.8630057 - Gamma=34.8593342 (M=  37) - s=0.0100\n",
      " 113 - L= 1.8630152 - Gamma=34.8591365 (M=  37) - s=0.0100\n",
      " 114 - L= 1.8630243 - Gamma=34.8558541 (M=  37) - s=0.0100\n",
      " 115 - L= 1.8630324 - Gamma=34.8741147 (M=  37) - s=0.0100\n",
      " 116 - L= 1.8630446 - Gamma=34.8811980 (M=  37) - s=0.0100\n",
      " 117 - L= 1.8630550 - Gamma=34.8652953 (M=  37) - s=0.0100\n",
      " 118 - L= 1.8630801 - Gamma=34.9129381 (M=  38) - s=0.0100\n",
      " 119 - L= 1.8630993 - Gamma=34.9064465 (M=  38) - s=0.0100\n",
      " 120 - L= 1.8631149 - Gamma=34.9053415 (M=  38) - s=0.0100\n",
      " 121 - L= 1.8631259 - Gamma=34.8884046 (M=  38) - s=0.0100\n",
      " 122 - L= 1.8631427 - Gamma=34.9198771 (M=  38) - s=0.0100\n",
      " 123 - L= 1.8631634 - Gamma=34.9603767 (M=  38) - s=0.0100\n",
      " 124 - L= 1.8631968 - Gamma=34.9711526 (M=  38) - s=0.0100\n",
      " 125 - L= 1.8632293 - Gamma=34.9645679 (M=  38) - s=0.0100\n",
      " 126 - L= 1.8632630 - Gamma=34.9330311 (M=  38) - s=0.0100\n",
      " 127 - L= 1.8633368 - Gamma=35.0024731 (M=  38) - s=0.0100\n",
      " 128 - L= 1.8633863 - Gamma=35.0001743 (M=  38) - s=0.0100\n",
      " 129 - L= 1.8634307 - Gamma=34.9943501 (M=  38) - s=0.0100\n",
      " 130 - L= 1.8634823 - Gamma=34.9522537 (M=  38) - s=0.0100\n",
      " 131 - L= 1.8635454 - Gamma=35.0076146 (M=  38) - s=0.0100\n",
      " 132 - L= 1.8636056 - Gamma=35.0205961 (M=  38) - s=0.0100\n",
      " 133 - L= 1.8637048 - Gamma=35.0918997 (M=  38) - s=0.0100\n",
      " 134 - L= 1.8638331 - Gamma=35.0162455 (M=  38) - s=0.0100\n",
      " 135 - L= 1.8639764 - Gamma=34.9999353 (M=  38) - s=0.0100\n",
      " 136 - L= 1.8640966 - Gamma=35.0680067 (M=  38) - s=0.0100\n",
      " 137 - L= 1.8642148 - Gamma=35.0840914 (M=  38) - s=0.0100\n",
      " 138 - L= 1.8643307 - Gamma=35.0009476 (M=  38) - s=0.0100\n",
      " 139 - L= 1.8644662 - Gamma=35.0712321 (M=  38) - s=0.0100\n",
      " 140 - L= 1.8645955 - Gamma=35.0677859 (M=  38) - s=0.0100\n",
      " 141 - L= 1.8647242 - Gamma=35.0633395 (M=  38) - s=0.0100\n",
      " 142 - L= 1.8648865 - Gamma=35.1317953 (M=  38) - s=0.0100\n",
      " 143 - L= 1.8652164 - Gamma=34.9574584 (M=  38) - s=0.0100\n",
      " 144 - L= 1.8655583 - Gamma=34.9226984 (M=  38) - s=0.0100\n",
      " 145 - L= 1.8655653 - Gamma=34.9148474 (M=  37) - s=0.0100\n",
      " 146 - L= 1.8658964 - Gamma=34.8953290 (M=  37) - s=0.0100\n",
      " 147 - L= 1.8661787 - Gamma=34.9712996 (M=  37) - s=0.0100\n",
      " 148 - L= 1.8665189 - Gamma=34.9940496 (M=  37) - s=0.0100\n",
      " 149 - L= 1.8668808 - Gamma=34.9564658 (M=  37) - s=0.0100\n",
      " 150 - L= 1.8671240 - Gamma=34.9601440 (M=  37) - s=0.0100\n",
      " 151 - L= 1.8673086 - Gamma=34.9532973 (M=  37) - s=0.0100\n",
      " 152 - L= 1.8674794 - Gamma=35.0297113 (M=  37) - s=0.0100\n",
      " 153 - L= 1.8676445 - Gamma=34.9985547 (M=  37) - s=0.0100\n",
      " 154 - L= 1.8677825 - Gamma=35.0419055 (M=  37) - s=0.0100\n",
      " 155 - L= 1.8679105 - Gamma=35.0377790 (M=  37) - s=0.0100\n",
      " 156 - L= 1.8680295 - Gamma=35.0391666 (M=  37) - s=0.0100\n",
      " 157 - L= 1.8681217 - Gamma=35.0399421 (M=  37) - s=0.0100\n",
      " 158 - L= 1.8682061 - Gamma=35.0386283 (M=  37) - s=0.0100\n",
      " 159 - L= 1.8682798 - Gamma=35.0848474 (M=  37) - s=0.0100\n",
      " 160 - L= 1.8683361 - Gamma=35.0928410 (M=  37) - s=0.0100\n",
      " 161 - L= 1.8683889 - Gamma=35.0923600 (M=  37) - s=0.0100\n",
      " 162 - L= 1.8684416 - Gamma=35.0924719 (M=  37) - s=0.0100\n",
      " 163 - L= 1.8684875 - Gamma=35.0734062 (M=  37) - s=0.0100\n",
      " 164 - L= 1.8685385 - Gamma=35.0692976 (M=  37) - s=0.0100\n",
      " 165 - L= 1.8685767 - Gamma=35.0614319 (M=  37) - s=0.0100\n",
      " 166 - L= 1.8686107 - Gamma=35.0616059 (M=  37) - s=0.0100\n",
      " 167 - L= 1.8686424 - Gamma=35.0801048 (M=  37) - s=0.0100\n",
      " 168 - L= 1.8686803 - Gamma=35.0657584 (M=  37) - s=0.0100\n",
      " 169 - L= 1.8687037 - Gamma=35.0659174 (M=  37) - s=0.0100\n",
      " 170 - L= 1.8687268 - Gamma=35.0658484 (M=  37) - s=0.0100\n",
      " 171 - L= 1.8687494 - Gamma=35.0657787 (M=  37) - s=0.0100\n",
      " 172 - L= 1.8687703 - Gamma=35.0659932 (M=  37) - s=0.0100\n",
      " 173 - L= 1.8687841 - Gamma=35.0659212 (M=  37) - s=0.0100\n",
      " 174 - L= 1.8687977 - Gamma=35.0656606 (M=  37) - s=0.0100\n",
      " 175 - L= 1.8688112 - Gamma=35.0545247 (M=  37) - s=0.0100\n",
      " 176 - L= 1.8688263 - Gamma=35.0583249 (M=  37) - s=0.0100\n",
      " 177 - L= 1.8688390 - Gamma=35.0583516 (M=  37) - s=0.0100\n",
      " 178 - L= 1.8688510 - Gamma=35.0590972 (M=  37) - s=0.0100\n",
      " 179 - L= 1.8688622 - Gamma=35.0938691 (M=  38) - s=0.0100\n",
      " 180 - L= 1.8688728 - Gamma=35.0939089 (M=  38) - s=0.0100\n",
      " 181 - L= 1.8688822 - Gamma=35.0920409 (M=  38) - s=0.0100\n",
      " 182 - L= 1.8688910 - Gamma=35.0826701 (M=  38) - s=0.0100\n",
      " 183 - L= 1.8688975 - Gamma=35.0905694 (M=  38) - s=0.0100\n",
      " 184 - L= 1.8689040 - Gamma=35.0905607 (M=  38) - s=0.0100\n",
      " 185 - L= 1.8689105 - Gamma=35.0905754 (M=  38) - s=0.0100\n",
      " 186 - L= 1.8689164 - Gamma=35.0906184 (M=  38) - s=0.0100\n",
      " 187 - L= 1.8689219 - Gamma=35.0897045 (M=  38) - s=0.0100\n",
      " 188 - L= 1.8689272 - Gamma=35.0840447 (M=  38) - s=0.0100\n",
      " 189 - L= 1.8689324 - Gamma=35.0840096 (M=  38) - s=0.0100\n",
      " 190 - L= 1.8689371 - Gamma=35.0860298 (M=  38) - s=0.0100\n",
      " 191 - L= 1.8689408 - Gamma=35.0857365 (M=  38) - s=0.0100\n",
      " 192 - L= 1.8689433 - Gamma=35.1019683 (M=  38) - s=0.0100\n",
      " 193 - L= 1.8689490 - Gamma=35.0941310 (M=  38) - s=0.0100\n",
      " 194 - L= 1.8689526 - Gamma=35.0929539 (M=  38) - s=0.0100\n",
      " 195 - L= 1.8689554 - Gamma=35.0930822 (M=  38) - s=0.0100\n",
      " 196 - L= 1.8689580 - Gamma=35.0909442 (M=  38) - s=0.0100\n",
      " 197 - L= 1.8689605 - Gamma=35.0909939 (M=  38) - s=0.0100\n",
      " 198 - L= 1.8689630 - Gamma=35.0996033 (M=  38) - s=0.0100\n",
      " 199 - L= 1.8689654 - Gamma=35.0997861 (M=  38) - s=0.0100\n",
      " 200 - L= 1.8689677 - Gamma=35.0997953 (M=  38) - s=0.0100\n",
      "MODEL: RVM accuracy:  0.603448275862069 +/-: 0.007584765610924944\n",
      "Initial alpha = [[0.06840021]]\n",
      "   1 - L=-537.4949064 - Gamma= 1.9999687 (M=   2) - s=0.0100\n",
      "   2 - L=-446.2890006 - Gamma= 2.9998698 (M=   3) - s=0.0100\n",
      "   3 - L=-399.8816895 - Gamma= 3.9996757 (M=   4) - s=0.0100\n",
      "   4 - L=-360.4874975 - Gamma= 4.9994486 (M=   5) - s=0.0100\n",
      "   5 - L=-307.5760044 - Gamma= 5.9992488 (M=   6) - s=0.0100\n",
      "   6 - L=-273.5576140 - Gamma= 6.9989250 (M=   7) - s=0.0100\n",
      "   7 - L=-251.8009845 - Gamma= 7.9985029 (M=   8) - s=0.0100\n",
      "   8 - L=-223.3668098 - Gamma= 8.9981140 (M=   9) - s=0.0100\n",
      "   9 - L=-203.3858794 - Gamma= 9.9974848 (M=  10) - s=0.0100\n",
      "  10 - L=-188.1411013 - Gamma=10.9968317 (M=  11) - s=0.0100\n",
      "  11 - L=-168.6422328 - Gamma=11.9962526 (M=  12) - s=0.0100\n",
      "  12 - L=-139.9957114 - Gamma=12.9958206 (M=  13) - s=0.0100\n",
      "  13 - L=-117.8217573 - Gamma=13.9952952 (M=  14) - s=0.0100\n",
      "  14 - L=-103.2725413 - Gamma=14.9944848 (M=  15) - s=0.0100\n",
      "  15 - L=-88.4672290 - Gamma=15.9935122 (M=  16) - s=0.0100\n",
      "  16 - L=-76.4226547 - Gamma=16.9924064 (M=  17) - s=0.0100\n",
      "  17 - L=-64.5151145 - Gamma=17.9913639 (M=  18) - s=0.0100\n",
      "  18 - L=-56.7888350 - Gamma=18.9900706 (M=  19) - s=0.0100\n",
      "  19 - L=-47.1094225 - Gamma=19.9886702 (M=  20) - s=0.0100\n",
      "  20 - L=-41.0558028 - Gamma=20.9866593 (M=  21) - s=0.0100\n",
      "  21 - L=-34.4807505 - Gamma=21.9846701 (M=  22) - s=0.0100\n",
      "  22 - L=-30.8617160 - Gamma=22.9818612 (M=  23) - s=0.0100\n",
      "  23 - L=-26.4687800 - Gamma=23.9792960 (M=  24) - s=0.0100\n",
      "  24 - L=-21.2441267 - Gamma=24.9767095 (M=  25) - s=0.0100\n",
      "  25 - L=-17.3616471 - Gamma=25.9737376 (M=  26) - s=0.0100\n",
      "  26 - L=-15.6046318 - Gamma=26.9651101 (M=  27) - s=0.0100\n",
      "  27 - L=-13.5699679 - Gamma=27.9559291 (M=  28) - s=0.0100\n",
      "  28 - L=-12.0243838 - Gamma=28.9497737 (M=  29) - s=0.0100\n",
      "  29 - L=-11.9819204 - Gamma=27.9632711 (M=  28) - s=0.0100\n",
      "  30 - L=-10.3288273 - Gamma=28.9574062 (M=  29) - s=0.0100\n",
      "  31 - L=-7.1727752 - Gamma=29.9534221 (M=  30) - s=0.0100\n",
      "  32 - L=-5.5677593 - Gamma=30.9464412 (M=  31) - s=0.0100\n",
      "  33 - L=-3.9516981 - Gamma=31.9398783 (M=  32) - s=0.0100\n",
      "  34 - L=-2.8925844 - Gamma=32.9302835 (M=  33) - s=0.0100\n",
      "  35 - L=-2.8373920 - Gamma=31.9353753 (M=  32) - s=0.0100\n",
      "  36 - L=-1.9241842 - Gamma=32.9256429 (M=  33) - s=0.0100\n",
      "  37 - L=-0.6628478 - Gamma=33.9145541 (M=  34) - s=0.0100\n",
      "  38 - L=-0.0192573 - Gamma=34.8942836 (M=  35) - s=0.0100\n",
      "  39 - L= 0.3130401 - Gamma=35.8601762 (M=  36) - s=0.0100\n",
      "  40 - L= 0.8150436 - Gamma=36.8371047 (M=  37) - s=0.0100\n",
      "  41 - L= 1.0008985 - Gamma=37.7886110 (M=  38) - s=0.0100\n",
      "  42 - L= 1.0947704 - Gamma=38.7167451 (M=  39) - s=0.0100\n",
      "  43 - L= 1.1825768 - Gamma=38.7184818 (M=  39) - s=0.0100\n",
      "  44 - L= 1.2408844 - Gamma=39.6075844 (M=  40) - s=0.0100\n",
      "  45 - L= 1.3005470 - Gamma=39.6090327 (M=  40) - s=0.0100\n",
      "  46 - L= 1.3609141 - Gamma=40.4879304 (M=  41) - s=0.0100\n",
      "  47 - L= 1.3937050 - Gamma=40.4890966 (M=  41) - s=0.0100\n",
      "  48 - L= 1.4173136 - Gamma=41.2928333 (M=  42) - s=0.0100\n",
      "  49 - L= 1.4589421 - Gamma=41.9507411 (M=  43) - s=0.0100\n",
      "  50 - L= 1.4759940 - Gamma=41.9477880 (M=  43) - s=0.0100\n",
      "  51 - L= 1.4926625 - Gamma=41.9590304 (M=  43) - s=0.0100\n",
      "  52 - L= 1.5063139 - Gamma=41.9648700 (M=  43) - s=0.0100\n",
      "  53 - L= 1.5195001 - Gamma=41.9684944 (M=  43) - s=0.0100\n",
      "  54 - L= 1.5307451 - Gamma=41.9697072 (M=  43) - s=0.0100\n",
      "  55 - L= 1.5418358 - Gamma=41.9726257 (M=  43) - s=0.0100\n",
      "  56 - L= 1.5529096 - Gamma=41.9741354 (M=  43) - s=0.0100\n",
      "  57 - L= 1.5640705 - Gamma=42.6790706 (M=  44) - s=0.0100\n",
      "  58 - L= 1.5745115 - Gamma=42.6799861 (M=  44) - s=0.0100\n",
      "  59 - L= 1.5835990 - Gamma=42.6839117 (M=  44) - s=0.0100\n",
      "  60 - L= 1.5925572 - Gamma=42.6648943 (M=  44) - s=0.0100\n",
      "  61 - L= 1.6012769 - Gamma=42.6709559 (M=  44) - s=0.0100\n",
      "  62 - L= 1.6092347 - Gamma=42.6199779 (M=  44) - s=0.0100\n",
      "  63 - L= 1.6169478 - Gamma=42.6239129 (M=  44) - s=0.0100\n",
      "  64 - L= 1.6246318 - Gamma=42.6250044 (M=  44) - s=0.0100\n",
      "  65 - L= 1.6313824 - Gamma=42.6259601 (M=  44) - s=0.0100\n",
      "  66 - L= 1.6366864 - Gamma=42.6129047 (M=  44) - s=0.0100\n",
      "  67 - L= 1.6418490 - Gamma=42.6140891 (M=  44) - s=0.0100\n",
      "  68 - L= 1.6462457 - Gamma=42.6132563 (M=  44) - s=0.0100\n",
      "  69 - L= 1.6506948 - Gamma=43.1430270 (M=  45) - s=0.0100\n",
      "  70 - L= 1.6546372 - Gamma=43.2088677 (M=  45) - s=0.0100\n",
      "  71 - L= 1.6583308 - Gamma=43.2082457 (M=  45) - s=0.0100\n",
      "  72 - L= 1.6617308 - Gamma=43.2180484 (M=  45) - s=0.0100\n",
      "  73 - L= 1.6650737 - Gamma=43.2188583 (M=  45) - s=0.0100\n",
      "  74 - L= 1.6682210 - Gamma=43.2611746 (M=  45) - s=0.0100\n",
      "  75 - L= 1.6712064 - Gamma=43.2462965 (M=  45) - s=0.0100\n",
      "  76 - L= 1.6740659 - Gamma=43.2567461 (M=  45) - s=0.0100\n",
      "  77 - L= 1.6767580 - Gamma=43.2717665 (M=  45) - s=0.0100\n",
      "  78 - L= 1.6795046 - Gamma=43.2770754 (M=  45) - s=0.0100\n",
      "  79 - L= 1.6819548 - Gamma=43.2782023 (M=  45) - s=0.0100\n",
      "  80 - L= 1.6843949 - Gamma=43.2785709 (M=  45) - s=0.0100\n",
      "  81 - L= 1.6866365 - Gamma=43.2774029 (M=  45) - s=0.0100\n",
      "  82 - L= 1.6888449 - Gamma=43.7018442 (M=  46) - s=0.0100\n",
      "  83 - L= 1.6909266 - Gamma=43.7024828 (M=  46) - s=0.0100\n",
      "  84 - L= 1.6920809 - Gamma=43.7755630 (M=  46) - s=0.0100\n",
      "  85 - L= 1.6932134 - Gamma=43.7779184 (M=  46) - s=0.0100\n",
      "  86 - L= 1.6940060 - Gamma=43.7226852 (M=  46) - s=0.0100\n",
      "  87 - L= 1.6945604 - Gamma=43.7185755 (M=  46) - s=0.0100\n",
      "  88 - L= 1.6950434 - Gamma=43.7180614 (M=  46) - s=0.0100\n",
      "  89 - L= 1.6954778 - Gamma=43.7014755 (M=  46) - s=0.0100\n",
      "  90 - L= 1.6959957 - Gamma=43.9547762 (M=  47) - s=0.0100\n",
      "  91 - L= 1.6963629 - Gamma=43.9805187 (M=  47) - s=0.0100\n",
      "  92 - L= 1.6965460 - Gamma=44.0300231 (M=  47) - s=0.0100\n",
      "  93 - L= 1.6966186 - Gamma=44.0316491 (M=  47) - s=0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94 - L= 1.6966801 - Gamma=44.0730682 (M=  47) - s=0.0100\n",
      "  95 - L= 1.6967034 - Gamma=44.0639381 (M=  47) - s=0.0100\n",
      "  96 - L= 1.6967237 - Gamma=44.0612173 (M=  47) - s=0.0100\n",
      "  97 - L= 1.6967459 - Gamma=44.1063870 (M=  47) - s=0.0100\n",
      "  98 - L= 1.6967668 - Gamma=44.1050795 (M=  47) - s=0.0100\n",
      "  99 - L= 1.6967860 - Gamma=44.1117281 (M=  47) - s=0.0100\n",
      " 100 - L= 1.6968093 - Gamma=44.0995731 (M=  47) - s=0.0100\n",
      " 101 - L= 1.6968257 - Gamma=44.0954071 (M=  47) - s=0.0100\n",
      " 102 - L= 1.6968401 - Gamma=44.0962366 (M=  47) - s=0.0100\n",
      " 103 - L= 1.6968540 - Gamma=44.0961576 (M=  47) - s=0.0100\n",
      " 104 - L= 1.6968653 - Gamma=44.0930258 (M=  47) - s=0.0100\n",
      " 105 - L= 1.6968763 - Gamma=44.0928284 (M=  47) - s=0.0100\n",
      " 106 - L= 1.6968842 - Gamma=44.0928558 (M=  47) - s=0.0100\n",
      " 107 - L= 1.6968918 - Gamma=44.0927690 (M=  47) - s=0.0100\n",
      " 108 - L= 1.6968993 - Gamma=44.0905296 (M=  47) - s=0.0100\n",
      " 109 - L= 1.6969065 - Gamma=44.0895646 (M=  47) - s=0.0100\n",
      " 110 - L= 1.6969130 - Gamma=44.0863667 (M=  47) - s=0.0100\n",
      " 111 - L= 1.6969199 - Gamma=44.0899498 (M=  47) - s=0.0100\n",
      " 112 - L= 1.6969276 - Gamma=44.1036836 (M=  47) - s=0.0100\n",
      " 113 - L= 1.6969364 - Gamma=44.1245095 (M=  47) - s=0.0100\n",
      " 114 - L= 1.6969493 - Gamma=44.1146862 (M=  47) - s=0.0100\n",
      " 115 - L= 1.6969574 - Gamma=44.1182513 (M=  47) - s=0.0100\n",
      " 116 - L= 1.6969648 - Gamma=44.1128653 (M=  47) - s=0.0100\n",
      " 117 - L= 1.6969719 - Gamma=44.1093637 (M=  47) - s=0.0100\n",
      " 118 - L= 1.6969777 - Gamma=44.1207900 (M=  47) - s=0.0100\n",
      " 119 - L= 1.6969870 - Gamma=44.1119265 (M=  47) - s=0.0100\n",
      " 120 - L= 1.6969981 - Gamma=44.1343039 (M=  47) - s=0.0100\n",
      " 121 - L= 1.6970076 - Gamma=44.1378276 (M=  47) - s=0.0100\n",
      " 122 - L= 1.6970141 - Gamma=44.1362114 (M=  47) - s=0.0100\n",
      " 123 - L= 1.6970207 - Gamma=44.1282505 (M=  47) - s=0.0100\n",
      " 124 - L= 1.6970294 - Gamma=44.1241944 (M=  47) - s=0.0100\n",
      " 125 - L= 1.6970394 - Gamma=44.1384737 (M=  47) - s=0.0100\n",
      " 126 - L= 1.6970458 - Gamma=44.1384252 (M=  47) - s=0.0100\n",
      " 127 - L= 1.6970520 - Gamma=44.1384604 (M=  47) - s=0.0100\n",
      " 128 - L= 1.6970583 - Gamma=44.1545585 (M=  47) - s=0.0100\n",
      " 129 - L= 1.6970659 - Gamma=44.1574640 (M=  47) - s=0.0100\n",
      " 130 - L= 1.6970797 - Gamma=44.1451144 (M=  47) - s=0.0100\n",
      " 131 - L= 1.6970860 - Gamma=44.1441659 (M=  47) - s=0.0100\n",
      " 132 - L= 1.6970916 - Gamma=44.1407970 (M=  47) - s=0.0100\n",
      " 133 - L= 1.6970977 - Gamma=44.1387118 (M=  47) - s=0.0100\n",
      " 134 - L= 1.6971030 - Gamma=44.1339875 (M=  47) - s=0.0100\n",
      " 135 - L= 1.6971085 - Gamma=44.1418589 (M=  47) - s=0.0100\n",
      " 136 - L= 1.6971142 - Gamma=44.1441456 (M=  47) - s=0.0100\n",
      " 137 - L= 1.6971208 - Gamma=44.1552433 (M=  47) - s=0.0100\n",
      " 138 - L= 1.6971315 - Gamma=44.1436476 (M=  47) - s=0.0100\n",
      " 139 - L= 1.6971456 - Gamma=44.1664376 (M=  47) - s=0.0100\n",
      " 140 - L= 1.6971529 - Gamma=44.1624127 (M=  47) - s=0.0100\n",
      " 141 - L= 1.6971609 - Gamma=44.1648736 (M=  47) - s=0.0100\n",
      " 142 - L= 1.6971708 - Gamma=44.1530052 (M=  47) - s=0.0100\n",
      " 143 - L= 1.6971811 - Gamma=44.1661851 (M=  47) - s=0.0100\n",
      " 144 - L= 1.6971877 - Gamma=44.1667300 (M=  47) - s=0.0100\n",
      " 145 - L= 1.6971938 - Gamma=44.1658077 (M=  47) - s=0.0100\n",
      " 146 - L= 1.6971999 - Gamma=44.1801270 (M=  47) - s=0.0100\n",
      " 147 - L= 1.6972061 - Gamma=44.1785073 (M=  47) - s=0.0100\n",
      " 148 - L= 1.6972123 - Gamma=44.1804848 (M=  47) - s=0.0100\n",
      " 149 - L= 1.6972236 - Gamma=44.1669889 (M=  47) - s=0.0100\n",
      " 150 - L= 1.6972377 - Gamma=44.1610865 (M=  47) - s=0.0100\n",
      " 151 - L= 1.6972450 - Gamma=44.1553445 (M=  47) - s=0.0100\n",
      " 152 - L= 1.6972515 - Gamma=44.1554531 (M=  47) - s=0.0100\n",
      " 153 - L= 1.6972579 - Gamma=44.1554238 (M=  47) - s=0.0100\n",
      " 154 - L= 1.6972638 - Gamma=44.1650380 (M=  47) - s=0.0100\n",
      " 155 - L= 1.6972709 - Gamma=44.1797433 (M=  47) - s=0.0100\n",
      " 156 - L= 1.6972817 - Gamma=44.1658240 (M=  47) - s=0.0100\n",
      " 157 - L= 1.6973019 - Gamma=44.1687342 (M=  47) - s=0.0100\n",
      " 158 - L= 1.6973106 - Gamma=44.1681189 (M=  47) - s=0.0100\n",
      " 159 - L= 1.6973179 - Gamma=44.1636835 (M=  47) - s=0.0100\n",
      " 160 - L= 1.6973288 - Gamma=44.1487553 (M=  47) - s=0.0100\n",
      " 161 - L= 1.6973405 - Gamma=44.1615600 (M=  47) - s=0.0100\n",
      " 162 - L= 1.6973527 - Gamma=44.1799484 (M=  47) - s=0.0100\n",
      " 163 - L= 1.6973604 - Gamma=44.1775364 (M=  47) - s=0.0100\n",
      " 164 - L= 1.6973682 - Gamma=44.1791306 (M=  47) - s=0.0100\n",
      " 165 - L= 1.6973826 - Gamma=44.1607428 (M=  47) - s=0.0100\n",
      " 166 - L= 1.6974004 - Gamma=44.1534011 (M=  47) - s=0.0100\n",
      " 167 - L= 1.6974080 - Gamma=44.1534780 (M=  47) - s=0.0100\n",
      " 168 - L= 1.6974156 - Gamma=44.1474052 (M=  47) - s=0.0100\n",
      " 169 - L= 1.6974227 - Gamma=44.1606981 (M=  47) - s=0.0100\n",
      " 170 - L= 1.6974312 - Gamma=44.1458191 (M=  47) - s=0.0100\n",
      " 171 - L= 1.6974478 - Gamma=44.1476392 (M=  47) - s=0.0100\n",
      " 172 - L= 1.6974633 - Gamma=44.1614686 (M=  47) - s=0.0100\n",
      " 173 - L= 1.6974749 - Gamma=44.1429655 (M=  47) - s=0.0100\n",
      " 174 - L= 1.6974911 - Gamma=44.1355451 (M=  47) - s=0.0100\n",
      " 175 - L= 1.6975055 - Gamma=44.1329645 (M=  47) - s=0.0100\n",
      " 176 - L= 1.6975181 - Gamma=44.1497786 (M=  47) - s=0.0100\n",
      " 177 - L= 1.6975280 - Gamma=44.1318319 (M=  47) - s=0.0100\n",
      " 178 - L= 1.6975472 - Gamma=44.1333064 (M=  47) - s=0.0100\n",
      " 179 - L= 1.6975584 - Gamma=44.1443741 (M=  47) - s=0.0100\n",
      " 180 - L= 1.6975701 - Gamma=44.1235263 (M=  47) - s=0.0100\n",
      " 181 - L= 1.6975909 - Gamma=44.1145316 (M=  47) - s=0.0100\n",
      " 182 - L= 1.6976040 - Gamma=44.1307195 (M=  47) - s=0.0100\n",
      " 183 - L= 1.6976164 - Gamma=44.1226441 (M=  47) - s=0.0100\n",
      " 184 - L= 1.6976326 - Gamma=44.0967251 (M=  47) - s=0.0100\n",
      " 185 - L= 1.6976552 - Gamma=44.0978176 (M=  47) - s=0.0100\n",
      " 186 - L= 1.6976674 - Gamma=44.1087324 (M=  47) - s=0.0100\n",
      " 187 - L= 1.6976815 - Gamma=44.0827122 (M=  47) - s=0.0100\n",
      " 188 - L= 1.6977101 - Gamma=44.0713159 (M=  47) - s=0.0100\n",
      " 189 - L= 1.6977281 - Gamma=44.0674505 (M=  47) - s=0.0100\n",
      " 190 - L= 1.6977460 - Gamma=44.0851624 (M=  47) - s=0.0100\n",
      " 191 - L= 1.6977661 - Gamma=44.0520681 (M=  47) - s=0.0100\n",
      " 192 - L= 1.6977909 - Gamma=44.0527447 (M=  47) - s=0.0100\n",
      " 193 - L= 1.6978123 - Gamma=44.0517509 (M=  47) - s=0.0100\n",
      " 194 - L= 1.6978317 - Gamma=44.0500162 (M=  47) - s=0.0100\n",
      " 195 - L= 1.6978513 - Gamma=44.0398025 (M=  47) - s=0.0100\n",
      " 196 - L= 1.6978837 - Gamma=43.9938184 (M=  47) - s=0.0100\n",
      " 197 - L= 1.6979244 - Gamma=44.0119907 (M=  47) - s=0.0100\n",
      " 198 - L= 1.6979483 - Gamma=44.0310428 (M=  47) - s=0.0100\n",
      " 199 - L= 1.6979717 - Gamma=44.0189714 (M=  47) - s=0.0100\n",
      " 200 - L= 1.6980210 - Gamma=43.9561790 (M=  47) - s=0.0100\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.6101 - acc: 0.6739 - val_loss: 0.3417 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.3105 - acc: 0.8913 - val_loss: 0.1315 - val_acc: 0.9167\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.2787 - acc: 0.9130 - val_loss: 0.0224 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.1146 - acc: 0.9783 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.0277 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 4.1732e-04 - acc: 1.0000 - val_loss: 8.8188e-04 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 4.1894e-04 - acc: 1.0000 - val_loss: 1.3324e-04 - val_acc: 1.0000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s - loss: 7.7608e-04 - acc: 1.0000 - val_loss: 7.0263e-05 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 9.7936e-05 - acc: 1.0000 - val_loss: 5.9799e-05 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 6.7088e-05 - acc: 1.0000 - val_loss: 4.8490e-05 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 3.6821e-05 - acc: 1.0000 - val_loss: 4.2759e-05 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 7.9419e-05 - acc: 1.0000 - val_loss: 2.0328e-05 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 7.8363e-05 - acc: 1.0000 - val_loss: 1.4724e-05 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 2.0996e-05 - acc: 1.0000 - val_loss: 1.1779e-05 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 6.1079e-05 - acc: 1.0000 - val_loss: 3.6481e-06 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 1.7383e-05 - acc: 1.0000 - val_loss: 3.2626e-06 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 5.0099e-06 - acc: 1.0000 - val_loss: 3.2357e-06 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 2.7379e-06 - acc: 1.0000 - val_loss: 2.9388e-06 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 2.9290e-06 - acc: 1.0000 - val_loss: 2.4207e-06 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 6.8876e-06 - acc: 1.0000 - val_loss: 2.0967e-06 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 1.0951e-06 - acc: 1.0000 - val_loss: 1.9198e-06 - val_acc: 1.0000\n",
      "MODEL: DNN accuracy:  1.0 +/-: 0.0\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  Creating X,y\n",
      "++++++++++++++++++++++++++++++  RESULTS FOR CLASSIFICATION WITH GENOMIC DATA ++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ ..processing feature array (58, 230, 1) and class vector (58,)\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.7195 - acc: 0.4565 - val_loss: 0.6196 - val_acc: 0.4167\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.5431 - acc: 0.7609 - val_loss: 0.2827 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.4902 - acc: 0.7391 - val_loss: 0.2513 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.3528 - acc: 0.8696 - val_loss: 0.1667 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.3053 - acc: 0.8478 - val_loss: 0.0993 - val_acc: 1.0000\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.2216 - acc: 0.9130 - val_loss: 0.6882 - val_acc: 0.6667\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.3775 - acc: 0.7609 - val_loss: 0.2683 - val_acc: 0.8333\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.1456 - acc: 0.9565 - val_loss: 0.4066 - val_acc: 0.7500\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.1599 - acc: 0.9130 - val_loss: 0.2218 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.1422 - acc: 0.9130 - val_loss: 0.3011 - val_acc: 0.8333\n",
      "Train on 46 samples, validate on 12 samples\n",
      "Epoch 1/5\n",
      "46/46 [==============================] - 0s - loss: 0.1931 - acc: 0.9565 - val_loss: 0.3256 - val_acc: 0.7500\n",
      "Epoch 2/5\n",
      "46/46 [==============================] - 0s - loss: 0.2372 - acc: 0.8913 - val_loss: 0.0798 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "46/46 [==============================] - 0s - loss: 0.2049 - acc: 0.9130 - val_loss: 0.0633 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "46/46 [==============================] - 0s - loss: 0.1133 - acc: 0.9565 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "46/46 [==============================] - 0s - loss: 0.1925 - acc: 0.9348 - val_loss: 0.0523 - val_acc: 1.0000\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9091\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.1347 - acc: 0.9574 - val_loss: 0.1402 - val_acc: 0.9091\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.0365 - acc: 1.0000 - val_loss: 0.1545 - val_acc: 0.9091\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.0786 - acc: 0.9787 - val_loss: 0.1698 - val_acc: 0.9091\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.0350 - acc: 1.0000 - val_loss: 0.1866 - val_acc: 0.9091\n",
      "Train on 47 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 0s - loss: 0.0860 - acc: 0.9574 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 0s - loss: 0.0410 - acc: 0.9787 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 0s - loss: 0.0747 - acc: 0.9787 - val_loss: 0.0086 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 0s - loss: 0.0390 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 0s - loss: 0.0480 - acc: 0.9787 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "MODEL: CNN accuracy:  0.9482758620689655 +/-: 0.004639138111195184\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "Rocket.X_GENOME = None\n",
    "Rocket.Y_CLASS = None\n",
    "Rocket.PREP_HASH = None\n",
    "\n",
    "RUNS, MODELS, ACC = Rocket.run_classification(method_list = METHOD_LIST, \n",
    "                          num_run = nruns,\n",
    "                          pipeline = {\"scaler\": {\"type\": SCALER},\n",
    "                                      \"pre_processing\": PREPROC_DICT,\n",
    "                                      \"feature_selection\": FSELECT_DICT, # mannwhitney         \n",
    "                                      \"dim_reduction\": DIMRED_DICT},\n",
    "                          parameters = {}, \n",
    "                          features = 'genomic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on average: 0.8275862068965518 +- 0.01633111735668216, median: 0.8189655172413794+-0.015052426764674092\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "        acc model       var\n",
      "0  0.810345    RF  0.021763\n",
      "1  0.724138   XGB  0.032442\n",
      "2  0.810345  LGBM  0.016539\n",
      "3  0.793103    ET  0.037001\n",
      "4  0.827586   SVM  0.014683\n",
      "5  0.862069    LR  0.013237\n",
      "6  0.896552  MLNN  0.015422\n",
      "7  0.603448   RVM  0.007585\n",
      "8  1.000000   DNN  0.000000\n",
      "9  0.948276   CNN  0.004639\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on average: {} +- {}, median: {}+-{}\".format(ACC.mean()[0], ACC.mean()[1], ACC.median()[0], ACC.median()[1]))\n",
    "print(\"+\"*40)\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "###########\n",
    "##Runs.append(AllResults)\n",
    "final_df = pandas.DataFrame()\n",
    "\n",
    "for idx, df in enumerate(RUNS):    \n",
    "    df['run'] = idx\n",
    "    final_df = final_df.append(df, ignore_index = True)\n",
    "final_df[Rocket.MODEL_PARAMETERS['ID']] = final_df[Rocket.MODEL_PARAMETERS['ID']].astype(str)\n",
    "final_df = final_df.sort_values(by=Rocket.MODEL_PARAMETERS['ID'])\n",
    "final_df['pred']= pandas.to_numeric(final_df['pred'])\n",
    "final_df_agg = final_df.groupby([Rocket.MODEL_PARAMETERS['ID'], 'method']).agg({'pred': [numpy.mean, numpy.median, numpy.std]})\n",
    "final_df_agg = final_df_agg['pred'].groupby(by=Rocket.MODEL_PARAMETERS['ID']).agg({'mean': [numpy.mean, numpy.median, numpy.std]})['mean']\n",
    "final_df.to_csv(\"out/patient_results_\"+Rocket.SET_NAME+\"_FDR0025.csv\")\n",
    "final_df_agg.to_csv(\"out/patient_results_agg_\"+Rocket.SET_NAME+\"_FDR0025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_weights, top_coeffs = _helpers.get_top_genes(MODELS=MODELS, n_max=5000, RexR=Rocket)\n",
    "#top_coeffs.to_csv(\"out/coeffs_\"+Rocket.SET_NAME+\"_FDR_alpha0025.csv\")\n",
    "#top_weights.to_csv(\"out/weights_\"+Rocket.SET_NAME+\"_FDR_alpha0025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting probesets to genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_helpers._probeset_mapper took 4.52 seconds to finish\n"
     ]
    }
   ],
   "source": [
    "probe_list = top_weights.index.tolist()\n",
    "probeset_col = 'Probe Set ID'\n",
    "desc_list = ['Pathway','Gene Title', 'Gene Symbol', 'Chromosomal Location']\n",
    "probeset_to_genome_map = _helpers._probeset_mapper(probeset_type = 'HG-U133',                                                    \n",
    "                                                   mapping_file = None,\n",
    "                                                   probeset_col = probeset_col,\n",
    "                                                   description_list = desc_list,\n",
    "                                                   probe_list = probe_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the top genomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "#from scipy.dspatial.distance import cosine\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cdist\n",
    "from decimal import Decimal\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from time import time\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TransPosed = Rocket.DATA_all_samples.T # all microarrays, may be multiple per patient versus all probesets, may be multiple per genome\n",
    "Normal = Rocket.DATA_merged_processed.loc[:, (Rocket.DATA_merged_processed.columns !='target') & \n",
    "                                             (Rocket.DATA_merged_processed.columns !='ID')]\n",
    "#AllNormal = Rocket.DATA_merged\n",
    "#probeset_weights = Rocket.get_probeset_weights(method = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 9827_corr2.CEL, dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'cosine', normalised = True, inflation = 2, minkowski_dim=1)\n",
    "##### apply Markov clustering\n",
    "#######################\n",
    "# non-distributed, non-sparse version, only for small-sized problems (N is order 1000)\n",
    "e = 2\n",
    "r = 2 \n",
    "epsilon = 1e-7\n",
    "convergence = 0.001\n",
    "num_iter = 10\n",
    "Orientation = 'col' # columnwise or rowwise\n",
    "\n",
    "# add loop\n",
    "def add_loop(df_matrix, value=0): \n",
    "    for i in df_matrix.index:\n",
    "        df_matrix.loc[i, i] = value\n",
    "    return df_matrix\n",
    "patient_sim = add_loop(patient_sim, 1)\n",
    "patient_sim = patient_sim - epsilon\n",
    "\n",
    "def normalise(sim, type = 'col'):\n",
    "    if(type == 'col'):\n",
    "        # column normalisation\n",
    "        for variable in sim.keys():\n",
    "            col_vec = sim[variable]\n",
    "            sum_val = sum([p for p in col_vec])\n",
    "            sim[variable] = sim[variable]/sum_val\n",
    "    elif (type == 'row'):\n",
    "        # row normalisation\n",
    "        for variable in sim.keys():\n",
    "            row_vec = sim.loc[variable, :]\n",
    "            sum_val = sum([p for p in row_vec])\n",
    "            sim.loc[variable,:] = sim.loc[variable,:]/sum_val\n",
    "    return sim\n",
    "\n",
    "# step E: expansion, get the nth power of the matrix\n",
    "def expansion(sim):\n",
    "    X = numpy.array(sim)\n",
    "    VarList = sim.keys()\n",
    "    if e == 1:\n",
    "        return sim\n",
    "    elif e > 1:        \n",
    "        return pandas.DataFrame(numpy.linalg.matrix_power(X, e), index = VarList, columns = VarList)\n",
    "     \n",
    "# step I: inflation, per column raise by rth power and column normalise\n",
    "def inflation(sim, type = 'col'):    \n",
    "    if type == 'col':\n",
    "        Axis = 0\n",
    "    elif type == 'row':\n",
    "        Axis = 1\n",
    "    return sim.apply(lambda x: x**r/sum(x**r), axis = Axis)\n",
    "\n",
    "# remove weak connections, values < epsilon\n",
    "def clean(sim):\n",
    "    return sim.applymap(lambda x:0 if x<epsilon else x)\n",
    "    \n",
    "def difference(old, new):\n",
    "    # relative zeroes over entire array\n",
    "    #return (new.apply(lambda x: numpy.ceil(x-epsilon)) - old.apply(lambda x: numpy.ceil(x-epsilon))).sum().sum()/len(old)**2    \n",
    "    return abs(new - old).sum().sum()/len(old)**2    \n",
    "\n",
    "#patient_sim = normalise(patient_sim, type = Orientation)\n",
    "_sim_a = patient_sim\n",
    "for i in range(0,num_iter):\n",
    "    # repeat E and I until convergence, the row-wise elements form the clusters.\n",
    "    _sim_b = clean(inflation(expansion(_sim_a), type = Orientation))\n",
    "    _sim_a = normalise(_sim_a, type = Orientation)\n",
    "    #if ((difference(_sim_a, _sim_b)) < convergence) & (i>0):\n",
    "    #    print(difference(_sim_a, _sim_b))\n",
    "    #    print(\"CONVERGED after \", i, \" iterations\")\n",
    "    #    break;\n",
    "    _sim_a = _sim_b\n",
    "\n",
    "result_mcl = clean(_sim_b)\n",
    "result_mcl.loc[result_mcl.loc['9827_corr2.CEL',:]>epsilon, '9827_corr2.CEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 patient clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "patient_sim = _helpers.patient_similarity(Normal, sim_type = 'pearson', normalised = False, inflation=1, minkowski_dim=1)\n",
    "##### apply Affinity Propagation\n",
    "#######################\n",
    "X = numpy.array(patient_sim)\n",
    "af = AffinityPropagation(preference=-10).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "patient_clusters = patient_sim.keys()[cluster_centers_indices].values\n",
    "patient_cluster_members = af.labels_\n",
    "print(\"There are {} patient clusters\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AggResults = Rocket.DATA_merged\n",
    "AggResults = _helpers._preprocess(AggResults, Rclass = Rocket)\n",
    "#AggResults = _helpers._group_patients(AggResults, method = 'mean')\n",
    "AggResults['cluster_ap'] = patient_cluster_members\n",
    "\n",
    "#AggResults.groupby(['Treatment risk group in ALL10', 'cluster_ap']).agg({'Microarray file': pandas.Series.nunique})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\pandas\\core\\frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "AggResults['FU_RFS'] = pandas.to_numeric(AggResults['FU_RFS'])\n",
    "AggResults['FU_EFS'] = pandas.to_numeric(AggResults['FU_EFS'])\n",
    "AggResults['FU_OS'] = pandas.to_numeric(AggResults['FU_OS'])\n",
    "AggResults['WhiteBloodCellcount'] = pandas.to_numeric(AggResults['WhiteBloodCellcount'])\n",
    "AggResults['Age'] = pandas.to_numeric(AggResults['Age'])\n",
    "AggResults['Gender'] = pandas.to_numeric(AggResults['Gender'])\n",
    "AggResults['code_RFS']= pandas.to_numeric(AggResults['code_RFS'])\n",
    "AggResults['code_EFS']= pandas.to_numeric(AggResults['code_EFS'])\n",
    "AggResults['code_OS']= pandas.to_numeric(AggResults['code_OS'])\n",
    "\n",
    "AggResults['mutations_NOTCH_pathway'] = pandas.to_numeric(AggResults['mutations_NOTCH_pathway'])\n",
    "AggResults['mutations_PTEN_AKT_pathway'] = pandas.to_numeric(AggResults['mutations_PTEN_AKT_pathway'])\n",
    "AggResults['mutations_IL7R_pathway'] = pandas.to_numeric(AggResults['mutations_IL7R_pathway'])\n",
    "#AggResults.replace(to_replace=9999, value=0.5, inplace=True)\n",
    "AggResults[['mutations_NOTCH_pathway', \n",
    "            'mutations_PTEN_AKT_pathway', \n",
    "            'mutations_IL7R_pathway']] = AggResults[['mutations_NOTCH_pathway', \n",
    "                                                    'mutations_PTEN_AKT_pathway', \n",
    "                                                    'mutations_IL7R_pathway']].replace([9999],[numpy.nan],\n",
    "                                                                                       inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "AggResults['comb_mutations_NOTCH_IL7R'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_NOTCH_PTEN'] = AggResults['mutations_NOTCH_pathway'] + AggResults['mutations_PTEN_AKT_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN'] =  AggResults['mutations_PTEN_AKT_pathway'] + AggResults['mutations_IL7R_pathway']\n",
    "AggResults['comb_mutations_IL7R_PTEN_NOTCH'] =  AggResults['mutations_PTEN_AKT_pathway']\\\n",
    "                                                + AggResults['mutations_IL7R_pathway']\\\n",
    "                                                + AggResults['mutations_NOTCH_pathway']\n",
    "\n",
    "\n",
    "patient_count = AggResults.groupby(['cluster_ap']).agg({'labnr_patient': pandas.Series.nunique})\n",
    "Clustered_by_patients_whitebloodcells = AggResults[AggResults['WhiteBloodCellcount'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'WhiteBloodCellcount': numpy.mean,\n",
    "    'Age': numpy.mean, \n",
    "    'Gender': numpy.mean})\n",
    "\n",
    "# Cancer_gene\n",
    "# Treatment_protocol\n",
    "# Treatment_risk_group_in_ALL_10\n",
    "\n",
    "Clustered_by_patients_CODE = AggResults.groupby(['cluster_ap']).agg(\n",
    "    {'code_RFS': numpy.mean, \n",
    "     'code_EFS': numpy.mean,\n",
    "     'code_OS': numpy.mean})\n",
    "\n",
    "Clustered_by_patients_FU_RFS = AggResults[AggResults['FU_RFS'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'FU_RFS': numpy.median, \n",
    "     'FU_EFS': numpy.median,\n",
    "     'FU_OS': numpy.median})\n",
    "Clustered_by_patients_NotchPath = AggResults[AggResults['mutations_NOTCH_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_NOTCH_pathway': numpy.mean})\n",
    "Clustered_by_patients_IL7RPath = AggResults[AggResults['mutations_IL7R_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_IL7R_pathway': numpy.mean})\n",
    "Clustered_by_patients_PTENAKTPath = AggResults[AggResults['mutations_PTEN_AKT_pathway'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'mutations_PTEN_AKT_pathway': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_IL7R = AggResults[AggResults['comb_mutations_NOTCH_IL7R'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_IL7R': numpy.mean})\n",
    "Clustered_by_patients_comb_NOTCH_PTEN = AggResults[AggResults['comb_mutations_NOTCH_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_NOTCH_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN = AggResults[AggResults['comb_mutations_IL7R_PTEN'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN': numpy.mean})\n",
    "Clustered_by_patients_comb_IL7R_PTEN_NOTCH = AggResults[AggResults['comb_mutations_IL7R_PTEN_NOTCH'].apply(lambda x: isnan(x) is False)].groupby(['cluster_ap']).agg(\n",
    "    {'comb_mutations_IL7R_PTEN_NOTCH': numpy.mean})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_agg = pandas.merge(Clustered_by_patients_whitebloodcells, Clustered_by_patients_CODE, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_IL7R_PTEN_NOTCH, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_IL7R, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_comb_NOTCH_PTEN, how = 'inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_FU_RFS, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_IL7RPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_NotchPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, Clustered_by_patients_PTENAKTPath, how='inner', left_index=True, right_index=True)\n",
    "cluster_agg = pandas.merge(cluster_agg, patient_count, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Cluster centers:\",patient_sim.keys()[cluster_centers_indices].values)\n",
    "print(patient_cluster_members)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    class_members = patient_cluster_members == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.', \n",
    "             label = patient_sim.keys()[cluster_centers_indices[k]])\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.legend()\n",
    "        \n",
    "plt.title('Estimated number of clusters from Affinity Propagation: %d' % n_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### CREATE graph from similarity matrix\n",
    "##################\n",
    "# nodes\n",
    "VarList = TransPosed.keys()\n",
    "nodes = []\n",
    "node_index = 0\n",
    "for patient_name in VarList:\n",
    "    nodes.append((node_index, {'name': patient_name}))\n",
    "    node_index = node_index + 1\n",
    "\n",
    "edges = []\n",
    "# edges\n",
    "patient_sim = patient_similarity(Normal, sim_type = 'pearson', normalised = True, inflation=2)\n",
    "node_index_x = 0\n",
    "node_index_y = 0\n",
    "for patient_name_x in VarList:\n",
    "    for patient_name_y in VarList:        \n",
    "        edges.append((node_index_x, node_index_y, patient_sim.iloc[node_index_x, node_index_y]))\n",
    "        node_index_y = node_index_y + 1\n",
    "    node_index_x = node_index_x + 1\n",
    "    node_index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_weighted_edges_from(edges, weight = 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:126: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  b = plt.ishold()\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:138: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  plt.hold(b)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\__init__.py:917: UserWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  warnings.warn(self.msg_depr_set % key)\n",
      "c:\\users\\bramva~1\\envs\\worken~1\\lib\\site-packages\\matplotlib\\rcsetup.py:152: UserWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  warnings.warn(\"axes.hold is deprecated, will be removed in 3.0\")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFCCAYAAABSJMy8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtwVOXh//HP2U2y2RCIJCGiQRHlIo0gVFJFLCJCNQIi\nIFpBBhD8AWpAgcHS4qVVy4yDo1YdryNUR7GUSlEErYpGBBwC4ZKEBIlyUS5JIDESkqwJe35/bPGr\nFUIu5+zZy/s102mFc57nM2rz4Xn27HMM0zRNAQAAS7mcDgAAQCSiYAEAsAEFCwCADShYAABsQMEC\nAGADChYAABtQsAAA2ICCBQDABhQsAAA2oGABALABBQsAgA0oWAAAbEDBAgBgAwoWAAAbULAAANiA\nggUAwAYULAAANqBgAQCwAQULAIANKFgAAGxAwQIAYAMKFgAAG1CwAADYgIIFAMAGFCwAADaIcTpA\nSCgrk5YskXbskKqqpKQkqXdvafJkqUMHp9MBAMKQYZqm6XQIx+TmSgsXSmvWBP66ru7/fs/rlUxT\nysqS5s+XMjOdyQgACEvRW7DPPy/NnSvV1gaK9HQMI1C2ixZJM2YELx8AIKxF5xbxyXKtqTnztaYZ\nuG7u3MBfU7IAgCaIvhVsbq40aFDTyvV/JSRIOTlSv36WxwIARJboe4p44cLAtnBL1NYG7gcA4Ayi\nawVbViZ17vzzh5maKz5e2r+fp4sBAI2KrhXskiWtH8MwrBkHABDRoqtgd+xo3epVCmwT5+dbkwcA\nELGiq2CrqqwZp7LSmnEAABErugo2Kcmacdq3t2YcAEDEiq6C7d078JBSa3i9Uq9e1uQBAEQsniJu\nJjM+XgZPEQMAziC6VrBpaYGzhQ2jRbefkPR2XZ2MtDRrcwEAIk50FawUOLjf623RrXWSTh4zYRiG\nrrzySstiAQAiS/QVbGZm4OD+hIRm3WYmJGiOpC0/+bWNGzfKMAx9+umnViYEAESA6PoM9qda+DYd\no5HtZZ/Pp7i4OBvCAgDCTfStYE+aMSNwcP+oUYEni/9329jrDfz6qFGB6/77Fh3TNHX77befckiP\nx9NoAQMAokf0rmB/qrw8cPxhfn7gEIn27QNfxZk06bRPC1dXV6tt27anHfL666/XmpMvcgcARB0K\ntpXOtGJds2aNrr/++iClAQCEiujdIraIaZqaOnXqaX8/KytLhmHo2LFjQUwFAHAaK1iLVFRUKCUl\npdFrXC6XGhoa+JwWAKIAK1iLJCcny+/3N3qN3++Xy+ViyxgAogAFayHDMM64ZSxJH3zwgQzD0LJl\ny4KUDAAQbGwR2+TgwYNKT09v0rXl5eVKTU21OREAIJgoWBv5/X653e4mXevxeHT8+PEmXw8ACG1s\nEdvI5XI1ejDFT/l8PsXExGjgwIFBSAYAsBsr2CD58ssv1aNHjyZf/9JLL+nOO++0MREAwE4UbBDV\n19fL4/GoOX/L9+zZowsuuMC+UAAAW7BFHESxsbHy+/0aO3Zsk+/p0qWL4uPj5fP5bEwGALAaBeuA\nZcuWKS8vr8nX+3w+xcfH6/LLL2/W6hcA4BwK1iF9+/ZVTU1Ns+7ZtGmTXC6XFi1aZFMqAIBV+Aw2\nBIwYMUKrVq1q9n0FBQXKyMiwIREAoLUo2BCRk5OjQYMGNfu+2NhYVVRUKDEx0fpQAIAWo2BDSFVV\nlc4666wW3durVy9t27ZNLhe7/gAQCvhpHEKSkpLk9/t17bXXNvve/Px8ud1uzZ8/34ZkAIDmYgUb\not59913deOONLb4/JyeHU6EAwEEUbAgrLS3VOeec0+Kv5rjdbh0+fJgXCQCAA9giDmFnn3226uvr\n1b9//xbdf+LECXXo0EHdunXTDz/8YHE6AEBjKNgQ53a7tWHDBv39739v8RglJSXyeDyaNm0aB1UA\nQJCwRRxG9u3bZ8m5xCtXrmzV57sAgDOjYMOMz+fT5Zdfru3bt7dqHJfLpa+++ooXCQCATdgiDjMe\nj0fbtm3T008/3apx/H6/unTpok6dOun48eMWpQMAnMQKNowVFhbqkksusWSsW265RUuXLuWgCgCw\nCD9Nw1hGRoaOHTum7t27t3qsZcuWye12a/HixRYkAwBQsGEuMTFRxcXFevDBBy0Z74477pDL5VJB\nQYEl4wFAtGKLOIJ88cUXLf7O7Kl06NBBRUVFSklJsWxMAIgWrGAjyBVXXKGjR4/q/PPPt2S88vJy\npaamKisri4MqAKCZKNgIk5ycrD179mjmzJmWjfn+++/L4/Fo0aJFHFQBAE3EFnEE+89//qPrrrvO\n8nHXrVunq666yvJxASCSULAR7uDBg+rTp4/Ky8stHTcpKUkFBQXq1KmTpeMCQKRgizjCnXvuuTpw\n4IAmTJhg6bhVVVU677zzdOWVV3JQBQCcAgUbBWJjY/Xaa69p2bJllo+9ceNGJSYm6g9/+IP8fr/l\n4wNAuGKLOMqUlJSoX79+qqqqsmX8VatWadiwYbaMDQDhhBVslOnatasOHTqk4cOH2zL+8OHD1aZN\nG+3atcuW8QEgXFCwUcjr9erdd9/VCy+8YMv4NTU1uvjii9WrVy9VVFTYMgcAhDq2iKPc9u3b1b9/\nf9XW1to2x5133qlnn31WcXFxts0BAKGGFWyUu/TSS3Xo0CENHDjQtjlefvlleTweLVmyhIMqAEQN\nChZKSkrSp59+qoULF9o6z+TJk+X1erV582Zb5wGAUMAWMX5m/fr1Gjx4sO1nD1900UX67LPPdO65\n59o6DwA4hRUsfmbAgAH65ptvdOmll9o6z1dffaX09HSNHTuWgyoARCQKFr+QlpamLVu2aN68ebbP\ntXz5ciUmJurJJ5/koAoAEYUtYjRq9erVGjlypBoaGmyfKyYmRqtXr9bQoUNtnwsA7EbB4oz27dun\noUOHavfu3UGZr2PHjsrJyVH37t2DMh8A2IEtYpxR586dlZ+frylTpgRlvsOHD6tHjx4aOnQoB1UA\nCFsULJrE4/HolVde0RtvvKGYmJigzPnRRx8pJSVFf/rTn2x/qhkArMYWMZqtqKhIQ4YM0cGDB4M2\np8vl0tKlSzV27FgZhhG0eQGgpVjBotl69uypXbt2afTo0UGb0+/369Zbb1VKSory8vKCNi8AtBQF\nixZJTEzU8uXL9dxzz8ntdgdt3srKSl122WXKzMwM6goaAJqLLWK0Wm5urrKysnT06NGgzz19+nQt\nWrRIbdq0CfrcANAYChaWqKio0NixY7V27VpH5n/xxRc1depUuVxsygAIDfw0giWSk5P14Ycf6tFH\nHw3aU8Y/NW3aNLVr106ffPJJ0OcGgFNhBQvLrV27VqNGjdL333/vyPw9e/bUypUr1a1bN0fmBwCJ\nFSxsMHjwYO3cuVP9+vVzZP6ioiJ1795dv//971VZWelIBgCgYGGL9PR0bdiwQbNnz1ZsbKwjGf7x\nj38oOTlZCxcuVH19vSMZAEQvtohhuxUrVmjChAmOvpbO4/Horbfe0siRIzmoAkBQULAIipKSEo0Y\nMULFxcWO5jjvvPO0cuVK9e3b19EcACIfW8QIiq5duyovL0+TJ09WXFycYzm++eYb/frXv1ZWVhYH\nVQCwFQWLoPF6vXr11Vf1wgsvKCEhwdEs77//vtLT0zVv3jxHt64BRC62iOGI7du3a8SIEfrmm2+c\njiK3262XXnpJkyZN4qAKAJahYOGYqqoqTZw4UR988IHq6uqcjqOUlBQtX75cgwYNcjoKgAjAH9fh\nmKSkJK1YsUJ//etfHd8ylqSjR4/qmmuuUf/+/VVSUuJ0HABhjhUsQsL69es1evRolZWVOR3lR1On\nTtXjjz+u9u3bOx0FQBiiYBEyysrKdMstt2jTpk2qra11Oo4kyTAMLVq0SNnZ2Y4dmAEgPLFFjJCR\nlpamjz/+WHPmzFFiYqLTcSRJpmlqzpw5SklJ0cqVK8WfRwE0FStYhKQ1a9Zo3LhxqqqqCqlS69mz\np95880316dPH6SgAQhwrWISkrKwsbdu2TX369AmJB6BOKioqUt++fTV27FgOqgDQKAoWIatz587a\nuHGjJk2apKSkJKfj/Mzy5cuVnp6uBQsWqKamxuk4AEIQW8QIC0uXLtX06dN17NixkNoylgIvEnjx\nxRc1YcIEDqoA8CMKFmGjqKhII0eO1MGDB0PyeMPzzz9fr7/+ugYOHOh0FAAhgD9uI2z07NlTeXl5\nuvHGG5WSkuJ0nF/Yv3+/rr76ag0ZMoSDKgBQsAgviYmJeuONN/TII48oKSkpJN/t+vHHH6tbt266\n5557VFlZ6XQcAA5hixhhKzc3V6NHj9Z3332n6upqp+Ocktvt1hNPPKG77rqLgyqAKEPBIqwdPXpU\nEyZM0JYtW0LqmMX/lZqaqldffVXDhw8PyVU3AOuxRYywlpKSolWrVmnWrFkhfWbwkSNHdOONNyoz\nM1Pbtm1zOg6AIGAFi4ixdu1a3XbbbaqpqQnZLeOTxo0bp0WLFumcc85xOgoAm1CwiCgHDhzQrbfe\nqq+//lqHDh1yOk6jDMPQgw8+qHnz5oXUaVUArMEWMSJKenq6PvnkE40fP16pqalOx2mUaZr685//\nrLPPPluvvfaa/H6/05EAWIgVLCLWihUrNHXqVPl8vpA8mKKDpImSeks6S5K/bVv1GjdOFz7yiNSh\ng7PhALQaBYuIVlJSojFjxqiiokLffvut03EkSf0kzZeUJcmU9NPN4RpJMS6Xfrj2WiU+9piUmelE\nRAAWoGAR8Wpra5Wdna3Vq1c7/rnsNElPSIqX5G7kuhOSTsTEqOFPfwp8Prtpk1RYKB0/LtXVSfHx\nUmKi9KtfSb/5jTR5MqteIMRQsIgaixcv1uzZs1VfX+/IlvHJcm3TjHvM//6nSQ9LJCVJF14omaZk\nGIHC7dBB6t2bAgYcQMEiqmzfvl1jxoxRQ0OD9u3bF7R5+0n6VM0rV8u43VJMjHTDDdL8+Ww7A0HC\nU8SIKpdeeqm2bNmiyy67TBdccEHQ5p2vwLawI06ckHw+acUKacAA6fnnnUoCRBVWsIhKpmnqySef\n1GOPPaa6ujpbX5reQdI+SV7bZmiB3/5W+uwzp1MAEY0VLKKSYRiaPXu2Vq5cqfbt29u6mp2owOeo\nIWXdusDntM8843QSIGJRsIhqV111lfLy8nTRRRepe/futszRWz//Kk5ImTlTOuccKTfX6SRAxKFg\nEfXS0tL0wQcf6JZbbtHZZ58tr9fazdyzLB3NBocPB77qc//9TicBIgqfwQI/sWbNGk2cOFGJiYna\ns2ePJWO+JmmCJSMFwe9/Ly1d6nQKICKwggV+IisrS7m5uUpNTVVGRoYlY1YoBD+DPZ233pL+8Aen\nUwARgRUscAo+n09z5szRO++8o/LyctXV1bV4rGoFPoMNm9esG0bg5Kh+/ZxOAoQ1ChZoxNKlS5Wd\nna127do1e8v45OESYVWuJ116qcSL4YFWoWCBMygqKtLo0aOVkJCgvLy8Jt0zTdJzCnwGE3blelJZ\nGccrAq3AZ7DAGfTs2VO5ubnq0aOHunbtKo/H0+j10yQ9rcBh/mFbrpI0caLTCYCwxgoWaCLTNPXC\nCy/ogQceUNu2bbV3795fXOPomcN24McD0GKsYIEmMgxDM2bM0Jo1a+T3+3X55Zf/4pr5CrEjEVur\nqMjpBEDYomCBZsrMzFReXp5SUlJ0ySWXKC4uTlLgzOEsRdj/qe64w+kEQNiKqJ8FQLCkpKTo3Xff\n1W233ab27durc+fOishPLDdvdjoBELb4DBZopbVr12r8+PFacuKErisvdzqO9fgRAbQIBQtY4MCB\nA9rbq5cGVFY6HcV6/IgAWoQtYsAC6enp6n/99U7HABBCKFjAIq4+fQLHDAKA2CIGrFNWJnXsGFFb\nqn5JZkOD3G6301GAsMMKFrBKWpqUmup0CkuZkmJjY1VSUuJ0FCDsULCAlS67zOkEljElHVfgBKtu\n3brpiSeecDoSEFYoWMBK11zjdAJL/b+f/O+5c+ee8vQqAKfGZ7CAlcrKpLPPdjqFJUyd+k/gHo9H\nBw8eVHJycrAjAWGFFSxgpbQ0KSPD6RSW2H+aX/f5fEpJSdHbb78d1DxAuKFgAatFwGeVpqRnznDN\nmDFjNHbs2GDEAcISW8SAHfr0kbZvdzpFi9VKOl/SkSZcm5ycrEOHDv340gMAAaxgATu8/LIUE+N0\nihbxS1qtppWrJFVUVMjj8WjLli02pgLCDwUL2CEzU/rb36QwPKChVtLCFtzXr18/3X///VbHAcIW\nW8SAnZ5/Xrr77rA53ckv6S5JL7ZijK5du2rXrl1yufjzO6IbBQvY7T//ka67zukUZ2RKWixpigVj\nuVwu7d+/X+np6RaMBoQn/ogJ2O13v5MGDXI6RaNMSWtlTblKkt/vV6dOnfTyyy9bNCIQfljBAsGQ\nmysNGCDV1zud5BdMBb7zeoFN4w8cOFA5OTk2jQ6ELlawQDBkZkpPPy3Fxjqd5Bd8ksbYOP5nn32m\nNm3aqLq62sZZgNBDwQLBMmNGoGQ9HqeT/Oi4pHsl2f0Fm5qaGrVt21YffvihzTMBoYOCBYJpxgzp\n88+l0aMDRevQ13j8CpTrHLXuieHm+t3vfqfJkycHcUbAOXwGCzilvFxaskTKz5fy8qTiYunECVun\nbPjvf95T4LuuTh0NkZaWpoMHD/Iid0Q0ChYIJUVF0ty5UkGBVFEh+f2B79D6/YEHpPz+Jg1z8v/U\nPkkVkiolFUjKlfR3Nf2UJrvt3LlTPXv2dDoGYAsKFgg35eXSs89Kq1ZJpaWB0jUM/ZCUpF3ffadd\n332n2m7dNHvHjpAp0sY88sgjWrBggdMxAMtRsECE2bhxo7KzsxUXF6fvv/9ehYWFTkc6o4yMDBUU\nFDgdA7AUDzkBEaZ///7atGmTpkyZoiNHjujmm2+W1+t1OlajCgsLFRsbq6NHjzodBbAMBQtEIJfL\npSlTpqi4uFjp6elKTEzUTTfd5HSsRjU0NCg1NVVLly51OgpgCbaIgShQWFiomTNnqrS0VKZpaufO\nnU5HalRWVpZWr17tdAygVShYIEqYpqm3335bc+bM0cUXX6x169appqbG6VinlZiYqPLycsXHxzsd\nBWgRtoiBKGEYhsaMGaOdO3fqiiuukNfr1bBhw5yOdVrV1dXyer364osvnI4CtAgFC0SZhIQEPfzw\nw9q8ebPi4+PVpUsXZWRkOB3rtPr376+ZM2c6HQNoNraIgSj30UcfadasWUpJSVFeXp6OHz/udKRT\n6tSpk/bt28eL3BE2+DcViHJDhgzRtm3bNGbMGHm9Xg0dOtTpSKf07bffyu12a+/evU5HAZqEggWg\n2NhYzZo1S4WFhTr//PPVsWNH/epXv3I61il16dJFTz31lNMxgDNiixjAL+Tm5io7O1s+n08lJSUh\n+S7XzMxMbdq0yekYwGlRsABOye/36/XXX9f8+fPVtWtXrVu3zulIvxAbG6sjR46oXbt2TkcBfoEt\nYgCn5HK5NHHiRBUVFek3v/mNUlJSdPHFFzsd62fq6+uVlJSk9957z+kowC+wggXQJMXFxZo1a5b2\n7NmjQ4cOhdy28ZgxY7R8+XKnYwA/omABNJlpmnrnnXd03333KSUlRZs3b3Y60s8kJSXpyJEjiomJ\ncToKwBYxgKYzDEMjR47Uzp07NXLkSCUnJ6t79+5Ox/pRVVWVYmNjlZ+f73QUgIIF0Hzx8fFasGCB\ntm3bpr59+6pTp05q27at07F+1Lt3b/3xj390OgaiHFvEAFotJydH2dnZMk0zpF6cfsEFF+jrr7+W\nYRhOR0EUYgULoNWuvvpq5eXlafr06UpNTVW3bt2cjiRJ2rt3r1wulw4fPux0FEQhChaAJWJiYnT3\n3XerqKhIgwcPVlpaWshsG59zzjlavHix0zEQZdgiBmCLrVu3Kjs7W6WlpSopKXE6jiRpwIAB+vzz\nz52OgShBwQKwjWmaevPNNzVv3jx5PB7t2bPH6UiKi4vTd999J6/X63QURDi2iAHYxjAMjR8/XsXF\nxbrllluUnJzs+LbxDz/8oISEBH366aeO5kDkYwULIGh2796te++9V9u3b9eBAwecjqMJEybotdde\nczoGIhQFCyDo3nvvPc2aNUs+n0/ffvuto1mSk5NVXl7Oi9xhOf6NAhB0w4YNU2Fhoe655x4lJycr\nMTHRsSwVFRVyu90h8yAWIgcFC8ARHo9H999/v3bs2KGRI0cqNTXV0TzdunXTY4895mgGRBa2iAGE\nhM8//1zZ2dk6dOiQSktLHcvRo0cPFRcXOzY/IgcFCyBknDhxQq+88ooeeOAB1dTU6Pjx445lqays\n1FlnneXY/Ah/bBEDCBlut1vTpk1TcXGxJk2a5GjBtW/fXsuWLXNsfoQ/VrAAQtaOHTs0c+ZMFRQU\n6OjRo45kGDJkiD788ENH5kZ4o2ABhDTTNLVs2TLNnTtXFRUVqqmpCXoGj8ejY8eOKTY2NuhzI3yx\nRQwgpBmGoVtvvVXFxcWaPXu2kpKSgp7B5/MpLi5OmzZtCvrcCF+sYAGEla+//lqzZ8/WJ598ou+/\n/z7o80+fPl3PP/980OdF+KFgAYSlDz74QDNnztT+/ftVV1cX1LlTU1NVWlrK6U9oFP92AAhL1113\nnfLz8/Xoo48G/WnjI0eOyO12h8R5yghdFCyAsBUXF6c5c+Zo586dmjhxotq0aRPU+Tt16qS//e1v\nQZ0T4YMtYgAR44svvtA999yjgoIC+Xy+oM2bkZGhgoKCoM2H8EDBAogofr9fixcv1v333x/U784a\nhqHq6molJCQEbU6ENraIAUQUl8ulKVOmqKSkRLNmzVJ8fHxQ5jVNU23atNHq1auDMh9CHytYABGt\nsLBQ2dnZ+vzzz1VfXx+UOYcNG6ZVq1YFZS6ELgoWQMQzTVMrVqxQdna2Dh48GJQ5PR6Pampq+CpP\nFOOfPICIZxiGRo8erd27d+vhhx+Wx+OxfU6fzye3283DT1GMggUQNRISEvTQQw9p165dGj16tNxu\nt+1z9urVS3PnzrV9HoQetogBRK2PP/5YM2bM0O7du22fKy0tzdEXySP4WMECiFrXXnutCgsL9dRT\nT8nr9do6V1lZmQzD0JEjR2ydB6GDggUQ1WJjYzVr1izt3btXd9xxhwzDsHW+Dh066OWXX7Z1DoQG\ntogB4Cdyc3M1bdo0bd261dZ5evfure3bt9s6B5xFwQLA//D7/Xr99dd1zz33qLq62rZ5DMNQXV2d\n4uLibJsDzmGLGAD+h8vl0sSJE3XgwAHNmTPHtnlM05TH41FOTo5tc8A5rGAB4AyKi4s1ffp0W4tw\n9OjR+te//mXb+Ag+ChYAmsA0Tb377ru64447bHuJQHx8vI4fP87pTxGCf4oA0ASGYejGG2/Ut99+\nq0ceecSWOerq6uR2u7Vnzx5bxkdwUbAA0Azx8fFasGCB9u/fr5tuusmWOS688EItWLDAlrERPGwR\nA0Ar5OTkaPz48Tpw4IDlY3fs2FGHDh2yfFwEBytYAGiFq6++Wnv37tWzzz5r+SEVhw8flmEYOnbs\nmKXjIjgoWABopZiYGN19990qKyvT5MmTLR+/Xbt2evPNNy0fF/ZiixgALLZ161bddttt2rVrl6Xj\n9u3bV3l5eZaOCftQsABgA9M0tXTpUk2cOFENDQ2WjWsYhurr64Pyqj20DlvEAGADwzA0btw4VVRU\n6L777rNsXNM0FRMTo9zcXMvGhD1YwQJAEOzevVvjxo3T5s2bLRvz5ptv1j//+U/LxoO1KFgACKL3\n3ntPY8aMkc/ns2S8+Ph41dbWWjIWrMUWMQAE0bBhw1RVVWXZaVB1dXUyDIPvy4YgChYAgszj8WjB\nggU6cOCABg8ebMmY5557rh566CFLxoI12CIGAIetX79eWVlZlhwokZaWptLSUgtSobVYwQKAwwYM\nGKDKyko999xzrR6rrKxMhmGopqbGgmRoDQoWAEKA2+3WXXfdpaNHj2rUqFGtHq9NmzZavny5BcnQ\nUmwRA0AI2rFjh6655hpVVFS0apw+ffpo69atFqVCc1CwABCiTNPUW2+9pXHjxrV6rBMnTvAi9yDj\n7zYAhCjDMHTbbbepurq61S8RcLvdys/PtygZmoIVLACEia+//loDBgzQ4cOHWzwGpz8FDwULAGFm\n9erVGjZsWIvvj4uLs+wkKZweW8QAEGZuuOEG+Xw+3XvvvS26/4cffpBhGK1+gAqNo2ABIAzFxcXp\nySef1KFDh3ThhRe2aIyUlBTLjmzEL7FFDAARYMOGDRowYECL7k1OTtbRo0ctTgRWsAAQAa688kqd\nOHFCf/nLX5p9b0VFxY8vcod1KFgAiBAul0sPPPCAKisr1bNnz2bfHxcXp3feeceGZNGJLWIAiFCF\nhYXq1auXmvtjPiMjQwUFBTalih6sYAEgQmVkZOjEiRN65plnmnVfYWGhDMOwKVX0YAULAFGgtrZW\nV111lfLy8pp135dffqlu3brZlCqysYIFgCjg9Xq1ZcsW7dmzp1mr0+7du1vydp9oxAoWAKLQG2+8\nodtvv71Z91AXzUPBAkCUqq+v16BBg7Rhw4Ym33Ps2DElJibamCpysEUMAFEqNjZW69evV2lpqdxu\nd5Puadu2rR5++GF7g0UIVrAAAEnSqlWrNGLEiCZd6/V6VVNTY3Oi8EbBAgB+5Pf7NXToUK1du7ZJ\n1zc0NDR59Rtt2CIGAPzI5XLp448/VlVVVZOuj4mJ0cqVK21OFZ5YwQIATmvdunUaOHDgGa/r1KmT\nvvnmmyAkCh8ULACgUaZpavjw4Vq9enWTrkUABQsAaJK6ujp5vd4zXrdr1y517949CIlCG5/BAgCa\nJD4+XqZpnvG4xR49eui3v/1tkFKFLlawAIAWGTVqlP797383ek00VwwFCwBosYaGBsXGxjZ6TXV1\ntdq0aROkRKGDLWIAQIvFxMTINE3t3r37tNckJiZq+vTpQUwVGljBAgAsc9NNNzX6vdhoqhwKFgBg\nKdM05XKdfoM0WmqHLWIAgKUMw5Bpmtq/f/9pf/+VV145/QBlZdLjj0u33y6NGBH478cfl8rLbUps\nD1awAABnE4a/AAACi0lEQVRbZWVl6f333z/l7/2sgnJzpYULpTVrAn9dV/d/v+f1SqYpZWVJ8+dL\nmZk2JrYGBQsACArDME7566ZpSs8/L82dK9XWBor09IMEynbRImnGDJuSWoOCBQAETWlpqTp27Piz\nX5sm6bn4eLl/umI9k4SEkC9ZChYAEHT9+vXTli1b1E/Sp5Ja9C3ZhAQpJ0fq18/SbFahYAEAjnnb\nMDRSUoveKGsY0qhR0r/+ZXEqa1CwAABnlJVJnTv//GGm5oqPl/bvlzp0sC6XRfiaDgDAGUuWtH4M\nw7BmHBtQsAAAZ+zY0brVqxR46jg/35o8FqNgAQDOqKqyZpzKSmvGsRgFCwBwRlKSNeO0b2/NOBaj\nYAEAzujdO/CQUmt4vVKvXtbksRhPEQMAnMFTxAAA2CAtLXC28GmOUDwjw5BuuCEky1ViBQsAcFJu\nrjRokFRT0/x7Q/wkJ1awAADnZGYGzhROSGjefSfPIg7RcpWkGKcDAACi3MkD+3mbDgAANti8OfA+\n2NWrA0VaW/t/v3fyfbA33BB4H2wIr1xPomABAKGlvDxw/GF+fuAQifbtA1/FmTQpZB9oOhUKFgAA\nG/CQEwAANqBgAQCwAQULAIANKFgAAGxAwQIAYAMKFgAAG1CwAADYgIIFAMAGFCwAADagYAEAsAEF\nCwCADShYAABsQMECAGADChYAABtQsAAA2ICCBQDABhQsAAA2oGABALABBQsAgA0oWAAAbEDBAgBg\nAwoWAAAbULAAANiAggUAwAYULAAANqBgAQCwAQULAIANKFgAAGxAwQIAYAMKFgAAG1CwAADYgIIF\nAMAGFCwAADagYAEAsAEFCwCADShYAABsQMECAGADChYAABv8f4noJqN/OpfrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f31beac128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### apply Spring-force\n",
    "#######################\n",
    "pos = nx.spring_layout(G, k = None, dim = 3, scale = 1.0)\n",
    "nx.draw_spring(G, k = 30, dim = 2, scale = 1.0, iterations =1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### APPLY community detector\n",
    "# maximize betweenness and modularity\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### LOAD IN DATA\n",
    "###################\n",
    "# https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
