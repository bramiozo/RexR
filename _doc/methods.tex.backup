\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

%opening
\title{Methods used}
\author{Bram van Es \& Sebastiaan Jong}

\begin{document}


\begin{abstract}

\end{abstract}


\section{Pre-processing}

\subsection{Cohort-bias removal}
%
Using a normalisation per cohort guarantees that the features have the same bounds 
over the cohorts and that the means are similar. 
The standard normalisation transforms the genome expression values $\mathbf{x}$ per genome as follows
\begin{equation}
  \mathbf{x} = \frac{\mathbf{x} - \overline{\mathbf{x}}}{\sigma},
\end{equation}
%
where $\mathbf{x}$ is the genome expression vector for some genome over all samples. 
This centers the mean and normalises the  expression values with the standard deviation. 
To ensure similar bounds we can then scale the values by largest absolute value per genome vector, i.e.
\begin{equation}
  \mathbf{x} = \frac{\mathbf{x}}{\max{\mathbf{x}}},
\end{equation}
%
and as an alternative to the standard normalisation we can also apply a minmax normalisation
\begin{equation}
 \mathbf{x} = \frac{\mathbf{x} - \min{\mathbf{x}}}{\max{\mathbf{x}} - \min{\mathbf{x}}}.
\end{equation}
%
There are also robust scalers which are appropriate if outlying expression values 
are still present in the data. In our case we assume that these values are filtered out a priori. \\ \\
%
To demonstrate the effect of these transformations with regard to cohort bias we take two genomes, one with high and with low variance
over the classifications. \\ \\
%
There are various more elaborate methods to remove bias, such as  
the SVD-based method from Alter et al.\cite{Alter2000}, the 
PCA-based bias removal methods EIGENSTRAT by Price et al.\cite{Price2006}, MANCIE by Zang et al.\cite{Zang2016}, the distance weighted discrimination (DWD)
approach from Benito et al.\cite{Benito2004} or the ComBat method by Johnson et al.\cite{Johnson2007} which applies an empirical Bayes approach. 
A comparison of bias removal methods is out of scope for this work, for more details we refer the reader to Johnson et al\cite{Johnson2007}.

\subsection{Dimension reduction}
\subsubsection{Covariance based transformation}

Partial least squares: overfitting

Latent Dirichlet Allocation: requires availability of classification for fitting, hence
the transformation is biased to the training set.

Principle Component Analysis: transformation of the feature space based on the eigenvectors 
of the covariance matrix. Can be applied to the entire dataset.  The downside is that we obfuscate the 
biological meaning of the . Any value in the feature set of the transformed matrix is a linear combination 
of $N$ genome expression values, where $N$ is the number of expression

For this reason variance-based feature reduction is the most suitable method to reduce the number of dimensions.

\subsubsection{Variance-based feature reduction}

Mann-Whitney U
FDR, Benjamini-Hochberg procedure - ANOVA F-value
FDR, Benjamini-Hochberg procedure - Wilcoxon 

\section{Classification}

\subsection{Tree based}


\subsection{Neural networks}

Accuracy, but not transparant

\subsection{Linear methods}

Simplicity, transparancy.

\subsection{Bagging}

increases the accuracy, removes method-specific biases and at the same time helps reduce overfitting

\section{Post-processing}

importance/weights, what are they, how are they obtained?

\section{Discussion}

\begin{itemize}
\item check for inflection point in ordering of eigenvalue to 'smartly' select the number of components, if we choose PCA, LDA
\item improval of bias removal by mimicking featurewise distribution of expression values
\end{itemize}

\bibliographystyle{plain}
\bibliography{methods}
\end{document}
