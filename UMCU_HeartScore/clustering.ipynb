{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "from scipy.stats import ks_2samp as ks2\n",
    "from scipy.stats import mannwhitneyu as mwu\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import wasserstein_distance as w1_dist\n",
    "from scipy.stats import energy_distance as w2_dist\n",
    "from scipy.stats import epps_singleton_2samp as epps\n",
    "from category_encoders import *\n",
    "\n",
    "import SimpSOM as sps\n",
    "import minisom as msom\n",
    "#import sompy\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM, NuSVC\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, BayesianRidge, LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cross_decomposition import PLSRegression as PLS\n",
    "from ngboost import NGBRegressor\n",
    "\n",
    "from sklearn.cluster import SpectralClustering, AffinityPropagation, OPTICS, AgglomerativeClustering\n",
    "#from hdbscan import HDBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_cols(d):\n",
    "    assert isinstance(d, dict), 'input variables is not a dictionary'\n",
    "    return list(itertools.chain.from_iterable(d.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation:\n",
    "* standard scaling\n",
    "* robust scaler \n",
    "\n",
    "Feature reduction:\n",
    "* minimum univariate distribution difference\n",
    "* PCA \n",
    "* UMAP\n",
    "\n",
    "Clustering method:\n",
    "* HDBSCAN\n",
    "* Spectral\n",
    "* SOM\n",
    "\n",
    "This analysis should provide us with an intuition of the separability of the targets \n",
    "with the given features.\n",
    "\n",
    "**Output**: clusters can be used as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG: 646 cols, \t CELLDYN: 40 cols\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"T:\\laupodteam\\AIOS\\Bram\")\n",
    "HS = pd.read_csv(\"data/HeartScore/Data/MATRIX_FULL_23jul2019_ECG.csv\", sep=\";\")\n",
    "index_cols = ['pathos_key', 'upod_id'] \n",
    "date_cols = ['AcquisitionDateTime_ECG'] \n",
    "meta_cols = ['setsrc', 'Analyzer']\n",
    "pheno_cols = ['AGE', 'gender', 'BMI', 'RF_Diab', 'RF_Smok', 'RF_HyperTens', 'RF_HyperChol', 'RF_CVDHist', 'RF_FamHist', 'RF_obese30']\n",
    "ign_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_RiskFacts', 'HS_new2', 'tn_slope2', 'HN_TN'] # remove\n",
    "tn_cols = ['tn_admission'] # moreve tn_slope2 and HN_TN\n",
    "\n",
    "rem_cols = ['Door']+['delay_Celldyn', 'HS_new']+date_cols+meta_cols\n",
    "HS.drop(rem_cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# convert bool in int64\n",
    "for _col in HS.columns.tolist():\n",
    "    if str(HS[_col].dtype)=='bool':\n",
    "        HS[_col] = HS[_col].astype(int)\n",
    "        \n",
    "target = 'casecontrol'\n",
    "HS.rename(index=str, columns={target: 'target'}, inplace=True)\n",
    "tmap = {'Control': 0, 'NSTEMI': 1}\n",
    "HS['target'] = HS.target.map(tmap)\n",
    "#HS.drop(target, axis=1, inplace=True)\n",
    "\n",
    "gmap = {'M': 0, 'F': 1}\n",
    "HS['gender'] = HS.gender.map(gmap)\n",
    "\n",
    "HS.set_index(index_cols, inplace=True)\n",
    "\n",
    "cols = HS.columns.tolist()\n",
    "var_cols = list(set(cols) - set(meta_cols) - set(index_cols) -set(date_cols) - set(['target']) - set(ign_cols))\n",
    "\n",
    "cell_dyn_cols = [\"c_b_wbc\",\"c_b_wvf\",\"c_b_neu\",\"c_b_seg\",\"c_b_bnd\",\"c_b_ig\",\"c_b_lym\",\"c_b_lyme\",\"c_b_vlym\",\"c_b_mon\",\"c_b_mone\",\"c_b_blst\",\n",
    "                 \"c_b_eos\",\"c_b_bas\",\"c_b_pneu\",\"c_b_pseg\",\"c_b_pbnd\",\"c_b_pig\",\"c_b_plym\",\"c_b_plyme\",\"c_b_pvlym\",\"c_b_pmon\",\"c_b_pmone\",\n",
    "                 \"c_b_pblst\",\"c_b_peos\",\"c_b_pbas\",\"c_b_namn\",\"c_b_nacv\",\"c_b_nimn\",\"c_b_nicv\",\"c_b_npmn\",\"c_b_npcv\",\"c_b_ndmn\",\n",
    "                 \"c_b_ndcv\",\"c_b_nfmn\",\"c_b_nfcv\",\"c_b_Lamn\",\"c_b_Lacv\",\"c_b_Limn\",\"c_b_Licv\"] # delay_Celldyn: remove\n",
    "ecg_cols_agg =  [\"VentricularRate_ECG\",\"AtrialRate_ECG\",\"P_RInterval_ECG\",\"QRS_Duration_ECG\",\"Q_TInterval_ECG\",\n",
    "                 \"QTCCalculation_ECG\",\"PAxis_ECG\",\"RAxis_ECG\",\"TAxis_ECG\",\"QRSCount_ECG\",\"QOnset_ECG\",\n",
    "                 \"QOffset_ECG\",\"POnset_ECG\",\"POffset_ECG\",\"T_Onset_ECG\",\"T_Offset_ECG\",\"QRS_Onset_ECG\",\n",
    "                 \"QRS_Offset_ECG\",\"AvgRRInterval_ECG\",\"QTcFredericia_ECG\",\"QTcFramingham_ECG\",\"QTc_Bazett_ECG\"]\n",
    "\n",
    "ecg_leads = ['Lead_I_', 'Lead_II_', 'Lead_III_', 'Lead_V1_', 'Lead_V2_', 'Lead_V3_', 'Lead_V4_', 'Lead_V5_', 'Lead_V6_', 'Lead_aVF_', 'Lead_aVL_', 'Lead_aVR_']\n",
    "ecg_msrmnt = ['MaxST_ECG',  'Max_R_Ampl_ECG', 'Max_S_Ampl_ECG', 'MinST_ECG', 'PFull_Area_ECG', 'PP_Area_ECG', 'PP_Duration_ECG',\n",
    " 'PP_PeakAmpl_ECG', 'PP_PeakTime_ECG', 'P_Area_ECG', 'P_Duration_ECG', 'P_PeakAmpl_ECG', 'P_PeakTime_ECG', 'QRS_Area_ECG', 'QRS_Balance_ECG',\n",
    " 'QRS_Deflection_ECG', 'QRSint_ECG', 'Q_Area_ECG', 'Q_Duration_ECG', 'Q_PeakAmpl_ECG', 'Q_PeakTime_ECG', 'RP_Area_ECG', 'RP_Duration_ECG', 'RP_PeakAmpl_ECG',\n",
    " 'RP_PeakTime_ECG', 'R_Area_ECG', 'R_Duration_ECG', 'R_PeakAmpl_ECG', 'R_PeakTime_ECG', 'SP_Area_ECG', 'SP_Duration_ECG', 'SP_PeakAmpl_ECG', \n",
    " 'SP_PeakTime_ECG', 'STE_ECG', 'STJ_ECG', 'STM_ECG', 'S_Area_ECG', 'S_Duration_ECG', 'S_PeakAmpl_ECG', 'S_PeakTime_ECG',\n",
    " 'TFull_Area_ECG', 'TP_Area_ECG', 'TP_Duration_ECG', 'TP_PeakAmpl_ECG', 'TP_PeakTime_ECG', 'T_Area_ECG', 'T_Duration_ECG', 'T_End_ECG',\n",
    " 'T_PeakAmpl_ECG', 'T_PeakTime_ECG', 'T_Special_ECG', 'P_OnsetAmpl_ECG']\n",
    "\n",
    "\n",
    "ecg_cols_dyn = [_lead+_msrmnt for _lead in ecg_leads for _msrmnt in ecg_msrmnt]\n",
    "\n",
    "ecg_cols_agg = list(set(ecg_cols_agg) & set(var_cols))\n",
    "ecg_cols_dyn = list(set(ecg_cols_dyn) & set(var_cols))\n",
    "cell_dyn_cols = list(set(cell_dyn_cols) & set(var_cols))\n",
    "\n",
    "ecg_cols = list(set(ecg_cols_agg+ecg_cols_dyn))\n",
    "other_cols = list(set(var_cols)-set(ecg_cols)-set(cell_dyn_cols))\n",
    "\n",
    "col_dict = {'ecg': ecg_cols, 'celldyn': cell_dyn_cols, 'other': other_cols}\n",
    "print(\"ECG: {} cols, \\t CELLDYN: {} cols\".format(len(ecg_cols), len(cell_dyn_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= {'ecg': MinMaxScaler(), 'celldyn': MinMaxScaler(), 'other': MinMaxScaler()} # StandardScaler(), MinMaxScaler(), RobustScaler() or None\n",
    "dim_reduction = None # {'ecg': PCA(n_components=6), 'celldyn': PCA(n_components=20)} # dict with dimension reduction per data group, or one dim red for all, or None, methods: PCA, NMF, UMAP\n",
    "# dict with column name and impute type: median, mean, remove, regressor, (nmf?), or None, or knnimputer, or iterative which uses a round-robin approach using BayesianRidge as the regressor\n",
    "feature_expansion = ['celldyn']\n",
    "\n",
    "feature_weights = 'glm' # glm, tree, gam\n",
    "clustering = 'hdbscan' # hdbscan, SOM, spectral\n",
    "remove_nan_patients = False\n",
    "# NGBRegressor(), BayesianRidge(), MLPRegressor(hidden_layer_sizes=(70,70,30))\n",
    "imputance = {'BMI': BayesianRidge(), \n",
    "             'P_RInterval_ECG': BayesianRidge(), \n",
    "             'POnset_ECG': BayesianRidge(), \n",
    "             'PAxis_ECG': BayesianRidge(), \n",
    "             'POffset_ECG': BayesianRidge()}\n",
    "missing_dummy = True\n",
    "variance_remove = True\n",
    "remove_multicoll = False\n",
    "remove_outlying_samples_from_train = False\n",
    "remove_outlying_samples_from_test = False\n",
    "remove_weak_univariates = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn-contrib/categorical-encoding\n",
    "dummy_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_new2', 'HS_RiskFacts', 'HN_TN']\n",
    "HS[dummy_cols] = HS[dummy_cols].astype('category')\n",
    "HS = pd.get_dummies(HS, prefix_sep={_dummy: \"_dummy_\" for _dummy in dummy_cols})\n",
    "dummy_cols = [_col for _col in HS.columns if '_dummy_' in _col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_cols = ['tn_admission', 'tn_slope2', 'tn_diff_abs', 'tn_diff_rel']+dummy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS.loc[HS['tn_admission']>4000, 'tn_admission'] = 4000\n",
    "HS.loc[HS['tn_slope2']<-100, 'tn_slope2'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS['tn_diff_abs'] = HS[['tn_admission', 'tn_slope2']].apply(lambda x: np.sign(x[0]*x[1])*np.log10(np.abs(x[0]*x[1])+0.01), axis=1)\n",
    "HS['tn_diff_rel'] = HS[['tn_admission', 'tn_slope2']].apply(lambda x: x[1]/(x[0]+1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not impute, rather treat missingness as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num NaN columns : ECG 4\tCELLDYN 0\tOTHER 1\n",
      "Num NaN samples : ECG 75\tCELLDYN 0\tOTHER 64\tALL 135\n"
     ]
    }
   ],
   "source": [
    "print(\"Num NaN columns : ECG {}\\tCELLDYN {}\\tOTHER {}\".format((HS[ecg_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                       (HS[cell_dyn_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                       (HS[other_cols].isna().sum(axis=0)>0).sum()))\n",
    "###\n",
    "print(\"Num NaN samples : ECG {}\\tCELLDYN {}\\tOTHER {}\\tALL {}\".format((HS[ecg_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[cell_dyn_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[other_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS.isna().sum(axis=1)>0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_nan_patients:\n",
    "    nan_patients = HS[var_cols].isna().sum(axis=1)[HS[var_cols].isna().sum(axis=1)>0].index\n",
    "    HS.drop(index=nan_patients, inplace=True)\n",
    "    print(\"Num NaN samples : ECG {}\\tCELLDYN {}\\tOTHER {}\".format((HS[ecg_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[cell_dyn_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[other_cols].isna().sum(axis=1)>0).sum()))\n",
    "    \n",
    "    print(\"Num NaN columns : ECG {}\\tCELLDYN {}\\tOTHER {}\".format((HS[ecg_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                           (HS[cell_dyn_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                           (HS[other_cols].isna().sum(axis=0)>0).sum()))   \n",
    "\n",
    "    # y = HS[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputance\n",
    "\n",
    "# consider adding Gaussian noise on top of the prediction\n",
    "# https://www.kaggle.com/shashankasubrahmanya/missing-data-imputation-using-regression\n",
    "\n",
    "if missing_dummy:\n",
    "    # for NaN cols, treat non-nan as 0, nan as 1\n",
    "    _c = HS[var_cols].columns\n",
    "    nan_cols = _c[(HS[var_cols].isna().sum()>0)==True].tolist()    \n",
    "    dummy_list = []\n",
    "    for _ncol in nan_cols:\n",
    "        dummy_name = 'availdummy_'+_ncol\n",
    "        dummy_list.append(dummy_name)\n",
    "        HS[dummy_name] = (~pd.isna(HS[_ncol])).astype(int)\n",
    "        var_cols.append(dummy_name)\n",
    "        dummy_comb = \"_\".join(dummy_list)\n",
    "        HS[dummy_comb] = HS[dummy_list].apply(lambda x: np.product(x), axis=1)\n",
    "        col_dict['other'] = col_dict['other'] + [dummy_name, dummy_comb]\n",
    "    \n",
    "if HS[var_cols].isna().sum().sum()>0:\n",
    "    nan_cols = list(HS[var_cols].isna().sum()[HS[var_cols].isna().sum()>0].index)\n",
    "    dat = HS.copy()\n",
    "    if imputance is not None:\n",
    "        if isinstance(imputance, dict):\n",
    "            for _imp_key, _imp_val in imputance.items():\n",
    "                if type(_imp_val)==str:\n",
    "                    if _imp_val == 'median': \n",
    "                        dat.loc[dat[_imp_key].isna(), _imp_key] = np.nanmedian(dat[_imp_key])\n",
    "                    elif _imp_val == 'mean':\n",
    "                        dat.loc[dat[_imp_key].isna(), _imp_key] = np.nanmean(dat[_imp_key])\n",
    "                    elif _imp_val == 'remove':\n",
    "                        dat = dat.dropna(subset=[_imp_key])\n",
    "                elif 'sklearn' in str(type(_imp_val)):  \n",
    "                    _sub_cols = list(set(var_cols)  - set(nan_cols))\n",
    "                    _y = dat.loc[~dat[_imp_key].isna(), _imp_key]\n",
    "                    _X_train = dat.loc[~dat[_imp_key].isna(), _sub_cols]\n",
    "                    _X_test = dat.loc[dat[_imp_key].isna(), _sub_cols]\n",
    "                    try:\n",
    "                        dat.loc[dat[_imp_key].isna(), _imp_key] = _imp_val.fit(_X_train, _y).predict(_X_test)\n",
    "                    except Exception as e:\n",
    "                        print(\"Imputance failed for {}, shapes: {}, {}, {}\".format(_imp_key, _X_train.shape, _y.shape, _X_test.shape))\n",
    "                        if _X_test.shape[0]==0:\n",
    "                            print(\"Hmm, you probably already ran the imputer, please reload the data...\")\n",
    "        else:\n",
    "            if imputance=='iterative':\n",
    "                imp = IterativeImputer(estimator=BayesianRidge(), max_iter=10)\n",
    "            elif imputance=='knnimputer':\n",
    "                imp= KNNImputer(n_neighbors=5)\n",
    "\n",
    "            dat = pd.DataFrame(data=imp.fit_transform(dat[var_cols]), index=HS.index, columns=var_cols)\n",
    "            dat = dat.join(dat[meta_cols])\n",
    "else:\n",
    "    dat = HS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature combiner\n",
    "\n",
    "By default \n",
    "* CELLDYN: $\\frac{A}{A+B+\\epsilon}$, $\\sqrt{A\\cdot B}$ -> +800 features\n",
    "* ECG: None\n",
    "* OTHER: $\\frac{A}{A+B+\\epsilon}$, $\\sqrt{A\\cdot B}$ -> +600 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def _ssqrt(x, y):\n",
    "    return np.sqrt(np.multiply(x, y))\n",
    "@jit\n",
    "def _divisor(x, y, eps=1e-3):    \n",
    "    return np.divide((x), (x+y+eps))\n",
    "@jit\n",
    "def expander2(x, fun=None):\n",
    "    '''\n",
    "        x : np array \n",
    "        fun : expansion function, assumes pairwise expansion\n",
    "    '''\n",
    "    num_rows, num_cols = x.shape[0], x.shape[1]\n",
    "    _num_cols = np.int((num_cols**2-num_cols)/2)\n",
    "    xex = np.zeros(shape=(num_rows, _num_cols)) \n",
    "    k=0\n",
    "    for jl in range(0, num_cols):\n",
    "        for jr in range(jl+1, num_cols):\n",
    "            k +=1\n",
    "            xex[:, k] =   fun(x[:,jl], x[:,jr])\n",
    "    return xex\n",
    "\n",
    "def _cols_(cols, prefix=None):\n",
    "    prefix = prefix+\"_\" if prefix is not None else \"\"\n",
    "    return [prefix+cols[jl]+\"_\"+cols[jr] for jl in range(0, len(cols)) for jr in range(jl+1, len(cols))]\n",
    "\n",
    "var_cols = get_var_cols(col_dict)\n",
    "for _key in feature_expansion:\n",
    "    _cols = col_dict[_key]\n",
    "    dat[_cols] = dat[_cols].astype(np.float64)\n",
    "    tcols_sqrt = _cols_(dat[_cols].columns.tolist(), prefix='SQRT') \n",
    "    t = expander2(dat[_cols].values, fun=_ssqrt)\n",
    "    celldyn_sqrt = pd.DataFrame(data=t, index=dat.index, columns=tcols_sqrt)\n",
    "\n",
    "    tcols_div = _cols_(dat[_cols].columns.tolist(), prefix='DIV') \n",
    "    t = expander2(dat[_cols].values, fun=_divisor)\n",
    "    celldyn_div = pd.DataFrame(data=t, index=dat.index, columns=tcols_div)\n",
    "\n",
    "    col_dict[_key] = list(set(_cols + tcols_sqrt +tcols_div))\n",
    "    var_cols = list(set(var_cols + col_dict[_key]))\n",
    "    \n",
    "    dat = dat.join(celldyn_sqrt)\n",
    "    dat = dat.join(celldyn_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd = defaultdict(int)\n",
    "#for _col in dat[col_dict['other']].columns.tolist():\n",
    "#    cd[_col] += 1\n",
    "#for k, v in cd.items():\n",
    "#    if v>1:\n",
    "#        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _condition_number(x, ignore_nan=True):\n",
    "    return np.linalg.cond(x.dropna(), p=2)\n",
    "\n",
    "print(\"PRE: Condition numbers:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t Other {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                        _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                        _condition_number(dat[col_dict['other']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \n",
    "# if not embedding..\n",
    "if scaler is not None:\n",
    "    if isinstance(scaler, dict):        \n",
    "        for _imp_key, _scaler in scaler.items():\n",
    "            if _scaler is not None:\n",
    "                dat[col_dict[_imp_key]] = _scaler.fit_transform(dat[col_dict[_imp_key]]) \n",
    "    else:    \n",
    "        dat[var_cols] = pd.DataFrame(data=scaler.fit_transform(dat[var_cols]), index=HS.index, columns=var_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if variance_remove:\n",
    "    def remove_min_variance(x, threshold=0.001):\n",
    "        cols_to_keep = (x.var()/(x.max()-x.min()))\\\n",
    "            [((x.var()/(x.max()-x.min()))>threshold)].index   \n",
    "        drop_cols = list(set(x.columns)-set(cols_to_keep))\n",
    "        return x[cols_to_keep], drop_cols\n",
    "\n",
    "    tmp_keep, drop_cols = remove_min_variance(dat[var_cols], threshold=0.001)\n",
    "    print(\"Removing {} columns due to lack of variance\".format(dat[var_cols].shape[1] - tmp_keep.shape[1]))\n",
    "\n",
    "    col_dict['ecg'] = set(col_dict['ecg']).difference(set(drop_cols))\n",
    "    col_dict['celldyn'] = set(col_dict['celldyn']).difference(set(drop_cols))\n",
    "    col_dict['other'] = set(col_dict['other']).difference(set(drop_cols))\n",
    "    var_cols = set(var_cols).difference(set(drop_cols))\n",
    "\n",
    "\n",
    "    (tmp_keep.std()/(tmp_keep.max() - tmp_keep.min())).plot.hist(bins=50)\n",
    "    plt.title('Relative STD')\n",
    "\n",
    "    col_dict = {'ecg': list(col_dict['ecg']), 'celldyn': list(col_dict['celldyn']), 'other': list(col_dict['other'])}\n",
    "\n",
    "    print(\"POST: Condition numbers:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t Other {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['other']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to deal with the very large variance features as they may contain little information, yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_variance_features = dat[var_cols].columns[dat[var_cols].std()>0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove low-variant features is actually to remove low entropy features. As the idea of low variance is completely dependent on the scaling of the feature low entropy will give a \n",
    "more robust feature filter. For continuous variables we have to use *differential entropy*, described as; \n",
    "$$H = - \\int \\rho(x) \\ln \\rho(x) dx $$\n",
    "\n",
    "The best we can do is to approximate this using the histograms of the features;\n",
    "\n",
    "$$H \\approx - \\sum \\rho(b_i) \\ln \\rho(b_i) \\Delta x_i$$\n",
    "\n",
    "where $b_i$ represent the bins. To bound it between zero and one we should write\n",
    "\n",
    "$$H \\approx - \\frac{\\sum \\rho(b_i) \\ln \\rho(b_i) \\Delta x_i}{\\sum \\Delta x_i}$$\n",
    "\n",
    "Note that the correct differential entropy is actually written as \n",
    "$$H = - \\int \\rho(x) \\ln \\frac{\\rho(x)}{m(x)} dx $$\n",
    "\n",
    "where $m(x)$ is the invariant measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def _diff_entropy(x, eps=1e-6, bins=10):\n",
    "    rhos, xs = np.histogram(x, density=True, bins=bins)\n",
    "    xmean =  (xs[1:] + xs[:-1])/2\n",
    "    xdiff = xs[1:] - xs[:-1]\n",
    "    H = -np.sum(rhos*np.log(rhos+eps)*xdiff)\n",
    "    Hr = H/np.sum(xdiff)\n",
    "    return Hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cols = get_var_cols(col_dict)\n",
    "feat_entropy = pd.DataFrame(data=dat[var_cols].apply(func=_diff_entropy, axis=0).apply(pd.Series))\n",
    "feat_entropy.columns = ['Hr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(22,6))\n",
    "feat_entropy.Hr.plot.hist(bins=40, ax=ax)\n",
    "ax.axvline(feat_entropy.Hr.median(), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_hr = feat_entropy.loc[(feat_entropy.Hr>-0.5)].index\n",
    "low_hr = feat_entropy.loc[(feat_entropy.Hr<-2.15)].index\n",
    "print(len(high_hr), len(low_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(22,7))\n",
    "n = 20 \n",
    "N = len(high_hr)\n",
    "for i in range(0, n):\n",
    "    j = np.random.randint(0, N)\n",
    "    dat[high_hr[j]].plot.hist(bins=50, histtype='step', ax=ax[0])\n",
    "ax[0].set_title('high HR')\n",
    "N = len(low_hr)    \n",
    "for i in range(0, n):\n",
    "    j = np.random.randint(0, N)\n",
    "    dat[low_hr[j]].plot.hist(bins=50, histtype='step', ax=ax[1])\n",
    "ax[1].set_title('low HR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection\n",
    "* ANOVA\n",
    "* KS\n",
    "* 1st and 2nd Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fs_ws1():\n",
    "    scores_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore = w1_dist(pos[:,column], neg[:,column])\n",
    "        return zscore\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = results_\n",
    "        return self\n",
    "    \n",
    "class fs_ws2():\n",
    "    scores_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore = w2_dist(pos[:,column], neg[:,column])\n",
    "        return zscore\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = results_\n",
    "        return self\n",
    "    \n",
    "class fs_mannwhitney():\n",
    "    pvalues_ = None\n",
    "    scores_ = None\n",
    "    results_ = None\n",
    "    def __init__(self, pvalue = 0.01, mode='auto'):\n",
    "        # mode : 'auto', 'exact', 'asymp'\n",
    "        self.pvalue = pvalue\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore, p_value = mwu(pos[:,column], neg[:,column], alternative=\"less\") # mode=self.mode\n",
    "        return zscore, p_value\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        self.results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = self.results_[:, 0]\n",
    "        self.pvalues_ = self.results_[:, 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        not_signif = self.p_values<self.pvalue\n",
    "        to_delete = [idx for idx, item in enumerate(not_signif) if item == False]\n",
    "        return np.delete(x, to_delete, axis = 1), to_delete\n",
    "\n",
    "class fs_ks():\n",
    "    pvalues_ = None\n",
    "    scores_ = None\n",
    "    results_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore, p_value = ks2(pos[:,column], neg[:,column])\n",
    "        return zscore, p_value\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        self.results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = self.results_[:, 0]\n",
    "        self.pvalues_ = self.results_[:, 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        not_signif = self.p_values<self.pvalue\n",
    "        to_delete = [idx for idx, item in enumerate(not_signif) if item == False]\n",
    "        return np.delete(x, to_delete, axis = 1), to_delete\n",
    "\n",
    "\n",
    "class fs_epps():\n",
    "    pvalues_ = None\n",
    "    scores_ = None\n",
    "    results_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore, p_value = epps(pos[:,column], neg[:,column])\n",
    "        return zscore, p_value\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        self.results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = self.results_[:, 0]\n",
    "        self.pvalues_ = self.results_[:, 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        not_signif = self.p_values<self.pvalue\n",
    "        to_delete = [idx for idx, item in enumerate(not_signif) if item == False]\n",
    "        return np.delete(x, to_delete, axis = 1), to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "\n",
    "y = dat['target']\n",
    "\n",
    "if remove_weak_univariates:\n",
    "    anov = dict()\n",
    "    anov['celldyn'] = pd.DataFrame(data=f_classif(dat[col_dict['celldyn']], y), columns=col_dict['celldyn'], index=['F', 'pval']).T\n",
    "    anov['ecg'] = pd.DataFrame(data=f_classif(dat[col_dict['ecg']], y), columns=col_dict['ecg'], index=['F', 'pval']).T\n",
    "    anov['other'] = pd.DataFrame(data=f_classif(dat[col_dict['other']], y), columns=col_dict['other'], index=['F', 'pval']).T\n",
    "\n",
    "    chisq = dict()\n",
    "    chisq['celldyn'] = pd.DataFrame(data=chi2(dat[col_dict['celldyn']].abs(), y), columns=col_dict['celldyn'], index=['Chi', 'pval']).T\n",
    "    chisq['ecg'] = pd.DataFrame(data=chi2(dat[col_dict['ecg']].abs(), y), columns=col_dict['ecg'], index=['Chi', 'pval']).T\n",
    "    chisq['other'] = pd.DataFrame(data=chi2(dat[col_dict['other']].abs(), y), columns=col_dict['other'], index=['Chi', 'pval']).T\n",
    "\n",
    "    mi = dict()\n",
    "    mi['celldyn'] = pd.DataFrame(data=mutual_info_classif(dat[col_dict['celldyn']].abs(), y), index=col_dict['celldyn'], columns=['mi'])\n",
    "    mi['ecg'] = pd.DataFrame(data=mutual_info_classif(dat[col_dict['ecg']].abs(), y), index=col_dict['ecg'], columns=['mi'])\n",
    "    mi['other'] = pd.DataFrame(data=mutual_info_classif(dat[col_dict['other']].abs(), y), index=col_dict['other'], columns=['mi'])\n",
    "\n",
    "\n",
    "    ks_dist = dict()\n",
    "    KS = fs_ks()\n",
    "    KS.fit(dat[col_dict['other']].values, y)\n",
    "    ks_dist['other'] = pd.DataFrame(KS.results_, columns=['KS', 'pval'], index=col_dict['other'])\n",
    "\n",
    "    KS.fit(dat[col_dict['celldyn']].values, y)\n",
    "    ks_dist['celldyn'] = pd.DataFrame(KS.results_, columns=['KS', 'pval'], index=col_dict['celldyn'])\n",
    "\n",
    "    KS.fit(dat[col_dict['ecg']].values, y)\n",
    "    ks_dist['ecg'] = pd.DataFrame(KS.results_, columns=['KS', 'pval'], index=col_dict['ecg'])\n",
    "\n",
    "\n",
    "    mwu_dist = dict()\n",
    "    MW = fs_mannwhitney()\n",
    "    MW.fit(dat[col_dict['other']].values, y)\n",
    "    mwu_dist['other'] = pd.DataFrame(MW.results_, columns=['MWS', 'pval'], index=col_dict['other'])\n",
    "\n",
    "    MW.fit(dat[col_dict['celldyn']].values, y)\n",
    "    mwu_dist['celldyn'] = pd.DataFrame(MW.results_, columns=['MWS', 'pval'], index=col_dict['celldyn'])\n",
    "\n",
    "    MW.fit(dat[col_dict['ecg']].values, y)\n",
    "    mwu_dist['ecg'] = pd.DataFrame(MW.results_, columns=['MWS', 'pval'], index=col_dict['ecg'])\n",
    "\n",
    "    mwu_dist['celldyn']['MWS'] = 4*(mwu_dist['celldyn']['MWS'])/(dat.shape[0]*(dat.shape[0]+1))\n",
    "    mwu_dist['ecg']['MWS'] = 4*(mwu_dist['ecg']['MWS'])/(dat.shape[0]*(dat.shape[0]+1))\n",
    "    mwu_dist['other']['MWS'] = 4*(mwu_dist['other']['MWS'])/(dat.shape[0]*(dat.shape[0]+1))\n",
    "\n",
    "\n",
    "    try:\n",
    "        epps_dist = dict()\n",
    "        EPPS = fs_epps()\n",
    "        EPPS.fit(dat[col_dict['other']].values, y)\n",
    "        epps_dist['other'] = pd.DataFrame(EPPS.results_, columns=['EPPS', 'pval'], index=col_dict['other'])\n",
    "\n",
    "        EPPS.fit(dat[col_dict['ecg']].values, y)\n",
    "        epps_dist['ecg'] = pd.DataFrame(EPPS.results_, columns=['EPPS', 'pval'], index=col_dict['ecg'])\n",
    "\n",
    "        EPPS.fit(dat[col_dict['celldyn']].values, y)\n",
    "        epps_dist['celldyn'] = pd.DataFrame(EPPS.results_, columns=['EPPS', 'pval'], index=col_dict['celldyn'])\n",
    "    except Exception as e:\n",
    "        if str(e)=='SVD did not converge':\n",
    "            print(\"!! \\t Remove the multicollinearity of the input matrix \\t!!\")\n",
    "        else:\n",
    "            print(\"Oooeps: {}\".format(e))\n",
    "\n",
    "    # w1\n",
    "    wass1_dist = dict()\n",
    "    W1D = fs_ws1()\n",
    "    W1D.fit(dat[col_dict['other']].values, y)\n",
    "    wass1_dist['other'] = pd.DataFrame(W1D.scores_, columns=['W1'], index=col_dict['other'])\n",
    "\n",
    "    W1D.fit(dat[col_dict['celldyn']].values, y)\n",
    "    wass1_dist['celldyn'] = pd.DataFrame(W1D.scores_, columns=['W1'], index=col_dict['celldyn'])\n",
    "\n",
    "    W1D.fit(dat[col_dict['ecg']].values, y)\n",
    "    wass1_dist['ecg'] = pd.DataFrame(W1D.scores_, columns=['W1'], index=col_dict['ecg'])\n",
    "\n",
    "    # w2\n",
    "    wass2_dist = dict()\n",
    "    W2D = fs_ws2()\n",
    "    W2D.fit(dat[col_dict['other']].values, y)\n",
    "    wass2_dist['other'] = pd.DataFrame(W2D.scores_, columns=['W2'], index=col_dict['other'])\n",
    "\n",
    "    W2D.fit(dat[col_dict['celldyn']].values, y)\n",
    "    wass2_dist['celldyn'] = pd.DataFrame(W2D.scores_, columns=['W2'], index=col_dict['celldyn'])\n",
    "\n",
    "    W2D.fit(dat[col_dict['ecg']].values, y)\n",
    "    wass2_dist['ecg'] = pd.DataFrame(W2D.scores_, columns=['W2'], index=col_dict['ecg'])\n",
    "\n",
    "    ecg_inc_anova = anov['ecg'][anov['ecg'].F>5].index.tolist()\n",
    "    celldyn_inc_anova = anov['celldyn'][anov['celldyn'].F>5].index.tolist()\n",
    "\n",
    "    ecg_inc_mi = mi['ecg'][mi['ecg'].mi>0.003].index.tolist()\n",
    "    celldyn_inc_mi = mi['celldyn'][mi['celldyn'].mi>0.003].index.tolist()\n",
    "\n",
    "    col_dict['ecg'] = list(set(ecg_inc_anova+ecg_inc_mi))\n",
    "    col_dict['celldyn'] = list(set(celldyn_inc_anova+celldyn_inc_mi))\n",
    "    var_cols = get_var_cols(col_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-collinearity\n",
    "\n",
    "I.e. **the problem of removing all correlated pairs as efficiently as possible, meaning by removing a minimum number of nodes at the lowest computational cost.**\n",
    "\n",
    "The efficient removal of multicollinearity can be cast in graph optimisation problem.\n",
    "\n",
    "* Maximal Clique Enumeration (MCE): from all the maximal cliques, keep only the node with the lowest mean similarity. This is iterative; \n",
    "    * start with the largest cliques, exclude, redetermine cliques, exclude, etc. Until only pairs are left\n",
    "    * Remove all nodes with occurrence > 1, then randomly remove nodes   \n",
    "* Recursively remove 50% of the pairs until no pairs are left\n",
    "* Instead of removing the collinear pairs multiply them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosim\n",
    "   \n",
    "def coll_cols(x, threshold=0.99, how='corr_aff', ignore_nan=True):\n",
    "    '''\n",
    "     x : df\n",
    "     threshold : maximum correlation or maximum VIF\n",
    "     how : correlation (corr_aff, corr_pair) of VIF-based (vif)\n",
    "    '''\n",
    "    cols = x.columns.tolist()\n",
    "    if ignore_nan:\n",
    "        x=x.dropna() # x.fillna(x.median())   \n",
    "    if how=='vif':\n",
    "        cols_to_remove = []\n",
    "        x = add_constant(x)\n",
    "        x[cols] = StandardScaler().fit_transform(x[cols]) \n",
    "        vifs = []\n",
    "        for idx, col in tqdm(enumerate(cols)):\n",
    "            _vif = vif(x.values, idx)\n",
    "            if _vif != np.inf:                \n",
    "                vifs.append(_vif)\n",
    "            if vif(x.values, idx)>threshold:\n",
    "                cols_to_remove.append(col)\n",
    "        vifs = np.array(vifs)\n",
    "        print(\"VIF:\\t max {}, \\t min {}, \\t mean {}, \\t median {}\".format(vifs.max(), vifs.min(), vifs.mean(), np.median(vifs)))\n",
    "        plt.hist(np.log10(vifs), bins=50, density=True)\n",
    "    elif how=='corr_aff':\n",
    "        corrcoefs = x.corr().values  # np.corrcoef(tmp[var_cols]) , tmp[var_cols].corr().values         \n",
    "\n",
    "        AF = AffinityPropagation(damping=0.65, max_iter=200, convergence_iter=50, preference=None, affinity='precomputed')\n",
    "        labels = AF.fit_predict(-1/corrcoefs**2)\n",
    "        exemplars = AF.cluster_centers_indices_\n",
    "        cols_to_remove = [var_cols[i] for i in exemplars]\n",
    "    elif how=='corr_pair':\n",
    "        # simplistic \n",
    "        corrcoefs = cosim(np.transpose(x)) # cosim(x) # x.corr().values # np.abs(np.corrcoef(x, rowvar=False)), cosim(x)\n",
    "        print(corrcoefs.shape, x.shape)\n",
    "        conn_count = { k: 0 for k in cols}\n",
    "        conn_nodes = defaultdict(list)\n",
    "        for idx, _colx in enumerate(cols):\n",
    "            for jdx, _coly in enumerate(cols):\n",
    "                if jdx<idx:\n",
    "                    if corrcoefs[idx, jdx]>threshold:\n",
    "                        conn_count[_colx]  += 1\n",
    "                        conn_count[_coly]  += 1\n",
    "                        conn_nodes[_colx]  += [_coly]\n",
    "                        conn_nodes[_coly]  += [_colx]         \n",
    "        cols_to_remove = [_v[0] for _v in conn_nodes.values()]\n",
    "        \n",
    "    print(\"{} columns should be removed due to collinearity.\".format(len(cols_to_remove)))  \n",
    "    return cols_to_remove\n",
    "if remove_multicoll:\n",
    "    drop_cols = coll_cols(dat[col_dict['ecg']], how='corr_pair', threshold=0.98, ignore_nan=False)\n",
    "    col_dict['ecg'] = list(set(col_dict['ecg']).difference(set(drop_cols)))\n",
    "\n",
    "    drop_cols = coll_cols(dat[col_dict['celldyn']], how='corr_pair', threshold=0.98, ignore_nan=False)\n",
    "    col_dict['celldyn'] = list(set(col_dict['celldyn']).difference(set(drop_cols)))\n",
    "\n",
    "    #drop_cols = coll_cols(dat[col_dict['other']], how='vif', threshold=0.98, ignore_nan=False)\n",
    "    #col_dict['other'] = list(set(col_dict['other']).difference(set(drop_cols)))\n",
    "\n",
    "    #var_cols = list(set(var_cols).difference(set(drop_cols)))\n",
    "\n",
    "    print(\"POST: Condition numbers:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t Other {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['other']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding\n",
    "* PCA\n",
    "* UMAP\n",
    "* NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim reduction\n",
    "if dim_reduction is not None:\n",
    "    if isinstance(dim_reduction, dict):\n",
    "        assert(set(dim_reduction.keys()).issubset(['ecg', 'celldyn'])), \"Check the dim_reduction keys\"\n",
    "        ecols = ['ecg_'+str(i) for i in range(0, dim_reduction['ecg'].n_components)]\n",
    "        ecg_red = pd.DataFrame(dim_reduction['ecg'].fit_transform(dat[ecg_cols]), index=dat.index, columns=ecols)\n",
    "        ccols = ['celldyn_'+str(i) for i in range(0, dim_reduction['celldyn'].n_components)]\n",
    "        celldyn_red = pd.DataFrame(dim_reduction['celldyn'].fit_transform(dat[cell_dyn_cols]), index=dat.index, columns=ccols)\n",
    "        \n",
    "        dat_red = dat[other_cols].join(ecg_red).join(celldyn_red)\n",
    "    else:\n",
    "        rcols = ['red_'+str(i) for i in range(0, dim_reduction['ecg'].n_components)]\n",
    "        tot_red = pd.DataFrame(dim_reduction.fit_transform(dat[var_cols]), index=dat.index, columns=rcols)\n",
    "        dat_red = dat[other_cols].join(tot_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahalanobis distance for outliers\n",
    "def mahalanobis(x=None, cov=None):\n",
    "    # source: https://www.machinelearningplus.com/statistics/mahalanobis-distance/\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(x)\n",
    "    if not cov:\n",
    "        cov = np.cov(x.values.T)\n",
    "    inv_covmat = sc.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "var_cols = col_dict['ecg']+col_dict['celldyn']+col_dict['other']\n",
    "\n",
    "out_idx_dict= {}\n",
    "\n",
    "iso = IsolationForest(n_estimators=400, n_jobs=4)\n",
    "out_in = iso.fit_predict(dat[var_cols])\n",
    "out_idx_dict['iso'] = np.where(out_in==-1)\n",
    "in_idx = np.where(out_in==1)\n",
    "\n",
    "ocs = OneClassSVM(kernel='rbf', gamma='scale', max_iter=1000)\n",
    "out_in = ocs.fit_predict(dat[var_cols])\n",
    "out_idx_dict['svm'] = np.where(out_in==-1)\n",
    "    \n",
    "try:\n",
    "    mah = mahalanobis(dat[var_cols])\n",
    "    dat['_maha'] = mah\n",
    "    # isolation forest\n",
    "    dat._maha.plot.hist(bins=50)\n",
    "except:\n",
    "    print(\"matrix is singular..you did not remove the collinearity did you ;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_outlying_samples_from_train:\n",
    "    dat.drop(dat.index[out_idx_dict['iso'][0]], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced dimensionality visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ECG: {} cols, \\t CELLDYN: {} cols, \\t OTHER:{} cols\".format(len(col_dict['ecg']), len(col_dict['celldyn']), len(col_dict['other'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_viz={}\n",
    "nc = 9\n",
    "red_cols = ['pc_'+str(i) for i in range(0, nc)]\n",
    "pc = {}\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20,7))\n",
    "\n",
    "pc['ecg'] = PCA(n_components=nc, svd_solver='full')\n",
    "tred = pc['ecg'].fit_transform(dat[col_dict['ecg']])\n",
    "ax[0].plot(pc['ecg'].explained_variance_ratio_)\n",
    "ax[0].set_title('Explained variance ECG')\n",
    "dat_viz['pca_celldyn']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "pc['celldyn'] = PCA(n_components=nc, svd_solver='full')\n",
    "tred = pc['celldyn'].fit_transform(dat[col_dict['celldyn']])\n",
    "ax[1].plot(pc['celldyn'].explained_variance_ratio_)\n",
    "ax[1].set_title('Explained variance CELLDYN')\n",
    "dat_viz['pca_ecg']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "pc['other'] = PCA(n_components=nc, svd_solver='full')\n",
    "tred = pc['other'].fit_transform(dat[col_dict['other']])\n",
    "ax[2].plot(pc['other'].explained_variance_ratio_)\n",
    "ax[2].set_title('Explained variance other')\n",
    "dat_viz['pca_other'] = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance of the first component is huge, this suggests that there is a leaking feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(18,10))\n",
    "ax[0,0].hist(pc['ecg'].components_[0], bins=100); ax[0,0].set_title('ecg PC 1')\n",
    "ax[0,1].hist(pc['ecg'].components_[1], bins=100); ax[0,1].set_title('ecg PC 2')\n",
    "ax[1,0].hist(pc['celldyn'].components_[0], bins=100); ax[1,0].set_title('celldyn PC 1')\n",
    "ax[1,1].hist(pc['celldyn'].components_[1], bins=100); ax[1,1].set_title('celldyn PC 2')\n",
    "ax[2,0].hist(pc['other'].components_[0], bins=100); ax[2,0].set_title('other PC 1')\n",
    "ax[2,1].hist(pc['other'].components_[1], bins=100); ax[2,1].set_title('other PC 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(18,7))\n",
    "sns.scatterplot(data=dat_viz['pca_ecg'], x='pc_0', y='pc_1', hue=y, ax=ax[0])\n",
    "sns.scatterplot(data=dat_viz['pca_celldyn'], x='pc_0', y='pc_1', hue=y, ax=ax[1])\n",
    "sns.scatterplot(data=dat_viz['pca_other'], x='pc_0', y='pc_1', hue=y, ax=ax[2])\n",
    "ax[0].set_title('PCA ECG')\n",
    "ax[1].set_title('PCA CELLDYN')\n",
    "ax[2].set_title('PCA OTHER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 20\n",
    "red_cols = ['pc_'+str(i) for i in range(0, nc)]\n",
    "\n",
    "nnmf = {}\n",
    "nnmf['ecg'] = NMF(n_components=nc)\n",
    "tred = nnmf['ecg'].fit_transform(dat[col_dict['ecg']]+dat[col_dict['ecg']].min().abs())\n",
    "dat_viz['nmnf_ecg']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "nnmf['celldyn'] = NMF(n_components=nc)\n",
    "tred = nnmf['celldyn'].fit_transform(dat[col_dict['celldyn']]+dat[col_dict['celldyn']].min().abs())\n",
    "dat_viz['nmnf_celldyn']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "nnmf['other'] = NMF(n_components=nc)\n",
    "tred = nnmf['other'].fit_transform(dat[col_dict['other']]+dat[col_dict['other']].min().abs())\n",
    "dat_viz['nmnf_other']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "print(\"Reconstruction errors: ECG {}, \\t CELLDYN {}, \\t OTHER {}\".format(nnmf['ecg'].reconstruction_err_, \n",
    "                                                                         nnmf['celldyn'].reconstruction_err_,\n",
    "                                                                         nnmf['other'].reconstruction_err_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(18,7))\n",
    "sns.scatterplot(data=dat_viz['nmnf_ecg'], x='pc_0', y='pc_1', hue=y, ax=ax[0])\n",
    "sns.scatterplot(data=dat_viz['nmnf_celldyn'], x='pc_0', y='pc_1', hue=y, ax=ax[1])\n",
    "sns.scatterplot(data=dat_viz['nmnf_other'], x='pc_0', y='pc_1', hue=y, ax=ax[2])\n",
    "ax[0].set_title('NMF ECG')\n",
    "ax[1].set_title('NMF CELLDYN')\n",
    "ax[2].set_title('NMF OTHER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_ecg = UMAP(n_components=2, n_neighbors=7, min_dist=0.01)\n",
    "um_celldyn = UMAP(n_components=2, n_neighbors=7, min_dist=0.01)\n",
    "um_other = UMAP(n_components=2, n_neighbors=7, min_dist=0.01)\n",
    "um_all = UMAP(n_components=2, n_neighbors=7, min_dist=0.01)\n",
    "\n",
    "tsne_ecg = TSNE(n_components=2, perplexity=50.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=2000)\n",
    "tsne_celldyn = TSNE(n_components=2, perplexity=50.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=2000)\n",
    "tsne_other = TSNE(n_components=2, perplexity=50.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=2000)\n",
    "tsne_all = TSNE(n_components=2, perplexity=50.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cols = [__val for _val in col_dict.values() for __val in _val]\n",
    "dat_viz['umap_other'] =  pd.DataFrame(data=um_other.fit_transform(dat[col_dict['other']]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_ecg'] = pd.DataFrame(data=um_ecg.fit_transform(dat[col_dict['ecg']]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_celldyn'] = pd.DataFrame(data=um_celldyn.fit_transform(dat[col_dict['celldyn']]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_all'] = pd.DataFrame(data=um_all.fit_transform(dat[var_cols]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "\n",
    "dat_viz['tsne_other'] =  pd.DataFrame(data=tsne_other.fit_transform(dat[col_dict['other']]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_ecg'] = pd.DataFrame(data=tsne_ecg.fit_transform(dat[col_dict['ecg']]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_celldyn'] = pd.DataFrame(data=tsne_celldyn.fit_transform(dat[col_dict['celldyn']]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_all'] = pd.DataFrame(data=tsne_all.fit_transform(dat[var_cols]), index=dat.index, columns=['pc_1', 'pc_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=2, figsize=(26,16))\n",
    "sns.scatterplot(data=dat_viz['umap_ecg'], x='pc_1', y='pc_2', hue=y, ax=ax[0,0])\n",
    "sns.scatterplot(data=dat_viz['umap_celldyn'], x='pc_1', y='pc_2', hue=y, ax=ax[0,1])\n",
    "sns.scatterplot(data=dat_viz['umap_other'], x='pc_1', y='pc_2', hue=y, ax=ax[0,2])\n",
    "sns.scatterplot(data=dat_viz['umap_all'], x='pc_1', y='pc_2', hue=y, ax=ax[0,3])\n",
    "ax[0,0].set_title('UMAP ECG')\n",
    "ax[0,1].set_title('UMAP CELLDYN')\n",
    "ax[0,2].set_title('UMAP OTHER')\n",
    "ax[0,3].set_title('UMAP ALL')\n",
    "\n",
    "sns.scatterplot(data=dat_viz['tsne_ecg'], x='pc_1', y='pc_2', hue=y, ax=ax[1,0])\n",
    "sns.scatterplot(data=dat_viz['tsne_celldyn'], x='pc_1', y='pc_2', hue=y, ax=ax[1,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_other'], x='pc_1', y='pc_2', hue=y, ax=ax[1,2])\n",
    "sns.scatterplot(data=dat_viz['tsne_all'], x='pc_1', y='pc_2', hue=y, ax=ax[1,3])\n",
    "ax[1,0].set_title('TSNE ECG')\n",
    "ax[1,1].set_title('TSNE CELLDYN')\n",
    "ax[1,2].set_title('TSNE OTHER')\n",
    "ax[1,3].set_title('TSNE ALL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_all_sup = UMAP(n_components=3, n_neighbors=10, min_dist=0.1)\n",
    "dat_viz['umap_all_sup'] = pd.DataFrame(data=um_all_sup.fit_transform(dat[var_cols], \n",
    "                                                             y=y), \n",
    "                                   index=dat.index, columns=['pcu_1', 'pcu_2', 'pcu_3'])\n",
    "\n",
    "um_ecg_sup = UMAP(n_components=3, n_neighbors=10, min_dist=0.1)\n",
    "dat_viz['umap_ecg_sup'] = pd.DataFrame(data=um_ecg_sup.fit_transform(dat[var_cols], \n",
    "                                                             y=y), \n",
    "                                   index=dat.index, columns=['pcu_1', 'pcu_2', 'pcu_3'])\n",
    "\n",
    "um_celldyn_sup = UMAP(n_components=3, n_neighbors=10, min_dist=0.1)\n",
    "dat_viz['umap_celldyn_sup'] = pd.DataFrame(data=um_celldyn_sup.fit_transform(dat[var_cols], \n",
    "                                                             y=y), \n",
    "                                   index=dat.index, columns=['pcu_1', 'pcu_2', 'pcu_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(26,18))\n",
    "sns.scatterplot(data=dat_viz['umap_all_sup'], x='pcu_1', y='pcu_2', hue=y, ax=ax[0,0])\n",
    "sns.scatterplot(data=dat_viz['umap_all_sup'], x='pcu_1', y='pcu_3', hue=y, ax=ax[0,1])\n",
    "sns.scatterplot(data=dat_viz['umap_all_sup'], x='pcu_2', y='pcu_3', hue=y, ax=ax[0,2])\n",
    "\n",
    "sns.scatterplot(data=dat_viz['umap_ecg_sup'], x='pcu_1', y='pcu_2', hue=y, ax=ax[1,0])\n",
    "sns.scatterplot(data=dat_viz['umap_ecg_sup'], x='pcu_1', y='pcu_3', hue=y, ax=ax[1,1])\n",
    "sns.scatterplot(data=dat_viz['umap_ecg_sup'], x='pcu_2', y='pcu_3', hue=y, ax=ax[1,2])\n",
    "\n",
    "sns.scatterplot(data=dat_viz['umap_celldyn_sup'], x='pcu_1', y='pcu_2', hue=y, ax=ax[2,0])\n",
    "sns.scatterplot(data=dat_viz['umap_celldyn_sup'], x='pcu_1', y='pcu_3', hue=y, ax=ax[2,1])\n",
    "sns.scatterplot(data=dat_viz['umap_celldyn_sup'], x='pcu_2', y='pcu_3', hue=y, ax=ax[2,2])\n",
    "\n",
    "ax[0,0].set_title('All features')\n",
    "ax[0,1].set_title('All features')\n",
    "ax[0,2].set_title('All features')\n",
    "\n",
    "ax[1,0].set_title('ECG features')\n",
    "ax[1,1].set_title('ECG features')\n",
    "ax[1,2].set_title('ECG features')\n",
    "\n",
    "ax[2,0].set_title('Celldyn features')\n",
    "ax[2,1].set_title('Celldyn features')\n",
    "ax[2,2].set_title('Celldyn features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised UMAP clustering seems to be well able to separate the targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_all_sup = PLS(n_components=2)\n",
    "dat_viz['pls_all_sup'] = pd.DataFrame(data=pls_all_sup.fit_transform(dat[var_cols], \n",
    "                                                             y=y)[0], \n",
    "                                   index=dat.index, columns=['pls_1', 'pls_2'])\n",
    "\n",
    "pls_ecg_sup = PLS(n_components=2)\n",
    "dat_viz['pls_ecg_sup'] = pd.DataFrame(data=pls_ecg_sup.fit_transform(dat[col_dict['ecg']], \n",
    "                                                             y=y)[0], \n",
    "                                   index=dat.index, columns=['pls_1', 'pls_2'])\n",
    "\n",
    "pls_celldyn_sup = PLS(n_components=2)\n",
    "dat_viz['pls_celldyn_sup'] = pd.DataFrame(data=pls_celldyn_sup.fit_transform(dat[col_dict['celldyn']], \n",
    "                                                             y=y)[0], \n",
    "                                   index=dat.index, columns=['pls_1', 'pls_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(26,7))\n",
    "sns.scatterplot(data=dat_viz['pls_all_sup'], x='pls_1', y='pls_2', hue=y, ax=ax[0])\n",
    "sns.scatterplot(data=dat_viz['pls_ecg_sup'], x='pls_1', y='pls_2', hue=y, ax=ax[1])\n",
    "sns.scatterplot(data=dat_viz['pls_celldyn_sup'], x='pls_1', y='pls_2', hue=y, ax=ax[2])\n",
    "\n",
    "ax[0].set_title('All features')\n",
    "ax[1].set_title('ECG features')\n",
    "ax[2].set_title('Celldyn features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-decomposition\n",
    "\n",
    "PLS, CCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear separability using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_ecg = LDA(n_components=2)\n",
    "lin_sep = pd.DataFrame(data=LDA_ecg.fit_transform(dat[col_dict['ecg']], y=y), index=dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_ecg.coef_[0], columns=['coeff'], index=col_dict['ecg'])\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='0')\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='1')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('ECG LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators = dict()\n",
    "strong_separators['lda'] = pd.concat([coeff.loc[coeff.coeff>q95], coeff.loc[coeff.coeff<q05]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0\n",
    "\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear ECG. \\t negative accuracy:{}, positive accuracy:{}'.format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celldyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_celldyn = LDA(n_components=2)\n",
    "lin_sep = pd.DataFrame(data=LDA_celldyn.fit_transform(dat[col_dict['celldyn']], y=y), index=dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_celldyn.coef_[0], columns=['coeff'], index=col_dict['celldyn'])\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='0')\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='1')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('CELLDYN LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators['lda'] = pd.concat([strong_separators['lda'], coeff.loc[coeff.coeff>q95], coeff.loc[coeff.coeff<q05]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear CELLDYN. \\t negative accuracy:{}, positive accuracy:{}'.format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_other = LDA(n_components=2)\n",
    "lin_sep = pd.DataFrame(data=LDA_celldyn.fit_transform(dat[col_dict['other']], y=y), index=dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_celldyn.coef_[0], columns=['coeff'], index=col_dict['other'])\n",
    "\n",
    "q90 = coeff.quantile(0.9)[0]\n",
    "q10 = coeff.quantile(0.1)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q90, color='black')\n",
    "ax[0].axvline(q10, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='0')\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='1')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('Other LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators['lda'] = pd.concat([strong_separators['lda'], coeff.loc[coeff.coeff>q90], coeff.loc[coeff.coeff<q10]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear Other. \\t negative accuracy:{}, positive accuracy:{}'.format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_all = LDA(n_components=2)\n",
    "var_cols = get_var_cols(col_dict)\n",
    "lin_sep = pd.DataFrame(data=LDA_all.fit_transform(dat[var_cols], y=y), index=dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_all.coef_[0], columns=['coeff'], index=var_cols)\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='0')\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='1')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('ALL LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators['lda'] = pd.concat([strong_separators['lda'], coeff.loc[coeff.coeff>q95], coeff.loc[coeff.coeff<q05]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear all. \\t negative accuracy:{}, positive accuracy:{}'.format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HeartScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_hs = LDA(n_components=2)\n",
    "lin_sep = pd.DataFrame(data=LDA_hs.fit_transform(dat[hs_cols], y=y), index=dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_hs.coef_[0], columns=['coeff'], index=hs_cols)\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='0')\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='1')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('HS LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators['lda'] = pd.concat([strong_separators['lda'], coeff.loc[coeff.coeff>q95], coeff.loc[coeff.coeff<q05]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear all. \\t negative accuracy:{}, positive accuracy:{}'.format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient clustering\n",
    "\n",
    "Using weighted correlation between the patient, where the weights are obtained from prior determined feature importances, say from LDA or PCA.\n",
    "\n",
    "We can apply a pair-wise distance metric:\n",
    "* KL-divergence\n",
    "* cosine, euclidean etc.\n",
    "\n",
    "Where it makes sense to include factor weights based on the separability.\n",
    "\n",
    "Clustering options \n",
    "* Spectral clustering\n",
    "* Louvain method (Fast community unfolding)\n",
    "* Label propagation\n",
    "* Walktrap community\n",
    "* Edge betweenness community\n",
    "* Leading eigenvector community\n",
    "* Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hausdorff distance between sample groups\n",
    "See the [wiki](https://en.wikipedia.org/wiki/Hausdorff_distance).\n",
    "\n",
    "SciPy has a function readily available, see [directed Hausdorff](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.directed_hausdorff.html)\n",
    "\n",
    "One way to find patient clusters is to maximize the Hausdoff distance between patient groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised UMAP performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = get_var_cols(col_dict)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat[cols], y, test_size=0.1, random_state=89)\n",
    "\n",
    "if remove_outlying_samples_from_test:\n",
    "    iso = IsolationForest(n_estimators=400, n_jobs=4)\n",
    "    iso.fit(X_train)\n",
    "    out_in = iso.predict(X_test)\n",
    "    X_test = X_test[np.where(out_in==1),:]\n",
    "    y_test = y_test[np.where(out_in==1)]\n",
    "    \n",
    "\n",
    "umap = UMAP(n_components=2, n_neighbors=7, min_dist=0.07)\n",
    "umap.fit(X_train, y_train)\n",
    "\n",
    "train_transform = pd.DataFrame(data=umap.transform(X_train), columns=['pcu_1', 'pcu_2'])\n",
    "test_transform = pd.DataFrame(data=umap.transform(X_test), columns=['pcu_1', 'pcu_2'])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(26,7))\n",
    "sns.scatterplot(data=train_transform, x='pcu_1', y='pcu_2', hue=y_train.values, ax=ax[0])\n",
    "sns.scatterplot(data=test_transform, x='pcu_1', y='pcu_2', hue=y_test.values, ax=ax[1])\n",
    "ax[0].set_title('All features, training set')\n",
    "ax[1].set_title('All features, test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = get_var_cols(col_dict)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=30, metric='minkowski', p=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat[cols], y, test_size=0.05, random_state=None)\n",
    "umap = UMAP(n_components=31, n_neighbors=30, min_dist=0.01)\n",
    "umap.fit(X_train, y_train)\n",
    "\n",
    "train_transform = umap.transform(X_train)\n",
    "test_transform = umap.transform(X_test)\n",
    "\n",
    "clf.fit(train_transform, y_train)\n",
    "y_pred = clf.predict(test_transform)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "model = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LDA()\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['LDA'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "model['LDA'] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=500, penalty='l1', solver='liblinear')\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['LR'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "model['LR'] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['RF'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "model['RF'] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nuSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NuSVC(nu=0.5)\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['SVC'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "model['SVC'] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25), max_iter=200, learning_rate_init=0.005, learning_rate='adaptive')\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['MLP'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "model['MLP'] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "We combine: RF+nuSVM+LR+LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [('RF', RandomForestClassifier(n_estimators=200)),\n",
    "        ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "        ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "        ('MLP', MLPClassifier(hidden_layer_sizes=(100,50,50), \n",
    "                              max_iter=200, learning_rate_init=0.005, learning_rate='adaptive'))]\n",
    "\n",
    "\n",
    "clf = VotingClassifier(clfs, voting='soft', weights=[10, 5, 7, 3])\n",
    "\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['ensemble_vote'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, \n",
    "                                                      scoring=scoring, \n",
    "                                                      cv=10, return_train_score=True))\n",
    "model['ensemble_vote'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=LogisticRegression(max_iter=500, solver='liblinear'), n_estimators=5)\n",
    "\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'prec_macro': 'precision_macro', \n",
    "           'rec_macro': make_scorer(recall_score, average='macro')}\n",
    "scores['ensemble_bag'] = pd.DataFrame(cross_validate(clf, dat[cols].values, y.values, \n",
    "                                                    scoring=scoring, \n",
    "                                                     cv=10, return_train_score=True))\n",
    "model['ensemble_bag'] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised SOM performance\n",
    "\n",
    "SuSi: [paper](https://arxiv.org/abs/1903.11114), [code](https://github.com/felixriese/susi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import susi\n",
    "\n",
    "cols = get_var_cols(col_dict)\n",
    "cclf = susi.SOMClustering(n_rows=31, n_columns=31)\n",
    "cclf.fit(dat[cols].values)\n",
    "print(\"-- SOM fitted --\")\n",
    "\n",
    "u_matrix = cclf.get_u_matrix()\n",
    "plt.imshow(np.squeeze(u_matrix), cmap=\"inferno_r\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = susi.SOMClassifier(n_rows=31,\n",
    "                         n_columns=31,\n",
    "                         n_iter_unsupervised=5000,\n",
    "                         n_iter_supervised=5000,\n",
    "                         random_state=0)\n",
    "clf.fit(X_train, y_train.astype(np.int32))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Vector Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neupy import algorithms\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat[cols], y, test_size=0.05, random_state=None)\n",
    "\n",
    "# reduce\n",
    "#red = UMAP(n_components=31, n_neighbors=30, min_dist=0.01)\n",
    "red = PCA(n_components=30)\n",
    "red.fit(X_train)\n",
    "X_train = red.transform(X_train)\n",
    "X_test = red.transform(X_test)\n",
    "\n",
    "\n",
    "clf = algorithms.LVQ(n_inputs=X_train.shape[1], n_classes=2, n_subclasses=11, shuffle_data=False)\n",
    "clf.train(X_train, y_train, epochs=500)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "clf = algorithms.LVQ21(n_inputs=X_train.shape[1], n_classes=2, n_subclasses=11, shuffle_data=False)\n",
    "clf.train(X_train, y_train, epochs=500)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "clf = algorithms.LVQ3(n_inputs=X_train.shape[1], n_classes=2, n_subclasses=11, shuffle_data=False)\n",
    "clf.train(X_train, y_train, epochs=500)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
