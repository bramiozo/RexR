{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import SimpSOM as sps\n",
    "import minisom as msom\n",
    "import sompy\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "import inspect\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "\n",
    "from sklearn.cluster import SpectralClustering, AffinityPropagation, OPTICS, AgglomerativeClustering\n",
    "import HDBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation:\n",
    "* standard scaling\n",
    "* robust scaler \n",
    "\n",
    "Feature reduction:\n",
    "* minimum univariate distribution difference\n",
    "* PCA \n",
    "* UMAP\n",
    "\n",
    "Clustering method:\n",
    "* HDBSCAN\n",
    "* Spectral\n",
    "* SOM\n",
    "\n",
    "This analysis should provide us with an intuition of the separability of the targets \n",
    "with the given features.\n",
    "\n",
    "**Output**: clusters can be used as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"T:\\laupodteam\\AIOS\\Bram\")\n",
    "HS = pd.read_csv(\"data/HeartScore/Data/MATRIX_FULL_23jul2019_ECG.csv\", sep=\";\")\n",
    "\n",
    "index_cols = ['pathos_key', 'upod_id'] \n",
    "date_cols = ['AcquisitionDateTime_ECG'] \n",
    "meta_cols = ['setsrc', 'Door', 'Analyzer']\n",
    "pheno_cols = ['AGE', 'gender', 'BMI', 'RF_Diab', 'RF_Smok', 'RF_HyperTens', 'RF_HyperChol', 'RF_CVDHist', 'RF_FamHist', 'RF_obese30']\n",
    "hs_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_new', 'HS_RiskFacts', 'HS_new2']\n",
    "tn_cols = ['tn_admission', 'tn_slope2', 'HN_TN']\n",
    "\n",
    "# convert bool in int64\n",
    "for _col in HS.columns.tolist():\n",
    "    if str(HS[_col].dtype)=='bool':\n",
    "        HS[_col] = HS[_col].astype(int)\n",
    "        \n",
    "target = 'casecontrol'\n",
    "HS.rename(index=str, columns={target: 'target'}, inplace=True)\n",
    "tmap = {'Control': 0, 'NSTEMI': 1}\n",
    "HS['target'] = HS.target.map(tmap)\n",
    "\n",
    "gmap = {'M': 0, 'F': 1}\n",
    "HS['gender'] = HS.gender.map(gmap)\n",
    "\n",
    "HS.set_index(index_cols, inplace=True)\n",
    "\n",
    "cols = HS.columns.tolist()\n",
    "var_cols = list(set(cols) - set(meta_cols) - set(index_cols) -set(date_cols) - set(['target']))\n",
    "\n",
    "cell_dyn_cols = [\"c_b_wbc\",\"c_b_wvf\",\"c_b_neu\",\"c_b_seg\",\"c_b_bnd\",\"c_b_ig\",\"c_b_lym\",\"c_b_lyme\",\"c_b_vlym\",\"c_b_mon\",\"c_b_mone\",\"c_b_blst\",\n",
    "                 \"c_b_eos\",\"c_b_bas\",\"c_b_pneu\",\"c_b_pseg\",\"c_b_pbnd\",\"c_b_pig\",\"c_b_plym\",\"c_b_plyme\",\"c_b_pvlym\",\"c_b_pmon\",\"c_b_pmone\",\n",
    "                 \"c_b_pblst\",\"c_b_peos\",\"c_b_pbas\",\"c_b_namn\",\"c_b_nacv\",\"c_b_nimn\",\"c_b_nicv\",\"c_b_npmn\",\"c_b_npcv\",\"c_b_ndmn\",\n",
    "                 \"c_b_ndcv\",\"c_b_nfmn\",\"c_b_nfcv\",\"c_b_Lamn\",\"c_b_Lacv\",\"c_b_Limn\",\"c_b_Licv\",\"delay_Celldyn\"]\n",
    "ecg_cols_agg =  [\"VentricularRate_ECG\",\"AtrialRate_ECG\",\"P_RInterval_ECG\",\"QRS_Duration_ECG\",\"Q_TInterval_ECG\",\n",
    "                 \"QTCCalculation_ECG\",\"PAxis_ECG\",\"RAxis_ECG\",\"TAxis_ECG\",\"QRSCount_ECG\",\"QOnset_ECG\",\n",
    "                 \"QOffset_ECG\",\"POnset_ECG\",\"POffset_ECG\",\"T_Onset_ECG\",\"T_Offset_ECG\",\"QRS_Onset_ECG\",\n",
    "                 \"QRS_Offset_ECG\",\"AvgRRInterval_ECG\",\"QTcFredericia_ECG\",\"QTcFramingham_ECG\",\"QTc_Bazett_ECG\"]\n",
    "\n",
    "ecg_leads = ['Lead_I_', 'Lead_II_', 'Lead_III_', 'Lead_V1_', 'Lead_V2_', 'Lead_V3_', 'Lead_V4_', 'Lead_V5_', 'Lead_V6_', 'Lead_aVF_', 'Lead_aVL_', 'Lead_aVR_']\n",
    "ecg_msrmnt = ['MaxST_ECG',  'Max_R_Ampl_ECG', 'Max_S_Ampl_ECG', 'MinST_ECG', 'PFull_Area_ECG', 'PP_Area_ECG', 'PP_Duration_ECG',\n",
    " 'PP_PeakAmpl_ECG', 'PP_PeakTime_ECG', 'P_Area_ECG', 'P_Duration_ECG', 'P_PeakAmpl_ECG', 'P_PeakTime_ECG', 'QRS_Area_ECG', 'QRS_Balance_ECG',\n",
    " 'QRS_Deflection_ECG', 'QRSint_ECG', 'Q_Area_ECG', 'Q_Duration_ECG', 'Q_PeakAmpl_ECG', 'Q_PeakTime_ECG', 'RP_Area_ECG', 'RP_Duration_ECG', 'RP_PeakAmpl_ECG',\n",
    " 'RP_PeakTime_ECG', 'R_Area_ECG', 'R_Duration_ECG', 'R_PeakAmpl_ECG', 'R_PeakTime_ECG', 'SP_Area_ECG', 'SP_Duration_ECG', 'SP_PeakAmpl_ECG', \n",
    " 'SP_PeakTime_ECG', 'STE_ECG', 'STJ_ECG', 'STM_ECG', 'S_Area_ECG', 'S_Duration_ECG', 'S_PeakAmpl_ECG', 'S_PeakTime_ECG',\n",
    " 'TFull_Area_ECG', 'TP_Area_ECG', 'TP_Duration_ECG', 'TP_PeakAmpl_ECG', 'TP_PeakTime_ECG', 'T_Area_ECG', 'T_Duration_ECG', 'T_End_ECG',\n",
    " 'T_PeakAmpl_ECG', 'T_PeakTime_ECG', 'T_Special_ECG', 'P_OnsetAmpl_ECG']\n",
    "\n",
    "\n",
    "ecg_cols_dyn = [_lead+_msrmnt for _lead in ecg_leads for _msrmnt in ecg_msrmnt]\n",
    "\n",
    "ecg_cols_agg = list(set(ecg_cols_agg) & set(var_cols))\n",
    "ecg_cols_dyn = list(set(ecg_cols_dyn) & set(var_cols))\n",
    "cell_dyn_cols = list(set(cell_dyn_cols) & set(var_cols))\n",
    "\n",
    "ecg_cols = list(set(ecg_cols_agg+ecg_cols_dyn))\n",
    "other_cols = list(set(var_cols)-set(ecg_cols)-set(cell_dyn_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= StandardScaler() # StandardScaler(), MinMaxScaler(), RobustScaler()\n",
    "dim_reduction = {'ecg': PCA(n_components=6), 'celldyn': PCA(n_components=20)} # dict with dimension reduction per data group, or one dim red for all, or None, methods: PCA, NMF, UMAP\n",
    "# dict with column name and impute type: median, mean, remove, regressor, (nmf?), or None, or knnimputer, or iterative which uses a round-robin approach using BayesianRidge as the regressor\n",
    "imputance = {'BMI': 'median', \n",
    "             'P_RInterval_ECG': BayesianRidge(), \n",
    "             'POnset_ECG': BayesianRidge(), \n",
    "             'PAxis_ECG': RANSACRegressor(), \n",
    "             'POffset_ECG': ExtraTreesRegressor(n_estimators=200),\n",
    "             'delay_Celldyn': MLPRegressor(hidden_layer_sizes=(80,50,30))}\n",
    "\n",
    "feature_weights = 'glm' # glm, tree, gam\n",
    "clustering = 'hdbscan' # hdbscan, SOM, spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "if scaling is not None:\n",
    "    dat = pd.DataFrame(data=scaler.fit_transform(HS[var_cols]), index=HS.index, columns=var_cols)\n",
    "    dat = dat.join(HS[meta_cols])\n",
    "else:\n",
    "    dat = HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputance\n",
    "nan_cols = list(dat[var_cols].isna().sum()[dat[var_cols].isna().sum()>0].index)\n",
    "if imputance is not None:\n",
    "    if isinstance(imputance, dict):\n",
    "        for _imp_key, _imp_val in imputance.items():\n",
    "            if type(_imp_val)==str:\n",
    "                if _imp_val == 'median': \n",
    "                    dat.loc[dat[_imp_key].isna(), _imp_key] = np.nanmedian(dat[_imp_key])\n",
    "                elif _imp_val == 'mean':\n",
    "                    dat.loc[dat[_imp_key].isna(), _imp_key] = np.nanmean(dat[_imp_key])\n",
    "                elif _imp_val == 'remove':\n",
    "                    dat = dat.dropna(subset=[_imp_key])\n",
    "            elif 'sklearn' in str(type(_imp_val)):  \n",
    "                _sub_cols = list(set(var_cols)  - set(nan_cols))\n",
    "                _y = dat.loc[~dat[_imp_key].isna(), _imp_key]\n",
    "                _X_train = dat.loc[~dat[_imp_key].isna(), _sub_cols]\n",
    "                _X_test = dat.loc[dat[_imp_key].isna(), _sub_cols]\n",
    "                try:\n",
    "                    dat.loc[dat[_imp_key].isna(), _imp_key] = _imp_val.fit(_X_train, _y).predict(_X_test)\n",
    "                except Exception as e:\n",
    "                    print(\"Imputance failed for {}, shapes: {}, {}, {}\".format(_imp_key, _X_train.shape, _y.shape, _X_test.shape))\n",
    "                    if _X_test.shape[0]==0:\n",
    "                        print(\"Hmm, you probably already ran the imputer, please reload the data...\")\n",
    "    else:\n",
    "        if imputance=='iterative':\n",
    "            imp = IterativeImputer(estimator=BayesianRidge(), max_iter=10)\n",
    "        elif imputance=='knnimputer':\n",
    "            imp= KNNImputer(n_neighbors=5)\n",
    "            \n",
    "        dat = pd.DataFrame(data=imp.fit_transform(dat[var_cols]), index=HS.index, columns=var_cols)\n",
    "        dat = dat.join(dat[meta_cols])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim reduction\n",
    "if dim_reduction is not None:\n",
    "    if isinstance(dim_reduction, dict):\n",
    "        assert(set(dim_reduction.keys()).issubset(['ecg', 'celldyn'])), \"Check the dim_reduction keys\"\n",
    "        ecols = ['ecg_'+str(i) for i in range(0, dim_reduction['ecg'].n_components)]\n",
    "        ecg_red = pd.DataFrame(dim_reduction['ecg'].fit_transform(dat[ecg_cols]), index=dat.index, columns=ecols)\n",
    "        ccols = ['celldyn_'+str(i) for i in range(0, dim_reduction['celldyn'].n_components)]\n",
    "        celldyn_red = pd.DataFrame(dim_reduction['celldyn'].fit_transform(dat[cell_dyn_cols]), index=dat.index, columns=ccols)\n",
    "        \n",
    "        dat_red = dat[other_cols].join(ecg_red).join(celldyn_red)\n",
    "    else:\n",
    "        rcols = ['red_'+str(i) for i in range(0, dim_reduction['ecg'].n_components)]\n",
    "        tot_red = pd.DataFrame(dim_reduction.fit_transform(dat[var_cols]), index=dat.index, columns=rcols)\n",
    "        dat_red = dat[other_cols].join(tot_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodic Boundary Conditions active.\n",
      "The weights will be initialised with PCA.\n",
      "Training SOM... done!\n"
     ]
    }
   ],
   "source": [
    "# Clustering\n",
    "# Cluster both dat_red and dat\n",
    "# SOM: https://github.com/fcomitani/SimpSOM\n",
    "# miniSOM: https://github.com/JustGlowing/minisom\n",
    "# SOMPY: https://gist.github.com/sevamoo/035c56e7428318dd3065013625f12a11\n",
    "# customSOM : https://pythonhosted.org/kohonen/_modules/kohonen/kohonen.html\n",
    "net = sps.somNet(24, 24, dat[var_cols].values, PBC=True, n_jobs=4, PCI=True)\n",
    "net.train(0.01, 1000)\n",
    "net.nodes_graph(colnum=0)\n",
    "net.diff_graph()\n",
    "#Project the datapoints on the new 2D network map.\n",
    "net.project(dat[var_cols].values, labels=HS.target.values) #  labels=labels\n",
    "#Cluster the datapoints according to the Quality Threshold algorithm.\n",
    "net.cluster(dat[var_cols].values, type='qthresh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral\n",
    "# pre-set 2 clusters\n",
    "\n",
    "# Agglomerative \n",
    "# pre-set 2 clusters\n",
    "\n",
    "# OPTICS \n",
    "\n",
    "\n",
    "# HDBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separability of clusters by target\n",
    "y = HS[['target']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# One more try, using supervised clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
