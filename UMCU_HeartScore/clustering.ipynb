{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 0\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "sns.set()\n",
    "sns.set_style(\"darkgrid\")\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "\n",
    "from scipy.stats import ks_2samp as ks2\n",
    "from scipy.stats import mannwhitneyu as mwu\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import wasserstein_distance as w1_dist\n",
    "from scipy.stats import energy_distance as w2_dist\n",
    "from scipy.stats import epps_singleton_2samp as epps\n",
    "from scipy.spatial.distance import directed_hausdorff as hausdorff\n",
    "from scipy.spatial.distance import correlation, euclidean, cosine, cityblock\n",
    "#from category_encoders import *\n",
    "\n",
    "#import SimpSOM as sps\n",
    "#import minisom as msom\n",
    "#import sompy\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "import itertools\n",
    "#from varname import varname\n",
    "import inspect\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier, \n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import OneClassSVM, NuSVC\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, BayesianRidge, LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA, NMF, KernelPCA, FactorAnalysis as FA, MiniBatchDictionaryLearning as DL\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.cross_decomposition import PLSRegression as PLS, CCA\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "from sklearn.cluster import SpectralClustering, AffinityPropagation, OPTICS, AgglomerativeClustering\n",
    "#import debacl as dcl\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from xgboost import XGBClassifier as XGB\n",
    "from lightgbm import LGBMClassifier as LGBM\n",
    "from catboost import CatBoostClassifier as CATB\n",
    "from interpret.glassbox import ExplainableBoostingClassifier as EBM\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Input \n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "from ngboost import NGBRegressor, NGBClassifier\n",
    "from ngboost.distns import LogNormal, Normal, k_categorical, Bernoulli\n",
    "from ngboost.learners import default_tree_learner\n",
    "from rulefit import RuleFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "This notebooks forms the playground for different methods:\n",
    "* graph clustering for multicollinearity removal\n",
    "* set distance for feature inclusion\n",
    "* repeated quantile binning to create a privacy and overfitting robust dataset\n",
    "* unsupervised model directives: use dimension reduction with density-based clustering techniques to define specific models. for each cluster.\n",
    "    1. identify cluster\n",
    "    2. apply appropriate model\n",
    "* comparison of methods: \n",
    "    * **[RuleFit](https://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf)**, XGboost, LGBM, CATboost, ExtraTrees, RandomForest, NGBoost \n",
    "    * GAM's\n",
    "    * SVM's\n",
    "    * NN's: 1D CNN's, DNN's\n",
    "    * GCP, NB, LR\n",
    "* comparison of clustering methods: HDBSCAN, OPTICS, spectral clustering\n",
    "* multi-omic integration methods like: sparse CCA/ICA/PLS/SNF/JNMF \n",
    "* making the results [interpretable](https://christophm.github.io/interpretable-ml-book/)\n",
    "\n",
    "Idea:\n",
    "* Use dimension reduction with the number of components equal to the number of feature with reduction weights as model feature weights will significantly increase performance compared to the non-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= {'ecg': QuantileTransformer(n_quantiles=100, output_distribution='uniform'), \n",
    "         'celldyn': QuantileTransformer(n_quantiles=100, output_distribution='uniform'), \n",
    "         'other': MinMaxScaler()} # StandardScaler(), MinMaxScaler(), RobustScaler() or None\n",
    "\n",
    "scaling_apriori = False\n",
    "scaling_aposteriori = True\n",
    "\n",
    "# dict with column name and impute type: median, mean, remove, regressor, (nmf?), or None, or knnimputer, or iterative which uses a round-robin approach using BayesianRidge as the regressor\n",
    "feature_expansion_intra = ['celldyn', 'ecg_cols_agg']\n",
    "feature_expansion_inter = [('pheno', 'celldyn'), \n",
    "                           ('pheno', 'ecg_cols_agg'), \n",
    "                           ('history', 'celldyn'), \n",
    "                           ('history', 'ecg_cols_agg')]\n",
    "\n",
    "#feature_weights = 'glm' # glm, tree, gam\n",
    "#clustering = 'hdbscan' # hdbscan, SOM, spectral\n",
    "remove_nan_patients = False\n",
    "# NGBRegressor(), BayesianRidge(), MLPRegressor(hidden_layer_sizes=(70,70,30)), https://blog.stata.com/2011/08/22/use-poisson-rather-than-regress-tell-a-friend/\n",
    "# P-features, P_RInterval_ECG/POnset_ECG/PAxis_ECG/POffset_ECG, if NaN, could mean absence of initial polar. which could mean atrium fibr.\n",
    "imputance = {'BMI': 'median', \n",
    "             'P_RInterval_ECG': None, \n",
    "             'POnset_ECG': None, \n",
    "             'PAxis_ECG': None, \n",
    "             'POffset_ECG': None}\n",
    "missing_dummy = True\n",
    "remove_nan_cols = True # remove only those columns with no imputance\n",
    "variance_remove = True\n",
    "remove_multicoll = True\n",
    "remove_outlying_samples_from_train = True\n",
    "remove_outlying_samples_from_test = False\n",
    "remove_weak_univariates = False\n",
    "univariate_inclusive = False\n",
    "keep_strongest_N = 1500 # keep strongest N features based on univariate scores\n",
    "remove_low_diffEntropy = False\n",
    "run_supervised_umap = False\n",
    "experimental_supervisors = False\n",
    "model_comparison = True\n",
    "patient_clustering = True\n",
    "make_simple_featureset_comparison = True\n",
    "extensive_feature_importances = False\n",
    "\n",
    "hausdorff_feature_builder = False\n",
    "wasserstein_feature_builder = False\n",
    "pairwisedistance_feature_builder = True\n",
    "complete_dist_improvement = False\n",
    "prune_cols = ['ecg', 'celldyn']\n",
    "base_cols = ['pheno', 'history', 'troponine']\n",
    "extratrees_feature_builder = True\n",
    "top_n_all = keep_strongest_N\n",
    "top_n = {'ecg': 300, 'celldyn': 300, \n",
    "         'pheno_celldyn': 25, \n",
    "         'pheno_ecg_cols_agg': 25, \n",
    "         'history_celldyn': 100, \n",
    "         'history_ecg_cols_agg': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_accs(df):    \n",
    "    set1 = df.loc[df.THRESHOLD>0.5].groupby(['featureset', 'with_celldyn']).mean()\n",
    "    set1['threshold_minimum'] = 0.5\n",
    "\n",
    "    set2 = df.loc[df.THRESHOLD>0.6].groupby(['featureset', 'with_celldyn']).mean()\n",
    "    set2['threshold_minimum'] = 0.6\n",
    "\n",
    "    set3 = df.loc[df.THRESHOLD>0.7].groupby(['featureset', 'with_celldyn']).mean()\n",
    "    set3['threshold_minimum'] = 0.7\n",
    "\n",
    "    set3 = df.loc[df.THRESHOLD>0.8].groupby(['featureset', 'with_celldyn']).mean()\n",
    "    set3['threshold_minimum'] = 0.8\n",
    "\n",
    "    set4 = df.loc[df.THRESHOLD>0.9].groupby(['featureset', 'with_celldyn']).mean()\n",
    "    set4['threshold_minimum'] = 0.9\n",
    "\n",
    "    set5 = df.loc[df.THRESHOLD>0.95].groupby(['featureset', 'with_celldyn']).mean()\n",
    "    set5['threshold_minimum'] = 0.95\n",
    "\n",
    "    acc_agg = pd.concat([set1,set2,set3,set4,set5])\n",
    "    acc_agg.reset_index(inplace=True)\n",
    "    acc_agg.sort_values(by=['threshold_minimum', 'featureset', 'with_celldyn'], inplace=True)\n",
    "    return acc_agg\n",
    "\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "\n",
    "@jit\n",
    "def amean(X):\n",
    "    return np.mean(X, axis=1)\n",
    "    \n",
    "@jit\n",
    "def hmean(X):\n",
    "    # only meaningful for positive real numbers\n",
    "    return sc.stats.hmean(X, axis=1)\n",
    "\n",
    "@jit\n",
    "def gmean(X):\n",
    "    return sc.stats.gmean(X, axis=1)\n",
    "\n",
    "def process_cliques(X, agg_function='amean', cliques=None, cols=None, prefix='mcoll_'):\n",
    "    '''\n",
    "    X: array containing columns that we want to change\n",
    "    agg_function: aggregation function for the clique, amean, hmean, gmean\n",
    "    cliques: list of lists with column indices\n",
    "    cols: column indices/names to ensure the indices in X are correct, if None, assumes that the column indices in cliques refer to X-cols directly\n",
    "    prefix: prefix for the new columns\n",
    "    \n",
    "    returns: X with changed columns, list of column names/indices added to the array\n",
    "    '''\n",
    "    \n",
    "    fun = amean if agg_function=='amean' else hmean if agg_function=='hmean'  else gmean\n",
    "    \n",
    "    _X = X[cols].values if cols is not None else X.values\n",
    "    \n",
    "    n_rows  = _X.shape[0]\n",
    "    n_cols  = len(cliques)\n",
    "    \n",
    "    new_cols = []\n",
    "    X_add = np.zeros((n_rows, n_cols))\n",
    "    for j, _clique in enumerate(cliques):\n",
    "        __X = _X[0:n_rows, _clique]\n",
    "        X_add[:, j] = fun(__X)\n",
    "        new_cols.append(prefix+\"clique_\"+str(j))\n",
    "    return pd.DataFrame(data=X_add, columns=new_cols, index=X.index) \n",
    "\n",
    "# univariate outlier replacer\n",
    "def featurewise_outlier_replacer(x, q=(0.01, 0.99), how='quantile'):\n",
    "    if how=='quantile':\n",
    "        # improvement..instead of blunt replacement with fixed number use sampler\n",
    "        lv, rv = x.quantile(q[0]), x.quantile(q[1])\n",
    "        t = x.copy()\n",
    "        t[t>rv] = rv\n",
    "        t[t<lv] = lv\n",
    "        return t\n",
    "\n",
    "def _condition_number(x, ignore_nan=True):\n",
    "    return np.linalg.cond(x.dropna(), p=2)\n",
    "\n",
    "def wass_sup_distance(x, y, merge=np.product, wass=w1_dist):\n",
    "    return wass(merge(x, axis=1), merge(y, axis=1))\n",
    "\n",
    "def get_pca_weights(reducer=None, cols=None, ncomp=10):\n",
    "    pcweights = np.zeros((len(cols),))\n",
    "    wt = 0\n",
    "    for pc in range(0, ncomp):\n",
    "        w = ncomp-pc\n",
    "        wt = wt + w\n",
    "        pcweights = w*np.abs(reducer.components_[pc]) + pcweights\n",
    "    pcweights = pcweights/wt\n",
    "    pcw = pcweights[np.argsort(pcweights)]\n",
    "    _cols = np.array(cols)[np.argsort(pcweights)]\n",
    "    return dict(zip(_cols,pcw))\n",
    "\n",
    "def get_var_cols(d):\n",
    "    assert isinstance(d, dict), 'input variables is not a dictionary'\n",
    "    return list(set(list(itertools.chain.from_iterable(d.values()))))\n",
    "\n",
    "def _rem_cols(cd, rem):\n",
    "    assert isinstance(cd, dict), 'Input variable 1 should be a dictionary'\n",
    "    assert isinstance(rem, list), 'Input variable 2 should be a list'\n",
    "    for _key in cd.keys():\n",
    "        cd[_key] = list(set(cd[_key]) - set(rem))\n",
    "    return cd\n",
    "\n",
    "def pairwise_distance(x, y, merge=np.mean, dist=correlation):\n",
    "    return dist(merge(x, axis=0), merge(y, axis=0))\n",
    "\n",
    "@jit\n",
    "def _ssqrt(x, y):\n",
    "    return np.sign(x*y)*np.sqrt(np.multiply(np.abs(x), np.abs(y)))\n",
    "@jit\n",
    "def _divisor(x, y, eps=1e-3):    \n",
    "    return np.sign(x)*np.divide((x**2), (x**2+y**2+eps))\n",
    "@jit\n",
    "def _sum(x, y):\n",
    "    return x+y\n",
    "@jit\n",
    "def expander2(x, y=None, fun=None):\n",
    "    '''\n",
    "        x : np array \n",
    "        fun : expansion function, assumes pairwise expansion\n",
    "    '''\n",
    "    if y is None:\n",
    "        num_rows, num_cols = x.shape[0], x.shape[1]\n",
    "        _num_cols = np.int((num_cols**2-num_cols)/2)\n",
    "        xex = np.zeros(shape=(num_rows, _num_cols)) \n",
    "        k=0\n",
    "        for jl in range(0, num_cols):\n",
    "            for jr in range(jl+1, num_cols):                \n",
    "                xex[:, k] =   fun(x[:,jl], x[:,jr])\n",
    "                k +=1\n",
    "    else:\n",
    "        num_rows_l, num_rows_r, num_cols_l, num_cols_r = x.shape[0], y.shape[0], x.shape[1], y.shape[1]\n",
    "        _num_cols = np.int(num_cols_l*num_cols_r)\n",
    "        xex = np.zeros(shape=(num_rows_l, _num_cols))\n",
    "        k=0\n",
    "        for jl in range(0, num_cols_l):\n",
    "            for jr in range(0, num_cols_r):               \n",
    "                xex[:, k] =   fun(x[:,jl], y[:,jr])\n",
    "                k +=1\n",
    "    return xex\n",
    "\n",
    "def _cols_(cols, cols_r=None, prefix=None):\n",
    "    prefix = prefix+\"_\" if prefix is not None else \"\"\n",
    "    ms = len(cols)+1 if cols_r==None else 0\n",
    "    cols_r = cols if cols_r is None else cols_r\n",
    "    return [prefix+cols[jl]+\"_\"+cols_r[jr] for jl in range(0, len(cols)) for jr in range(np.min([jl+1, ms]), len(cols_r))]\n",
    "\n",
    "@jit\n",
    "def _diff_entropy(x, eps=1e-6, bins=20):\n",
    "    rhos, xs = np.histogram(x, density=True, bins=bins)\n",
    "    xmean =  (xs[1:] + xs[:-1])/2\n",
    "    xdiff = xs[1:] - xs[:-1]\n",
    "    H = -np.sum(rhos*np.log(rhos+eps)*xdiff)\n",
    "    Hr = H/np.sum(xdiff)\n",
    "    return Hr\n",
    "\n",
    "# add relative entropy  -np.sum(rhos*np.log(rhos/rhos_ref+eps)*xdiff) where rhos_ref is a reference distribution\n",
    "# for the relative distribution we can use the average empirical distribution of the features\n",
    "\n",
    "class fs_ws1():\n",
    "    scores_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore = w1_dist(pos[:,column], neg[:,column])\n",
    "        return zscore\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = results_\n",
    "        return self\n",
    "    \n",
    "class fs_ws2():\n",
    "    scores_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore = w2_dist(pos[:,column], neg[:,column])\n",
    "        return zscore\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = results_\n",
    "        return self\n",
    "    \n",
    "class fs_mannwhitney():\n",
    "    pvalues_ = None\n",
    "    scores_ = None\n",
    "    results_ = None\n",
    "    def __init__(self, pvalue = 0.01, mode='auto'):\n",
    "        # mode : 'auto', 'exact', 'asymp'\n",
    "        self.pvalue = pvalue\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore, p_value = mwu(pos[:,column], neg[:,column], alternative=\"less\") # mode=self.mode\n",
    "        return zscore, p_value\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        self.results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = self.results_[:, 0]\n",
    "        self.pvalues_ = self.results_[:, 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        not_signif = self.p_values<self.pvalue\n",
    "        to_delete = [idx for idx, item in enumerate(not_signif) if item == False]\n",
    "        return np.delete(x, to_delete, axis = 1), to_delete\n",
    "\n",
    "class fs_ks():\n",
    "    pvalues_ = None\n",
    "    scores_ = None\n",
    "    results_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore, p_value = ks2(pos[:,column], neg[:,column])\n",
    "        return zscore, p_value\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        self.results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = self.results_[:, 0]\n",
    "        self.pvalues_ = self.results_[:, 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        not_signif = self.p_values<self.pvalue\n",
    "        to_delete = [idx for idx, item in enumerate(not_signif) if item == False]\n",
    "        return np.delete(x, to_delete, axis = 1), to_delete\n",
    "\n",
    "class fs_epps():\n",
    "    pvalues_ = None\n",
    "    scores_ = None\n",
    "    results_ = None\n",
    "    def __init__(self, pvalue = 0.01):\n",
    "        self.pvalue = pvalue\n",
    "\n",
    "    def apply_test(self, pos, neg, column):\n",
    "        zscore, p_value = epps(pos[:,column], neg[:,column])\n",
    "        return zscore, p_value\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        zero_idx = np.where(y == 0)[0]\n",
    "        one_idx = np.where(y == 1)[0]\n",
    "        pos_samples = x[one_idx]\n",
    "        neg_samples = x[zero_idx]                \n",
    "        self.results_ = np.array(list(map(lambda c: \n",
    "            self.apply_test(pos_samples, neg_samples, c), range(0,x.shape[1]))))\n",
    "        self.scores_ = self.results_[:, 0]\n",
    "        self.pvalues_ = self.results_[:, 1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        not_signif = self.p_values<self.pvalue\n",
    "        to_delete = [idx for idx, item in enumerate(not_signif) if item == False]\n",
    "        return np.delete(x, to_delete, axis = 1), to_delete\n",
    "\n",
    "def mcc(y_true, y_prob, thresh=0.5, greedy='symmetric'):\n",
    "    if greedy=='symmetric':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(1-thresh)) & (y_true==1))\n",
    "    elif greedy=='negative':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(thresh)) & (y_true==1))        \n",
    "    mcc = (TP*TN - FP*FN)/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)) if (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)>0 else np.nan\n",
    "    return mcc\n",
    "    \n",
    "def balanced_accuracy(y_true, y_prob, thresh=0.5, greedy='symmetric'):\n",
    "    if greedy=='symmetric':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(1-thresh)) & (y_true==1))\n",
    "    elif greedy=='negative':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(thresh)) & (y_true==1))\n",
    "        \n",
    "    recall = TP/(FN+TP) if (FN+TP)>0 else np.nan\n",
    "    specificity = TN/(FP+TN) if (FP+TN)>0 else np.nan\n",
    "    return 0.5*(recall+specificity)\n",
    "\n",
    "def fb_score(y_true, y_prob, beta=1, thresh=0.5, greedy='symmetric'):\n",
    "    if greedy=='symmetric':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(1-thresh)) & (y_true==1)) \n",
    "    elif greedy=='negative':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(thresh)) & (y_true==1))         \n",
    "    prec = TP/(TP+FP) if (TP+FP)>0 else np.nan\n",
    "    rec =  TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
    "    return (1+beta**2)*prec*rec/(beta**2*prec+rec)\n",
    "\n",
    "def npv(y_true, y_prob, thresh=0.5, greedy='symmetric'): \n",
    "    if greedy=='symmetric':\n",
    "        TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(1-thresh)) & (y_true==1))  \n",
    "    elif greedy=='negative':\n",
    "        TN = np.sum((y_prob<(thresh)) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(thresh)) & (y_true==1))          \n",
    "    NPV = TN/(TN+FN) if (TN+FN)>0 else np.nan\n",
    "    return NPV\n",
    "\n",
    "def recall(y_true, y_prob, thresh=0.5, greedy='symmetric'):\n",
    "    if greedy=='symmetric':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        FN = np.sum((y_prob<(1-thresh)) & (y_true==1))\n",
    "    elif greedy=='negative':\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        FN = np.sum((y_prob<(thresh)) & (y_true==1))        \n",
    "    AP = np.sum(y_true)\n",
    "    recall = TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
    "    return recall, TP/AP  \n",
    "\n",
    "def prec(y_true, y_prob, thresh=0.5, greedy='symmetric'):\n",
    "    TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "    FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "    prec  = TP/(TP+FP) if (TP+FP)>0 else np.nan\n",
    "    return prec\n",
    "    \n",
    "def spec(y_true, y_prob, thresh=0.5, greedy='symmetric'):\n",
    "    TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "    FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "    spec = TN/(TN+FP) if (TN+FP)>0 else np.nan \n",
    "    return spec \n",
    "    \n",
    "        \n",
    "def fpr(y_true, y_prob, thresh=0.5, greedy='symmetric'):\n",
    "    if greedy=='symmetric':\n",
    "        TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "    elif greedy=='negative':\n",
    "        TN = np.sum((y_prob<(thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))        \n",
    "    FPR = FP/(FP+TN) if (FP+TN)>0 else np.nan\n",
    "    return FPR\n",
    "\n",
    "def roc(y_true, y_prob, thresh):\n",
    "    roc_arr = []\n",
    "    for _thresh in thresh:\n",
    "        TP = np.sum((y_prob>thresh) & (y_true==1))\n",
    "        TN = np.sum((y_prob<(1-thresh)) & (y_true==0))\n",
    "        FP = np.sum((y_prob>thresh) & (y_true==0))\n",
    "        FN = np.sum((y_prob<(1-thresh)) & (y_true==1)) \n",
    "\n",
    "        TPR = TP/(TP+FN) if (TP+FN)>0 else np.nan\n",
    "        FPR = FP/(FP+TN) if (FP+TN)>0 else np.nan      \n",
    "        roc_arr.append((TPR, FPR))\n",
    "    return roc_arr\n",
    "def get_accuracy_plots(y_test, y_pred, figax = None, make_plot=True):\n",
    "    #\n",
    "    threshold = np.arange(0.,1,0.025)\n",
    "    _metrics = []\n",
    "    for _thresh in threshold:    \n",
    "        bal_acc = balanced_accuracy(y_test, y_pred, thresh=_thresh)\n",
    "        f1_score = fb_score(y_test, y_pred, beta=1, thresh=_thresh)\n",
    "        _npv = npv(y_test, y_pred, thresh=_thresh)\n",
    "        _fpr = fpr(y_test, y_pred, thresh=_thresh)\n",
    "        rec, true_rec = recall(y_test, y_pred, thresh=_thresh)\n",
    "        _metrics.append({'BAL_ACC': bal_acc, \n",
    "                        'F1_SCORE': f1_score, \n",
    "                        'NPV': _npv, \n",
    "                        'REC': rec, \n",
    "                        'TRUE_REC': true_rec, \n",
    "                        'FPR': _fpr, \n",
    "                        'AUC': metrics.roc_auc_score(y_test, y_pred),\n",
    "                        'THRESHOLD': _thresh})\n",
    "    _metrics = pd.DataFrame(_metrics)\n",
    "    \n",
    "    if make_plot:\n",
    "        if figax is None:\n",
    "            fig, ax = plt.subplots(ncols=4, figsize=(28, 8))\n",
    "        else:\n",
    "            fig, ax = figax\n",
    "        pd.DataFrame(y_pred).hist(bins=20, ax=ax[1], histtype='step')\n",
    "        pd.DataFrame(y_test).hist(bins=2, ax=ax[1], color='black', histtype='step')\n",
    "        ax[1].set_title('Proba histo')\n",
    "        \n",
    "        #sns.lineplot(data=_metrics, x='THRESHOLD', y='F1_SCORE', label='F1', ax=ax[1])\n",
    "        sns.lineplot(data=_metrics, x='NPV', y='REC', color='green', ax=ax[0], ci=None)\n",
    "        sns.lineplot(data=_metrics, x='NPV', y='TRUE_REC', color='red', ax=ax[0], ci=None)\n",
    "        #sns.lineplot(data=_metrics, x='THRESHOLD', y='TRUE_REC', label='TRUE_REC', ax=ax[1])\n",
    "        #sns.lineplot(data=_metrics, x='THRESHOLD', y='BAL_ACC', label='BAL_ACC', ax=ax[1])\n",
    "        ax[0].set_title('NPV versus recall')   \n",
    "        ax[0].set_ylabel(\"recall\")\n",
    "\n",
    "        # TPR / FPR -> sensitivity / 1-specifity\n",
    "        roc_curve = pd.DataFrame(metrics.roc_curve(y_test, y_pred)[:2]).transpose()\n",
    "        roc_curve.columns = ['FPR', 'TPR']\n",
    "        sns.lineplot(data=roc_curve, x='FPR', y='TPR', ax=ax[2], ci=None)\n",
    "        #sns.scatterplot(data=roc_curve, x='FPR', y='TPR', ax=ax[2],s=100)\n",
    "        #ax[2].plot(np.arange(0,1,0.05),np.arange(0,1,0.05), color='black', )\n",
    "        ax[2].plot(np.array([0,1]), np.array([0,1]), ls=\"--\", c=\"black\")\n",
    "        ax[2].set_title(\"ROC\")\n",
    "\n",
    "        prec_recall =  pd.DataFrame(metrics.precision_recall_curve(y_test, y_pred)[:2]).transpose()\n",
    "        prec_recall.columns=['precision', 'recall']\n",
    "        sns.lineplot(data=prec_recall, x='precision', y='recall', ax=ax[3], ci=None)\n",
    "        ax[3].set_title(\"precision - recall\")\n",
    "    else:\n",
    "        fig, ax = None, None\n",
    "    \n",
    "    return _metrics, (fig, ax)\n",
    "\n",
    "## add simple CNN \n",
    "import keras.backend as K\n",
    "\n",
    "# keras\n",
    "#def dice_coef(y_true, y_pred, smooth, thresh):\n",
    "#    y_pred = K.cast((y_pred > thresh), dtype='float32')\n",
    "#    y_true_f = K.flatten(y_true)\n",
    "#    y_pred_f = K.flatten(y_pred)\n",
    "#    intersection = K.sum(y_true_f * y_pred_f)\n",
    "#    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)    \n",
    "    \n",
    "    \n",
    "# keras\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred, smooth=1e-5) #  thresh=0.5\n",
    "\n",
    "#dice_loss_model = dice_loss()\n",
    "\n",
    "\n",
    "def simple_1dcnn(dims=None, conv_layers=[(32,3,3,1), (32,3,3,1), (32,3,3,1), (32,3,3,1), (32,3,3,1)], init_dropout=0.55, final_dropout=0.55):\n",
    "    num_feats = dims[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(init_dropout, input_shape=(num_feats, 1)))\n",
    "    #model.add(Conv1D(conv_layers[0][0], conv_layers[0][1], input_shape=(num_feats, 1)))\n",
    "    for _l in conv_layers:\n",
    "        model.add(Conv1D(filters=_l[0], kernel_size=_l[1], strides=_l[3]))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=_l[2]))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(final_dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', dice_loss])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def simple_dnn(dims=None, conv_layers=[(128,0.05), (64,0.05), (48,0.05), (32,0.1)], init_dropout=0.25, final_dropout=0.5):\n",
    "    num_feats = dims[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(init_dropout, input_shape=(num_feats, )))\n",
    "    #model.add(Conv1D(conv_layers[0][0], conv_layers[0][1], input_shape=(num_feats, 1)))\n",
    "    for _l in conv_layers:\n",
    "        model.add(Dense(_l[0], activation='relu'))\n",
    "        model.add(Dropout(_l[1]))\n",
    "        \n",
    "    model.add(Dropout(final_dropout))    \n",
    "    model.add(Dense(1,  activation='sigmoid'))   \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', dice_loss])     # 'binary_crossentropy'     \n",
    "    return model\n",
    "\n",
    "def _transform(x, trans='minmax', n_quantiles=250, distr='uniform', reducer='PCA', n_comp=128):\n",
    "    if trans=='minmax':\n",
    "        mm = MinMaxScaler()\n",
    "        x = mm.fit_transform(x)\n",
    "    elif trans=='quantile':\n",
    "        mm = QuantileTransformer(n_quantiles=n_quantiles, output_distribution=distr)\n",
    "        x = mm.fit_transform(x)  \n",
    "    else:\n",
    "        mm = None\n",
    "    if reducer=='PCA':\n",
    "        reducer = PCA(n_components=n_comp)\n",
    "        x = reducer.fit_transform(x)\n",
    "    return x, [mm , reducer]\n",
    "    \n",
    "def fit_nn(X, y, network='dnn', verbose=0, epochs=20, batch_size=100, class_weight=None, convs_cnn=[(32,3,3), (32,3,3), (24,3,3), (32,3,3), (32,3,3)], \n",
    "          convs_dnn=[(128,0.05), (64,0.05), (32, 0.1)]):\n",
    "    if network == 'cnn':\n",
    "        X = np.expand_dims(X, 2)\n",
    "        clf = simple_1dcnn(dims=X.shape, conv_layers=convs_cnn)\n",
    "    else:\n",
    "        clf = simple_dnn(dims=X.shape, conv_layers=convs_dnn)\n",
    "    clf.fit(X, y, batch_size = batch_size, epochs = epochs, verbose = verbose, class_weight = class_weight)\n",
    "    return clf\n",
    "\n",
    "scoring = {'auc': 'roc_auc', \n",
    "           'balanced_accuracy': 'balanced_accuracy',\n",
    "           'f1_macro': 'f1_macro', \n",
    "           'f1_micro': 'f1_micro',\n",
    "           'prec_macro': 'precision_macro', \n",
    "           'prec_micro': 'precision_micro',\n",
    "           'mcc': make_scorer(mcc, thresh=0.5, greedy='symmetric'),\n",
    "           'spec': make_scorer(spec, thresh=0.5, greedy='symmetric'),\n",
    "           'prec': make_scorer(prec, thresh=0.5, greedy='symmetric'),\n",
    "           'npv': make_scorer(metrics.precision_score, pos_label=0),\n",
    "           'rec_macro': make_scorer(recall_score, average='macro'),\n",
    "           'rec_micro': make_scorer(recall_score, average='micro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"T:\\laupodteam\\AIOS\\Bram\")\n",
    "HS = pd.read_csv(\"data/HeartScore/Data/MATRIX_FULL_23jul2019_ECG.csv\", sep=\";\")\n",
    "FCD = pd.read_csv(\"data/HeartScore/Data/full_celldyn.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCD_ignore = ['case', 'ID', 'Obs', 'AGE', 'HS_AGE', 'HN_TN', 'HS_History', 'HS_ECG', \n",
    "              'RF_Diab', 'RF_Smok', 'RF_HyperTens', 'RF_HyperChol', 'RF_CVDHist', 'RF_FamHist',\n",
    "             'RF_obese30', 'BMI', 'hos_id', 'HIX_num', 'Seq2', 'Seq', 'RackNum', 'OpID', 'SpecID',\n",
    "              'tn_admission', 'tn_slope2', 'time', 'HS_RiskFacts', 'HS_new2',\n",
    "              'c_mode_cbc', 'c_mode_rtc', 'c_mode_cd61', 'CD348Test', 'c_mode_xLyse', 'c_mode_xCnt', 'CSFTest',\n",
    "              'WBCFlowTest', 'RBCFlowTest', 'PLTFlowTest', 'c_mode', 'c_fixedCells', 'Clin', 'c_outBnds', 'c_OutR_cbc',\n",
    "              'c_Cnt_wbc', 'c_cnt_plto', 'c_cnt_rbc', 'c_cnt_plti', 'c_cnt_plto', 'c_OutR_retc', 'c_DilLim_hb', 'c_DelLim_plt',\n",
    "              'c_DelLim_rbc', 'rbcDilLim', 'c_DilLim_reti', 'c_DilLim_wbc', 'SmpA', 'SmpB', 'SmpC', 'DltaChkStat', 'MovAvgStatWBC','delay_Celldyn', \n",
    "              'setsrc', 'Door', 'casecontrol', 'casestatus2018', 'hos_start_dt', 'SEH_Arrival_dt', 'hos_stop_dt', 'casestatus2016', 'gender', 'bestanden',\n",
    "              'fcs_id','bevat','Asp_dt', 'key', 'Analyzer', 'SWVer','BarCode', 'afname_dt', 'ontvangst_dt', 'min_hos_dt', 'Tn_first_dt', 'TubePos', 'HS_new']\n",
    "FCD.drop(FCD_ignore, axis=1, inplace=True)\n",
    "# voeg kolom toe met dt tussen Tn_first_dt en min_hos_dt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = ['pathos_key', 'upod_id'] \n",
    "date_cols = ['AcquisitionDateTime_ECG'] \n",
    "meta_cols = ['setsrc', 'Analyzer']\n",
    "pheno_cols = ['AGE', 'gender', 'BMI']\n",
    "history_cols = ['RF_Diab', 'RF_Smok', 'RF_HyperTens', 'RF_HyperChol', 'RF_CVDHist', 'RF_FamHist', 'RF_obese30']\n",
    "ign_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_RiskFacts', 'HS_new2', 'HN_TN', 'tn_slope2']\n",
    "tn_cols = ['tn_admission'] # moreve tn_slope2 and HN_TN\n",
    "tn_slope_cols = ['tn_slope2', 'tn_peak', 'tn_peak24', 'Tn_SlopeMax', 'delay_Celldyn']\n",
    "\n",
    "rem_cols = ['Door']+['delay_Celldyn', 'HS_new']+date_cols+meta_cols\n",
    "HS.drop(rem_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS.set_index(index_cols, inplace=True)\n",
    "FCD.set_index(index_cols, inplace=True)\n",
    "\n",
    "overlapping_cd_cols = list(set(HS.columns).intersection(set(FCD.columns)-set(index_cols)))\n",
    "FCD.drop(overlapping_cd_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS = HS.join(FCD, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bool in int64\n",
    "for _col in HS.columns.tolist():\n",
    "    if str(HS[_col].dtype)=='bool':\n",
    "        HS[_col] = HS[_col].astype(int)\n",
    "        \n",
    "target = 'casecontrol'\n",
    "HS.rename(index=str, columns={target: 'target'}, inplace=True)\n",
    "tmap = {'Control': 0, 'NSTEMI': 1}\n",
    "HS['target'] = HS.target.map(tmap)\n",
    "#HS.drop(target, axis=1, inplace=True)\n",
    "\n",
    "gmap = {'M': 0, 'F': 1}\n",
    "HS['gender'] = HS.gender.map(gmap)\n",
    "\n",
    "\n",
    "\n",
    "cols = HS.columns.tolist()\n",
    "var_cols = list(set(cols) - set(meta_cols) - set(index_cols) -set(date_cols) - set(['target']) - set(ign_cols))\n",
    "\n",
    "cell_dyn_cols =  ['PLT', 'Rtcfcv', 'Rtcfmn', 'c_b_HDW', 'c_b_Lacv', 'c_b_Lamn', 'c_b_Licv', 'c_b_Limn', 'c_b_MCHCr', 'c_b_MCHr', 'c_b_MCVr', \n",
    "                  'c_b_Picv', 'c_b_Pimn', 'c_b_Ppcv', 'c_b_Ppmn', 'c_b_bas', 'c_b_blst', 'c_b_bnd', 'c_b_eos', 'c_b_hb', 'c_b_hgb_usa', 'c_b_ht',\n",
    "                  'c_b_ig', 'c_b_irf', 'c_b_lym', 'c_b_lyme', 'c_b_mch', 'c_b_mch_Usa', 'c_b_mchc', 'c_b_mchc_usa', 'c_b_mcv', 'c_b_mon', 'c_b_mone', \n",
    "                  'c_b_mpv', 'c_b_nacv', 'c_b_namn', 'c_b_ndcv', 'c_b_ndmn', 'c_b_neu', 'c_b_nfcv', 'c_b_nfmn', 'c_b_nicv', 'c_b_nimn', 'c_b_npcv', \n",
    "                  'c_b_npmn', 'c_b_nrbc', 'c_b_pHPO', 'c_b_pHPR', 'c_b_pMAC', 'c_b_pMIC', 'c_b_pbas', 'c_b_pblst', 'c_b_pbnd', 'c_b_pct', 'c_b_pdw', \n",
    "                  'c_b_peos', 'c_b_pig', 'c_b_plti', 'c_b_plto', 'c_b_plym', 'c_b_plyme', 'c_b_pmon', 'c_b_pmone', 'c_b_pneu', 'c_b_pnrbc', 'c_b_prP',\n",
    "                  'c_b_pretc', 'c_b_pseg', 'c_b_pvlym', 'c_b_rbcfcv', 'c_b_rbcfmn', 'c_b_rbci', 'c_b_rbcicv', 'c_b_rbcimn', 'c_b_rbco', 'c_b_rdw',\n",
    "                  'c_b_retc', 'c_b_seg', 'c_b_vlym', 'c_b_wbc', 'c_b_wvf', 'c_cnt_retc'] # delay_Celldyn: remove\n",
    "ecg_cols_agg =  [\"VentricularRate_ECG\",\"AtrialRate_ECG\",\"P_RInterval_ECG\",\"QRS_Duration_ECG\",\"Q_TInterval_ECG\",\n",
    "                 \"QTCCalculation_ECG\",\"PAxis_ECG\",\"RAxis_ECG\",\"TAxis_ECG\",\"QRSCount_ECG\",\"QOnset_ECG\",\n",
    "                 \"QOffset_ECG\",\"POnset_ECG\",\"POffset_ECG\",\"T_Onset_ECG\",\"T_Offset_ECG\",\"QRS_Onset_ECG\",\n",
    "                 \"QRS_Offset_ECG\",\"AvgRRInterval_ECG\",\"QTcFredericia_ECG\",\"QTcFramingham_ECG\",\"QTc_Bazett_ECG\"]\n",
    "\n",
    "ecg_leads = ['Lead_I_', 'Lead_II_', 'Lead_III_', 'Lead_V1_', 'Lead_V2_', 'Lead_V3_', 'Lead_V4_', 'Lead_V5_', 'Lead_V6_', 'Lead_aVF_', 'Lead_aVL_', 'Lead_aVR_']\n",
    "ecg_msrmnt = ['MaxST_ECG',  'Max_R_Ampl_ECG', 'Max_S_Ampl_ECG', 'MinST_ECG', 'PFull_Area_ECG', 'PP_Area_ECG', 'PP_Duration_ECG',\n",
    " 'PP_PeakAmpl_ECG', 'PP_PeakTime_ECG', 'P_Area_ECG', 'P_Duration_ECG', 'P_PeakAmpl_ECG', 'P_PeakTime_ECG', 'QRS_Area_ECG', 'QRS_Balance_ECG',\n",
    " 'QRS_Deflection_ECG', 'QRSint_ECG', 'Q_Area_ECG', 'Q_Duration_ECG', 'Q_PeakAmpl_ECG', 'Q_PeakTime_ECG', 'RP_Area_ECG', 'RP_Duration_ECG', 'RP_PeakAmpl_ECG',\n",
    " 'RP_PeakTime_ECG', 'R_Area_ECG', 'R_Duration_ECG', 'R_PeakAmpl_ECG', 'R_PeakTime_ECG', 'SP_Area_ECG', 'SP_Duration_ECG', 'SP_PeakAmpl_ECG', \n",
    " 'SP_PeakTime_ECG', 'STE_ECG', 'STJ_ECG', 'STM_ECG', 'S_Area_ECG', 'S_Duration_ECG', 'S_PeakAmpl_ECG', 'S_PeakTime_ECG',\n",
    " 'TFull_Area_ECG', 'TP_Area_ECG', 'TP_Duration_ECG', 'TP_PeakAmpl_ECG', 'TP_PeakTime_ECG', 'T_Area_ECG', 'T_Duration_ECG', 'T_End_ECG',\n",
    " 'T_PeakAmpl_ECG', 'T_PeakTime_ECG', 'T_Special_ECG', 'P_OnsetAmpl_ECG']\n",
    "\n",
    "\n",
    "ecg_cols_dyn = [_lead+_msrmnt for _lead in ecg_leads for _msrmnt in ecg_msrmnt]\n",
    "\n",
    "ecg_cols_agg = list(set(ecg_cols_agg) & set(var_cols))\n",
    "ecg_cols_dyn = list(set(ecg_cols_dyn) & set(var_cols))\n",
    "cell_dyn_cols = list(set(cell_dyn_cols) & set(var_cols))\n",
    "\n",
    "ecg_cols = list(set(ecg_cols_agg+ecg_cols_dyn))\n",
    "other_cols = list(set(var_cols)-set(ecg_cols)-set(cell_dyn_cols))\n",
    "\n",
    "col_dict = {'ecg': ecg_cols, \n",
    "            'celldyn': cell_dyn_cols,\n",
    "            'ecg_cols_agg': ecg_cols_agg,\n",
    "            'ecg_cols_dyn': ecg_cols_dyn,\n",
    "            'history': history_cols,\n",
    "            'pheno' : pheno_cols,\n",
    "            'troponine': tn_cols,\n",
    "            'hs' : ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_RiskFacts', 'HS_new2', 'HN_TN', 'HS_p1', 'HS_p2'], \n",
    "            'slope': tn_slope_cols,\n",
    "            'other': other_cols}\n",
    "group_dict = {_v:k for k,v in col_dict.items() for _v in v}\n",
    "\n",
    "print(\"ECG: {},{} cols, \\t CELLDYN: {} cols, \\t OTHER: {} cols\".format(len(ecg_cols_dyn), len(ecg_cols_agg), len(cell_dyn_cols), len(other_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial cleaning\n",
    "\n",
    "Basically check for obviously outliers and more them, or cap the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS.mean().plot.hist(bins=100, figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparantly there are a few outlying flies in our ointment.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 trimming the edges\n",
    "HS = HS.apply(featurewise_outlier_replacer, q=(0.01, 0.99))\n",
    "# step 2 looking for remaining univariate outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn-contrib/categorical-encoding\n",
    "dummy_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_new2', 'HS_RiskFacts', 'HN_TN']\n",
    "HS[dummy_cols] = HS[dummy_cols].astype('category')\n",
    "HS = pd.get_dummies(HS, prefix_sep={_dummy: \"_dummy_\" for _dummy in dummy_cols})\n",
    "dummy_cols = [_col for _col in HS.columns if '_dummy_' in _col]\n",
    "\n",
    "hs_cols = ['tn_admission']+dummy_cols\n",
    "\n",
    "HS.loc[HS['tn_admission']>4000, 'tn_admission'] = 4000\n",
    "HS.loc[HS['tn_slope2']<-100, 'tn_slope2'] = 100\n",
    "\n",
    "HS['tn_diff_abs'] = HS[['tn_admission', 'tn_slope2']].apply(lambda x: np.sign(x[0]*x[1])*np.log10(np.abs(x[0]*x[1])+0.01), axis=1)\n",
    "HS['tn_diff_rel'] = HS[['tn_admission', 'tn_slope2']].apply(lambda x: x[1]/(x[0]+1), axis=1)\n",
    "\n",
    "tn_slope_cols= ['tn_slope2', 'tn_diff_abs', 'tn_diff_rel', 'tn_peak', 'tn_peak24']\n",
    "\n",
    "col_dict['slope'] = tn_slope_cols\n",
    "col_dict['hs'] = hs_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not impute, rather treat missingness as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num NaN columns : ECG {}\\tCELLDYN {}\\tOTHER {}\".format((HS[ecg_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                       (HS[cell_dyn_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                       (HS[other_cols].isna().sum(axis=0)>0).sum()))\n",
    "###\n",
    "print(\"Num NaN samples : ECG {}\\tCELLDYN {}\\tOTHER {}\\tALL {}\".format((HS[ecg_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[cell_dyn_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[other_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS.isna().sum(axis=1)>0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_nan_patients:\n",
    "    nan_patients = HS[var_cols].isna().sum(axis=1)[HS[var_cols].isna().sum(axis=1)>0].index\n",
    "    HS.drop(index=nan_patients, inplace=True)\n",
    "    print(\"Num NaN samples : ECG {}\\tCELLDYN {}\\tOTHER {}\".format((HS[ecg_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[cell_dyn_cols].isna().sum(axis=1)>0).sum(),\n",
    "                                                       (HS[other_cols].isna().sum(axis=1)>0).sum()))\n",
    "    \n",
    "    print(\"Num NaN columns : ECG {}\\tCELLDYN {}\\tOTHER {}\".format((HS[ecg_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                           (HS[cell_dyn_cols].isna().sum(axis=0)>0).sum(),\n",
    "                                                           (HS[other_cols].isna().sum(axis=0)>0).sum()))   \n",
    "\n",
    "    # y = HS[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputance\n",
    "\n",
    "# consider adding Gaussian noise on top of the prediction\n",
    "# https://www.kaggle.com/shashankasubrahmanya/missing-data-imputation-using-regression\n",
    "var_cols = get_var_cols(col_dict)\n",
    "if missing_dummy:\n",
    "    # for NaN cols, treat non-nan as 0, nan as 1\n",
    "    _c = HS[var_cols].columns\n",
    "    nan_cols = _c[(HS[var_cols].isna().sum()>0)==True].tolist()  \n",
    "    print(\"---- There are {} columns to be imputed or removed, correct? \\n \\\n",
    "          ---- the columns are: {}\".format(len(nan_cols), nan_cols))\n",
    "    dummy_list = []\n",
    "    for _ncol in nan_cols:\n",
    "        dummy_name = 'availdummy_'+_ncol\n",
    "        dummy_list.append(dummy_name)\n",
    "        HS[dummy_name] = (~pd.isna(HS[_ncol])).astype(int)\n",
    "        var_cols.append(dummy_name)\n",
    "        dummy_comb = \"_\".join(dummy_list)\n",
    "        HS[dummy_comb] = HS[dummy_list].apply(lambda x: np.product(x), axis=1)\n",
    "        col_dict[group_dict[_ncol]] = col_dict[group_dict[_ncol]] + [dummy_name, dummy_comb] # should be included in the relevant feature group (ecg, celldyn, other)\n",
    "    if remove_nan_cols:\n",
    "        # remove NaN cols\n",
    "        nan_cols = list(set(nan_cols)-set(['BMI']))\n",
    "        HS.drop(nan_cols, axis=1, inplace=True)\n",
    "        var_cols = list(set(var_cols) - set(nan_cols))\n",
    "        col_dict = _rem_cols(col_dict, nan_cols)\n",
    "\n",
    "if HS[var_cols].isna().sum().sum()>0:\n",
    "    nan_cols = list(HS[var_cols].isna().sum()[HS[var_cols].isna().sum()>0].index)\n",
    "    dat = HS.copy()\n",
    "    if imputance is not None:\n",
    "        if isinstance(imputance, dict):\n",
    "            for _imp_key, _imp_val in imputance.items():\n",
    "                if type(_imp_val)==str:\n",
    "                    if _imp_val == 'median': \n",
    "                        dat.loc[dat[_imp_key].isna(), _imp_key] = np.nanmedian(dat[_imp_key])\n",
    "                    elif _imp_val == 'mean':\n",
    "                        dat.loc[dat[_imp_key].isna(), _imp_key] = np.nanmean(dat[_imp_key])\n",
    "                    elif _imp_val == 'remove':\n",
    "                        dat = dat.dropna(subset=[_imp_key])\n",
    "                elif 'sklearn' in str(type(_imp_val)):  \n",
    "                    _sub_cols = list(set(var_cols)  - set(nan_cols))\n",
    "                    _y = dat.loc[~dat[_imp_key].isna(), _imp_key]\n",
    "                    _X_train = dat.loc[~dat[_imp_key].isna(), _sub_cols]\n",
    "                    _X_test = dat.loc[dat[_imp_key].isna(), _sub_cols]\n",
    "                    try:\n",
    "                        dat.loc[dat[_imp_key].isna(), _imp_key] = _imp_val.fit(_X_train, _y).predict(_X_test)\n",
    "                    except Exception as e:\n",
    "                        print(\"Imputance failed for {}, shapes: {}, {}, {}\".format(_imp_key, _X_train.shape, _y.shape, _X_test.shape))\n",
    "                        if _X_test.shape[0]==0:\n",
    "                            print(\"Hmm, you probably already ran the imputer, please reload the data...\")\n",
    "        else:\n",
    "            if imputance=='iterative':\n",
    "                imp = IterativeImputer(estimator=BayesianRidge(), max_iter=10)\n",
    "            elif imputance=='knnimputer':\n",
    "                imp= KNNImputer(n_neighbors=5)\n",
    "\n",
    "            dat = pd.DataFrame(data=imp.fit_transform(dat[var_cols]), index=HS.index, columns=var_cols)\n",
    "            dat = dat.join(dat[meta_cols])\n",
    "else:\n",
    "    dat = HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(col_dict['ecg']), len(col_dict['celldyn']), len(col_dict['other']))\n",
    "\n",
    "print(\"PRE: Condition numbers:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t Other {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                        _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                        _condition_number(dat[col_dict['other']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-decomposition\n",
    "\n",
    "**PLS, CCA, NMF, Dictionary learning, Factor Analysis** can be used to select informative combinations between feature sets, say between celldyn and ECG. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling, a priori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaling_apriori:\n",
    "    if scaler is not None:\n",
    "        if isinstance(scaler, dict):        \n",
    "            for _imp_key, _scaler in scaler.items():\n",
    "                if _scaler is not None:\n",
    "                    dat[col_dict[_imp_key]] = _scaler.fit_transform(dat[col_dict[_imp_key]]) \n",
    "        else:   \n",
    "            _cols = col_dict['ecg']+col_dict['celldyn']\n",
    "            dat[_cols] = pd.DataFrame(data=scaler.fit_transform(dat[_cols]), index=HS.index, columns=_cols)\n",
    "    dat.apply(lambda x: sum(np.isinf(x))).sum(), dat.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature combiner\n",
    "\n",
    "## Inter\n",
    "\n",
    "By default \n",
    "* CELLDYN  - pheno: $\\sqrt{A\\cdot B}$ -> +200 features\n",
    "* ECG AGG - pheno: $\\sqrt{A\\cdot B}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_combo = dict()\n",
    "feat_combo[('pheno', 'celldyn')] = ['SQRT']\n",
    "feat_combo[('history', 'celldyn')] = ['SQRT']\n",
    "feat_combo[('pheno', 'ecg_cols_agg')] = ['SQRT']\n",
    "feat_combo[('history', 'ecg_cols_agg')] = ['SQRT']\n",
    "inter_groups = []\n",
    "\n",
    "for _tuple in feature_expansion_inter:\n",
    "    inter_name = \"_\".join(_tuple)\n",
    "    print(\"Combining {} and {}\".format(_tuple[0], _tuple[1]))\n",
    "    \n",
    "    tcols_sqrt, tcols_div, tcols_sum = [], [], []\n",
    "    \n",
    "    _cols_l, _cols_r = col_dict[_tuple[0]], col_dict[_tuple[1]]\n",
    "    \n",
    "    if('SQRT' in feat_combo[_tuple]):        \n",
    "        tcols_sqrt = _cols_(dat[_cols_l].columns.tolist(), cols_r = dat[_cols_r].columns.tolist(), prefix='inter_SQRT') \n",
    "        t = expander2(dat[_cols_l].values, y=dat[_cols_r].values, fun=_ssqrt)\n",
    "        dat_sqrt = pd.DataFrame(data=t, index=dat.index, columns=tcols_sqrt)\n",
    "        dat = dat.join(dat_sqrt)\n",
    "        \n",
    "    if('DIV' in feat_combo[_tuple]):\n",
    "        tcols_div = _cols_(dat[_cols_l].columns.tolist(), cols_r = dat[_cols_r].columns.tolist(), prefix='inter_DIV') \n",
    "        t = expander2(dat[_cols_l].values, y=dat[_cols_r].values,fun=_divisor)\n",
    "        dat_div = pd.DataFrame(data=t, index=dat.index, columns=tcols_div)  \n",
    "        dat = dat.join(dat_div)\n",
    "    \n",
    "    if('SUM' in feat_combo[_tuple]):\n",
    "        tcols_sum = _cols_(dat[_cols_l].columns.tolist(), cols_r = dat[_cols_r].columns.tolist(), prefix='inter_SUM') \n",
    "        t = expander2(dat[_cols_l].values, y=dat[_cols_r].values, fun=_sum)\n",
    "        dat_sum = pd.DataFrame(data=t, index=dat.index, columns=tcols_sum)    \n",
    "        dat = dat.join(dat_sum)    \n",
    "\n",
    "    col_dict[inter_name] = list(set(tcols_sqrt + tcols_div + tcols_sum))    \n",
    "    inter_groups.append(inter_name)\n",
    "\n",
    "for inter in inter_groups:\n",
    "    print(\"Added {} features for {}\".format(len(col_dict[inter]), inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra\n",
    "By default \n",
    "* CELLDYN: $\\frac{A}{A+B+\\epsilon}$, $\\sqrt{A\\cdot B}$ -> +800 features\n",
    "* ECG AGG: $A+B$, $\\sqrt{A\\cdot B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = ['SQRT']\n",
    "# inter featureset : history * pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_combo = dict()\n",
    "feat_combo['celldyn'] = ['SQRT', 'DIV']\n",
    "feat_combo['ecg_cols_agg'] = ['SQRT', 'SUM']\n",
    "\n",
    "for _key in feature_expansion_intra:\n",
    "    print(_key)\n",
    "    _cols = col_dict[_key]\n",
    "    #dat[_cols] = dat[_cols].astype(np.float64)    \n",
    "    tcols_sqrt, tcols_div, tcols_sum = [], [], []\n",
    "    \n",
    "    if('SQRT' in feat_combo[_key]):        \n",
    "        tcols_sqrt = _cols_(dat[_cols].columns.tolist(), prefix='intra_SQRT')         \n",
    "        t = expander2(dat[_cols].values, fun=_ssqrt)\n",
    "        dat_sqrt = pd.DataFrame(data=t, index=dat.index, columns=tcols_sqrt)\n",
    "        dat = dat.join(dat_sqrt)\n",
    "        \n",
    "    if('DIV' in feat_combo[_key]):\n",
    "        tcols_div = _cols_(dat[_cols].columns.tolist(), prefix='intra_DIV') \n",
    "        t = expander2(dat[_cols].values, fun=_divisor)\n",
    "        dat_div = pd.DataFrame(data=t, index=dat.index, columns=tcols_div)  \n",
    "        dat = dat.join(dat_div)\n",
    "    \n",
    "    if('SUM' in feat_combo[_key]):\n",
    "        tcols_sum = _cols_(dat[_cols].columns.tolist(), prefix='intra_SUM') \n",
    "        t = expander2(dat[_cols].values, fun=_sum)\n",
    "        dat_sum = pd.DataFrame(data=t, index=dat.index, columns=tcols_sum)    \n",
    "        dat = dat.join(dat_sum)    \n",
    "    \n",
    "    if _key in ['ecg_cols_agg', 'ecg_cols_dyn']:\n",
    "        col_dict['ecg'] = list(set(col_dict['ecg']+_cols + tcols_sqrt + tcols_div + tcols_sum))\n",
    "    else:\n",
    "        col_dict[_key] = list(set(_cols + tcols_sqrt + tcols_div + tcols_sum))\n",
    "\n",
    "\n",
    "SQRT_cols= [_col for _col in dat.columns.tolist() if 'SQRT_' in _col]\n",
    "DIV_cols = [_col for _col in dat.columns.tolist() if 'DIV_' in _col]\n",
    "\n",
    "print('SQRT cols:', dat[SQRT_cols].apply(lambda x: sum(np.isinf(x))).sum(), dat[SQRT_cols].isna().sum().sum())\n",
    "print('DIV cols:', dat[DIV_cols].apply(lambda x: sum(np.isinf(x))).sum(), dat[DIV_cols].isna().sum().sum())\n",
    "#\n",
    "print('ECG cols:', dat[col_dict['ecg']].apply(lambda x: sum(np.isinf(x))).sum(), dat[col_dict['ecg']].isna().sum().sum())\n",
    "print('CELLDYN cols:', dat[col_dict['celldyn']].apply(lambda x: sum(np.isinf(x))).sum(), dat[col_dict['celldyn']].isna().sum().sum())\n",
    "#\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "group_dict = {_v:k for k,v in col_dict.items() for _v in v}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ECG: {} cols, \\t CELLDYN: {} cols, \\t OTHER: {} cols\".format(len(col_dict['ecg']), len(col_dict['celldyn']), len(col_dict['other'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set distance between sample groups\n",
    "See the [wiki](https://en.wikipedia.org/wiki/Hausdorff_distance) for the Hausdorff distance as an example set distance.\n",
    "\n",
    "SciPy has a function readily available, see [directed Hausdorff](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.directed_hausdorff.html), \n",
    "and more optimised version like a [Fast Hausdorff distance](https://github.com/mavillan/py-hausdorff).\n",
    "\n",
    "One way to find patient clusters is to maximize the Hausdoff distance between patient groups.\n",
    "\n",
    "Feature selection through Hausdorff distance: **Given the target groups, iteratively add features if the feature increases the Hausdorff distance between the target groups**  \n",
    "\n",
    "We use two types of set distance:\n",
    "* Hausdorff distance\n",
    "* Multidimensional Wasserstein distance\n",
    "\n",
    "We iteratively add features based on their influence on the set-distance, \n",
    "to get an initial set-distance we start with the meta features **age**, **gender**, **BMI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_cols = inter_groups + prune_cols\n",
    "prune_cols = list(set(prune_cols))\n",
    "base_cols = [__column for _key in base_cols for __column in col_dict[_key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this step we use a scaled version of the dataset\n",
    "qt = QuantileTransformer(n_quantiles=250, output_distribution='uniform')\n",
    "dat_tmp = pd.DataFrame(data=qt.fit_transform(dat), columns=dat.columns, index=dat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "print(\"We start with..\")\n",
    "for jdx, _group in enumerate(col_dict.keys()):\n",
    "    print(\" \"*jdx, \"For {} we have {} columns\".format(_group, len(set(col_dict[_group]))))\n",
    "    tot += len(set(col_dict[_group]))\n",
    "print(\"We have a  total of {} columns\".format(tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hausdorff\n",
    "# we compare two groups, namely the groups defined by the targets, NSTEMI/Non-NSTEMI\n",
    "# Simple variant: about 25 it/s\n",
    "# Complete variant: about \n",
    "\n",
    "if hausdorff_feature_builder:\n",
    "    extreme_samples = []\n",
    "    keep_cols = defaultdict(list)\n",
    "    added_cols = []\n",
    "    set_distances = []\n",
    "    old_dist = hausdorff(dat_tmp.loc[dat.target==0, base_cols].values, \n",
    "                         dat_tmp.loc[dat.target==1, base_cols].values)\n",
    "    for _group in prune_cols:\n",
    "        added_cols = [] \n",
    "        for idx, _col in tqdm(enumerate(col_dict[_group])):\n",
    "            if complete_dist_improvement:\n",
    "                _cols = base_cols + [_col] + added_cols\n",
    "            else:\n",
    "                _cols = base_cols + [_col]\n",
    "                \n",
    "            g1 = dat_tmp.loc[dat_tmp.target==0, _cols].values\n",
    "            g2 = dat_tmp.loc[dat_tmp.target==1, _cols].values\n",
    "            new_dist = hausdorff(g1, g2)\n",
    "            set_distances.append({'idx': idx, 'sdist': new_dist[0]})\n",
    "\n",
    "            # exclude these rows from the distance determination.\n",
    "            extreme_samples.append(new_dist[1])\n",
    "            extreme_samples.append(new_dist[2])\n",
    "\n",
    "            if new_dist[0] > old_dist[0]:\n",
    "                keep_cols[_group].append(_col)\n",
    "                added_cols.append(_col)\n",
    "            if complete_dist_improvement:\n",
    "                old_dist = new_dist\n",
    "                \n",
    "    for _group in prune_cols:\n",
    "        col_dict[_group]= keep_cols[_group]\n",
    "    extreme_samples = list(set(extreme_samples))\n",
    "\n",
    "    distdf = pd.DataFrame(set_distances)\n",
    "    distdf['sdist'].plot.hist(bins=30)\n",
    "    plt.axvline(old_dist[0], color='black')\n",
    "    plt.title(\"Hausdorff distances\")\n",
    "\n",
    "    print(\"The Hausdorff distance is dominated by {} samples\".format(len(extreme_samples)))\n",
    "    print(\"{} ECG cols and {} CELLDYN cols add Hausdorff.distance\".format(len(col_dict['ecg']), len(col_dict['celldyn'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein in multiple-dimensions, wasserstein of expanding product\n",
    "# want to maximize wasserstein distance\n",
    "# about 25 it/second\n",
    "# ftilde = productsum(f+1), W(ftilde_1, ftilde_2)_new> _old\n",
    "base_val = 0.5\n",
    "if wasserstein_feature_builder:\n",
    "    extreme_samples = []\n",
    "    keep_cols = defaultdict(list)\n",
    "    added_cols = []\n",
    "    set_distances = []\n",
    "    old_dist = wass_sup_distance(dat_tmp.loc[dat_tmp.target==0, base_cols].values+base_val, dat_tmp.loc[dat_tmp.target==1, base_cols].values+base_val)\n",
    "    for _group in prune_cols:\n",
    "        added_cols = []\n",
    "        for idx, _col in tqdm(enumerate(col_dict[_group])):\n",
    "            if complete_dist_improvement:\n",
    "                _cols = base_cols + [_col] + added_cols\n",
    "            else:\n",
    "                _cols = base_cols + [_col]            \n",
    "            _cols = base_cols + [_col]\n",
    "            g1 = dat_tmp.loc[dat_tmp.target==0, _cols].values+base_val\n",
    "            g2 = dat_tmp.loc[dat_tmp.target==1, _cols].values+base_val\n",
    "            new_dist = wass_sup_distance(g1,g2)\n",
    "            set_distances.append({'idx': idx, 'wdist': new_dist})\n",
    "            \n",
    "            if new_dist > old_dist:\n",
    "                keep_cols[_group].append(_col)\n",
    "                added_cols.append(_col)\n",
    "            if complete_dist_improvement:\n",
    "                old_dist = new_dist    \n",
    "            \n",
    "    for _group in prune_cols:\n",
    "        col_dict[_group] = keep_cols[_group]\n",
    "\n",
    "    distdf = pd.DataFrame(set_distances)\n",
    "    distdf['wdist'].plot.hist(bins=30)\n",
    "    plt.axvline(old_dist, color='black')\n",
    "    plt.title(\"Wasserstein distances\")\n",
    "\n",
    "    print(\"{} ECG cols and {} CELLDYN cols add to the average Wasserstein distance\".format(len(col_dict['ecg']), len(col_dict['celldyn'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise distance: want to maximize average pairwise distance between sets, or as a proxy we can maximize the difference between the average feature vectors\n",
    "# \n",
    "if pairwisedistance_feature_builder:\n",
    "    extreme_samples = []\n",
    "    keep_cols = defaultdict(list)\n",
    "    added_cols = []\n",
    "    set_distances = []\n",
    "    old_dist = pairwise_distance(dat_tmp.loc[dat_tmp.target==0, base_cols].values, \n",
    "                                 dat_tmp.loc[dat_tmp.target==1, base_cols].values)\n",
    "    for _group in prune_cols:\n",
    "        added_cols = []\n",
    "        for idx, _col in tqdm(enumerate(col_dict[_group])):\n",
    "            if complete_dist_improvement:\n",
    "                _cols = base_cols + [_col] + added_cols\n",
    "            else:\n",
    "                _cols = base_cols + [_col]            \n",
    "            _cols = base_cols + [_col]\n",
    "            g1 = dat_tmp.loc[dat_tmp.target==0, _cols].values\n",
    "            g2 = dat_tmp.loc[dat_tmp.target==1, _cols].values\n",
    "            new_dist = pairwise_distance(g1,g2)\n",
    "            set_distances.append({'idx': idx, 'pdist': new_dist})\n",
    "            \n",
    "            if new_dist > old_dist:\n",
    "                keep_cols[_group].append(_col)\n",
    "                added_cols.append(_col)\n",
    "            if complete_dist_improvement:\n",
    "                old_dist = new_dist  \n",
    "            \n",
    "    for _group in prune_cols:\n",
    "        col_dict[_group] = keep_cols[_group]\n",
    "\n",
    "    distdf = pd.DataFrame(set_distances)\n",
    "    distdf['pdist'].plot.hist(bins=30)\n",
    "    plt.axvline(old_dist, color='black')\n",
    "    plt.title(\"Pairwise distances\")\n",
    "\n",
    "    print(\"{} ECG cols and {} CELLDYN cols add to the average pairwise distance\".format(len(col_dict['ecg']), \n",
    "                                                                                        len(col_dict['celldyn'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for jdx, _group in enumerate(col_dict.keys()):\n",
    "    print(\" \"*jdx, \"For {} we have {} columns\".format(_group, len(set(col_dict[_group]))))\n",
    "    tot += len(set(col_dict[_group]))\n",
    "print(\"We still have a  total of {} columns\".format(tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ensemble tree estimator feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = dict()\n",
    "if extratrees_feature_builder:\n",
    "    base_cols = [__column for _key in prune_cols for __column in col_dict[_key]]\n",
    "    ET = ExtraTreesClassifier(n_estimators=1000, n_jobs=8)\n",
    "    ET.fit(X=dat[base_cols], y=dat['target'])\n",
    "    importances = ET.feature_importances_\n",
    "    all_groups = np.array(base_cols)[np.argsort(importances)[-top_n_all:]].tolist()\n",
    "    \n",
    "    for _group in prune_cols:\n",
    "        _cols = col_dict[_group]\n",
    "        ET = ExtraTreesClassifier(n_estimators=1000, n_jobs=8)\n",
    "        ET.fit(X=dat[_cols], y=dat['target'])\n",
    "        importances = ET.feature_importances_\n",
    "        keep_cols[_group] = np.array(_cols)[np.argsort(importances)[-top_n[_group]:]].tolist()\n",
    "        col_dict[_group] = list(set(keep_cols[_group] + [_col for _col in all_groups if group_dict[_col]==_group]))\n",
    "        \n",
    "    print(\"{} ECG cols and {} CELLDYN cols add to the average pairwise distance\".format(len(col_dict['ecg']), \n",
    "                                                                                        len(col_dict['celldyn'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Contribution to first principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp=25\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,7))\n",
    "raw_pcre = PCA(n_components=ncomp)\n",
    "raw_pcre.fit(dat[col_dict['ecg']])\n",
    "\n",
    "raw_pcrd = PCA(n_components=ncomp)\n",
    "raw_pcrd.fit(dat[col_dict['celldyn']])\n",
    "\n",
    "ax[0].plot(raw_pcre.explained_variance_ratio_, label='ECG columns')\n",
    "ax[0].plot(raw_pcrd.explained_variance_ratio_, label='CELLDYN columns')\n",
    "ax[0].set_title(\"Raw data\")\n",
    "###########\n",
    "scaled_pcre = PCA(n_components=ncomp)\n",
    "scaled_pcre.fit(dat_tmp[col_dict['ecg']])\n",
    "\n",
    "scaled_pcrd = PCA(n_components=ncomp)\n",
    "scaled_pcrd.fit(dat_tmp[col_dict['celldyn']])\n",
    "\n",
    "ax[1].plot(scaled_pcre.explained_variance_ratio_, label='ECG columns')\n",
    "ax[1].plot(scaled_pcrd.explained_variance_ratio_, label='CELLDYN columns')\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pcweights = get_pca_weights(reducer=raw_pcrd, cols=col_dict['celldyn'], ncomp=10)\n",
    "raw_pcweightsE = get_pca_weights(reducer=raw_pcre, cols=col_dict['ecg'], ncomp=10)\n",
    "\n",
    "scaled_pcweights = get_pca_weights(reducer=scaled_pcrd, cols=col_dict['celldyn'], ncomp=10)\n",
    "scaled_pcweightsE = get_pca_weights(reducer=scaled_pcre, cols=col_dict['ecg'], ncomp=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(22,7))\n",
    "ncomp = 401\n",
    "x = ncomp-np.arange(0,ncomp)\n",
    "ax[0].plot(x, list(raw_pcweights.values())[-ncomp:], label='celldyn columns')\n",
    "ax[0].plot(x, list(raw_pcweightsE.values())[-ncomp:], label='ECG columns')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Raw data\")\n",
    "\n",
    "ax[1].plot(x, list(scaled_pcweights.values())[-ncomp:], label='celldyn columns')\n",
    "ax[1].plot(x, list(scaled_pcweightsE.values())[-ncomp:], label='ECG columns')\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take top 250 of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power indices\n",
    "\n",
    "If we view the features as \"players\" in a game that compete with other features to score points (i.e. predict the targets) we\n",
    "can apply game theoretical power indices to assess the strenght or power of a particular feature.\n",
    "Power indices are : Penrose-Banzhaf, Shapley-Shubik, Coleman-Shapley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE ARE NOW DONE WITH ADDING COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_ecgs = [_group for _group in inter_groups if 'ecg' in _group]\n",
    "inter_celldyn = [_group for _group in inter_groups if 'celldyn' in _group]\n",
    "\n",
    "inter_ecg_cols = [_col for _g in inter_ecgs for _col in col_dict[_g]]\n",
    "inter_celldyn_cols = [_col for _g in inter_celldyn for _col in col_dict[_g]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if variance_remove:\n",
    "    eps=1e-6\n",
    "    def remove_min_variance(x, threshold=0.001):\n",
    "        eps=1e-6\n",
    "        cols_to_keep = x.columns[(((x.var()+eps**2)/(x.max()-x.min()+eps))>threshold)]   \n",
    "        drop_cols = list(set(x.columns)-set(cols_to_keep))\n",
    "        return x[cols_to_keep], drop_cols\n",
    "    _cols = col_dict['ecg']+col_dict['celldyn']\n",
    "    tmp_keep, drop_cols = remove_min_variance(dat_tmp[_cols], threshold=0.05)\n",
    "    print(\"Removing {} columns due to lack of variance\".format(dat_tmp[_cols].shape[1] - tmp_keep.shape[1]))\n",
    "\n",
    "    col_dict['ecg'] = list(set(col_dict['ecg']).difference(set(drop_cols)))\n",
    "    col_dict['celldyn'] = list(set(col_dict['celldyn']).difference(set(drop_cols)))\n",
    "\n",
    "    ((tmp_keep.std()+eps)/(tmp_keep.max() - tmp_keep.min()+eps)).plot.hist(bins=50)\n",
    "    plt.title('Relative STD')\n",
    "\n",
    "    print(\"POST: Condition numbers:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t Other {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['other']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to deal with the very large variance features as they may contain little information, yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cols = col_dict['ecg']+col_dict['celldyn']\n",
    "large_variance_features = dat_tmp[_cols].columns[dat[_cols].std()>0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove low-variant features is actually to remove low entropy features. As the idea of low variance is completely dependent on the scaling of the feature low entropy will give a \n",
    "more robust feature filter. For continuous variables we have to use *differential entropy*, described as; \n",
    "$$H = - \\int \\rho(x) \\ln \\rho(x) dx $$\n",
    "\n",
    "The best we can do is to approximate this using the histograms of the features;\n",
    "\n",
    "$$H \\approx - \\sum \\rho(b_i) \\ln \\rho(b_i) \\Delta x_i$$\n",
    "\n",
    "where $b_i$ represent the bins. To bound it between zero and one we should write\n",
    "\n",
    "$$H \\approx - \\frac{\\sum \\rho(b_i) \\ln \\rho(b_i) \\Delta x_i}{\\sum \\Delta x_i}$$\n",
    "\n",
    "Note that the correct differential entropy is actually written as \n",
    "$$H = - \\int \\rho(x) \\ln \\frac{\\rho(x)}{m(x)} dx $$\n",
    "\n",
    "where $m(x)$ is the invariant measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cols = col_dict['ecg']+col_dict['celldyn']\n",
    "feat_entropy = pd.DataFrame(data=dat_tmp[_cols].apply(func=_diff_entropy, axis=0).apply(pd.Series))\n",
    "feat_entropy.columns = ['Hr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(22,6))\n",
    "feat_entropy.Hr.plot.hist(bins=40, ax=ax)\n",
    "ax.axvline(feat_entropy.Hr.quantile(0.01), color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_hr = feat_entropy.loc[(feat_entropy.Hr>-2)].index\n",
    "low_hr = feat_entropy.loc[(feat_entropy.Hr<-2)].index\n",
    "print(\"HIGH\", len(high_hr), \"LOW\", len(low_hr))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,7))\n",
    "n = 20 \n",
    "N = len(high_hr)\n",
    "for i in range(0, n):\n",
    "    j = np.random.randint(0, N)\n",
    "    dat_tmp[high_hr[j]].plot.hist(bins=50, histtype='step', ax=ax[0])\n",
    "ax[0].set_title('high differential entropy')\n",
    "N = len(low_hr)    \n",
    "if N>0:\n",
    "    for i in range(0, n):\n",
    "        j = np.random.randint(0, N)\n",
    "        dat_tmp[low_hr[j]].plot.hist(bins=50, histtype='step', ax=ax[1])\n",
    "    ax[1].set_title('low differential entropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove low HR cols\n",
    "if remove_low_diffEntropy:\n",
    "    #dat_tmp.drop(list(low_hr), axis=1, inplace=True)\n",
    "    col_dict = _rem_cols(col_dict, list(low_hr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection\n",
    "\n",
    "We use the union of featuresets obtained by applying \n",
    "* ANOVA \n",
    "* KS\n",
    "* mutual information value\n",
    "\n",
    "with visually determined cut-off values. Of course there is a range of alternatives such as recursive feature elimination but that is beyond the scope of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "\n",
    "y = dat['target']\n",
    "\n",
    "if remove_weak_univariates:\n",
    "    anov = dict()\n",
    "    anov['celldyn'] = pd.DataFrame(data=f_classif(dat_tmp[col_dict['celldyn']], y), columns=col_dict['celldyn'], index=['F', 'pval']).T\n",
    "    anov['ecg'] = pd.DataFrame(data=f_classif(dat_tmp[col_dict['ecg']], y), columns=col_dict['ecg'], index=['F', 'pval']).T\n",
    "    anov['other'] = pd.DataFrame(data=f_classif(dat_tmp[col_dict['other']], y), columns=col_dict['other'], index=['F', 'pval']).T\n",
    "\n",
    "    chisq = dict()\n",
    "    chisq['celldyn'] = pd.DataFrame(data=chi2(dat_tmp[col_dict['celldyn']].abs(), y), columns=col_dict['celldyn'], index=['Chi', 'pval']).T\n",
    "    chisq['ecg'] = pd.DataFrame(data=chi2(dat_tmp[col_dict['ecg']].abs(), y), columns=col_dict['ecg'], index=['Chi', 'pval']).T\n",
    "    chisq['other'] = pd.DataFrame(data=chi2(dat_tmp[col_dict['other']].abs(), y), columns=col_dict['other'], index=['Chi', 'pval']).T\n",
    "\n",
    "    mi = dict()\n",
    "    mi['celldyn'] = pd.DataFrame(data=mutual_info_classif(dat_tmp[col_dict['celldyn']].abs(), y), index=col_dict['celldyn'], columns=['mi'])\n",
    "    mi['ecg'] = pd.DataFrame(data=mutual_info_classif(dat_tmp[col_dict['ecg']].abs(), y), index=col_dict['ecg'], columns=['mi'])\n",
    "    mi['other'] = pd.DataFrame(data=mutual_info_classif(dat_tmp[col_dict['other']].abs(), y), index=col_dict['other'], columns=['mi'])\n",
    "\n",
    "\n",
    "    ks_dist = dict()\n",
    "    KS = fs_ks()\n",
    "    KS.fit(dat[col_dict['other']].values, y)\n",
    "    ks_dist['other'] = pd.DataFrame(KS.results_, columns=['KS', 'pval'], index=col_dict['other'])\n",
    "\n",
    "    KS.fit(dat[col_dict['celldyn']].values, y)\n",
    "    ks_dist['celldyn'] = pd.DataFrame(KS.results_, columns=['KS', 'pval'], index=col_dict['celldyn'])\n",
    "\n",
    "    KS.fit(dat[col_dict['ecg']].values, y)\n",
    "    ks_dist['ecg'] = pd.DataFrame(KS.results_, columns=['KS', 'pval'], index=col_dict['ecg'])\n",
    "\n",
    "    try:\n",
    "        mwu_dist = dict()\n",
    "        MW = fs_mannwhitney()\n",
    "        MW.fit(dat[col_dict['other']].values, y)\n",
    "        mwu_dist['other'] = pd.DataFrame(MW.results_, columns=['MWS', 'pval'], index=col_dict['other'])\n",
    "\n",
    "        MW.fit(dat[col_dict['celldyn']].values, y)\n",
    "        mwu_dist['celldyn'] = pd.DataFrame(MW.results_, columns=['MWS', 'pval'], index=col_dict['celldyn'])\n",
    "\n",
    "        MW.fit(dat[col_dict['ecg']].values, y)\n",
    "        mwu_dist['ecg'] = pd.DataFrame(MW.results_, columns=['MWS', 'pval'], index=col_dict['ecg'])\n",
    "\n",
    "        mwu_dist['celldyn']['MWS'] = 4*(mwu_dist['celldyn']['MWS'])/(dat.shape[0]*(dat.shape[0]+1))\n",
    "        mwu_dist['ecg']['MWS'] = 4*(mwu_dist['ecg']['MWS'])/(dat.shape[0]*(dat.shape[0]+1))\n",
    "        mwu_dist['other']['MWS'] = 4*(mwu_dist['other']['MWS'])/(dat.shape[0]*(dat.shape[0]+1))\n",
    "    except Exception as e:\n",
    "        print(\"Perhaps there is something wrong with your scaling?\")\n",
    "\n",
    "    try:\n",
    "        epps_dist = dict()\n",
    "        EPPS = fs_epps()\n",
    "        EPPS.fit(dat[col_dict['other']].values, y)\n",
    "        epps_dist['other'] = pd.DataFrame(EPPS.results_, columns=['EPPS', 'pval'], index=col_dict['other'])\n",
    "\n",
    "        EPPS.fit(dat[col_dict['ecg']].values, y)\n",
    "        epps_dist['ecg'] = pd.DataFrame(EPPS.results_, columns=['EPPS', 'pval'], index=col_dict['ecg'])\n",
    "\n",
    "        EPPS.fit(dat[col_dict['celldyn']].values, y)\n",
    "        epps_dist['celldyn'] = pd.DataFrame(EPPS.results_, columns=['EPPS', 'pval'], index=col_dict['celldyn'])\n",
    "    except Exception as e:\n",
    "        if str(e)=='SVD did not converge':\n",
    "            print(\"!! \\t Remove the multicollinearity of the input matrix \\t!!\")\n",
    "        else:\n",
    "            print(\"Oooeps: {}\".format(e))\n",
    "\n",
    "    # w1\n",
    "    wass1_dist = dict()\n",
    "    W1D = fs_ws1()\n",
    "    W1D.fit(dat[col_dict['other']].values, y)\n",
    "    wass1_dist['other'] = pd.DataFrame(W1D.scores_, columns=['W1'], index=col_dict['other'])\n",
    "\n",
    "    W1D.fit(dat[col_dict['celldyn']].values, y)\n",
    "    wass1_dist['celldyn'] = pd.DataFrame(W1D.scores_, columns=['W1'], index=col_dict['celldyn'])\n",
    "\n",
    "    W1D.fit(dat[col_dict['ecg']].values, y)\n",
    "    wass1_dist['ecg'] = pd.DataFrame(W1D.scores_, columns=['W1'], index=col_dict['ecg'])\n",
    "\n",
    "    # w2\n",
    "    wass2_dist = dict()\n",
    "    W2D = fs_ws2()\n",
    "    W2D.fit(dat[col_dict['other']].values, y)\n",
    "    wass2_dist['other'] = pd.DataFrame(W2D.scores_, columns=['W2'], index=col_dict['other'])\n",
    "\n",
    "    W2D.fit(dat[col_dict['celldyn']].values, y)\n",
    "    wass2_dist['celldyn'] = pd.DataFrame(W2D.scores_, columns=['W2'], index=col_dict['celldyn'])\n",
    "\n",
    "    W2D.fit(dat[col_dict['ecg']].values, y)\n",
    "    wass2_dist['ecg'] = pd.DataFrame(W2D.scores_, columns=['W2'], index=col_dict['ecg'])\n",
    "\n",
    "    ##############\n",
    "    ### FILTER ###\n",
    "    ##############\n",
    "#if remove_weak_univariates:\n",
    "    fig, ax = plt.subplots(ncols=5, figsize=(20, 8))\n",
    "    anov['ecg'].F.plot.kde(label='ecg', ax=ax[0])\n",
    "    anov['celldyn'].F.plot.kde(label='celldyn', ax=ax[0])\n",
    "    ax[0].set_title(\"ANOVA F-score\")\n",
    "    ax[0].axvline(1, color='black')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ks_dist['ecg'].KS.plot.kde(label='ecg', ax=ax[1])\n",
    "    ks_dist['celldyn'].KS.plot.kde(label='celldyn', ax=ax[1])\n",
    "    ax[1].set_title(\"KS score\")    \n",
    "    ax[1].axvline(0.08, color='black')\n",
    "    ax[1].legend()\n",
    "    \n",
    "    mi['ecg'].mi.plot.kde(label='ecg', ax=ax[2])\n",
    "    mi['celldyn'].mi.plot.kde(label='celldyn', ax=ax[2])\n",
    "    ax[2].set_title(\"MI value\")\n",
    "    ax[2].axvline(0.001, color='black')\n",
    "    ax[2].legend()\n",
    "    \n",
    "    wass1_dist['ecg'].W1.plot.kde(label='ecg', ax=ax[3])\n",
    "    wass1_dist['celldyn'].W1.plot.kde(label='celldyn', ax=ax[3])\n",
    "    ax[3].set_title(\"EMD distance\")\n",
    "    ax[3].axvline(0.03, color='black')\n",
    "    ax[3].legend()\n",
    "    \n",
    "    wass2_dist['ecg'].W2.plot.kde(label='ecg', ax=ax[4])\n",
    "    wass2_dist['celldyn'].W2.plot.kde(label='celldyn', ax=ax[4])\n",
    "    ax[4].set_title(\"Energy distance\")\n",
    "    ax[4].axvline(0.06, color='black')\n",
    "    ax[4].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_weak_univariates:\n",
    "    ecg_inc_ks = ks_dist['ecg'][ks_dist['ecg'].KS>0.08].index.tolist()\n",
    "    celldyn_inc_ks = ks_dist['celldyn'][ks_dist['celldyn'].KS>0.08].index.tolist()\n",
    "    \n",
    "    ecg_inc_anova = anov['ecg'][anov['ecg'].F>1].index.tolist()\n",
    "    celldyn_inc_anova = anov['celldyn'][anov['celldyn'].F>1].index.tolist()\n",
    "\n",
    "    ecg_inc_mi = mi['ecg'][mi['ecg'].mi>0.001].index.tolist()\n",
    "    celldyn_inc_mi = mi['celldyn'][mi['celldyn'].mi>0.001].index.tolist()\n",
    "\n",
    "    \n",
    "    ecg_inc_wass = wass1_dist['ecg'][wass1_dist['ecg'].W1>0.15].index.tolist()\n",
    "    celldyn_inc_wass = wass1_dist['celldyn'][wass1_dist['celldyn'].W1>0.15].index.tolist()\n",
    "\n",
    "    ecg_inc_wass2 = wass2_dist['ecg'][wass2_dist['ecg'].W2>0.025].index.tolist()\n",
    "    celldyn_inc_wass2 = wass2_dist['celldyn'][wass2_dist['celldyn'].W2>0.025].index.tolist()\n",
    "    \n",
    "    tot_scores_ecg = pd.concat([ks_dist['ecg'], anov['ecg'], wass1_dist['ecg'], wass2_dist['ecg'], mi['ecg'], chisq['ecg']], axis=1)\n",
    "    tot_scores_celldyn = pd.concat([ks_dist['celldyn'], anov['celldyn'], wass1_dist['celldyn'], wass2_dist['celldyn'], mi['celldyn'], chisq['celldyn']], axis=1)\n",
    "\n",
    "    tot_scores_ecg.drop('pval', axis=1, inplace=True)\n",
    "    tot_scores_celldyn.drop('pval', axis=1, inplace=True)\n",
    "\n",
    "    mmscaler = MinMaxScaler()\n",
    "    X=mmscaler.fit_transform(tot_scores_ecg)\n",
    "    _cols=tot_scores_ecg.columns.tolist()\n",
    "    ecg_scores = pd.DataFrame(data=X, index=tot_scores_ecg.index, columns=_cols)\n",
    "\n",
    "    X=mmscaler.fit_transform(tot_scores_celldyn)\n",
    "    _cols=tot_scores_celldyn.columns.tolist()\n",
    "    celldyn_scores = pd.DataFrame(data=X, index=tot_scores_celldyn.index, columns=_cols)\n",
    "\n",
    "    ecg_scores['AVG'] =ecg_scores.apply(lambda x: x.mean(), axis=1)\n",
    "    celldyn_scores['AVG'] = celldyn_scores.apply(lambda x: x.mean(), axis=1)    \n",
    "    \n",
    "    print(\"PRE: ECG cols {}, CELLDYN cols {}\".format(len(col_dict['ecg']), len(col_dict['celldyn'])))\n",
    "    \n",
    "    if isinstance(keep_strongest_N, int):\n",
    "        print(\"Keeping strongest {}\".format(keep_strongest_N))\n",
    "        assert keep_strongest_N>50, 'Sorry, we want a minimum of 50 features'\n",
    "        # we have keep_strongest_N features to keep\n",
    "        # we want to prune the ecg, celldyn features\n",
    "        Ntot = keep_strongest_N #- (dat.shape[1]-len(col_dict['ecg'])-len(col_dict['celldyn']))\n",
    "        # we use the number of the current ecg/celldyn cols as the weights\n",
    "        Necg, Ncelldyn = len(col_dict['ecg']), len(col_dict['celldyn'])\n",
    "        w_ecg, w_celldyn = Necg/(Necg+Ncelldyn), Ncelldyn/(Necg+Ncelldyn)\n",
    "        Necg = int(w_ecg*Ntot)\n",
    "        Ncelldyn = int(w_celldyn*Ntot)\n",
    "        # we want to keep only the top features\n",
    "        ecg_cols_to_keep = ecg_scores.sort_values('AVG', ascending=False)[:Necg].index.tolist()\n",
    "        celldyn_cols_to_keep = celldyn_scores.sort_values('AVG', ascending=False)[:Ncelldyn].index.tolist()\n",
    "        \n",
    "        col_dict['ecg'] = list(set(ecg_cols_to_keep))\n",
    "        col_dict['celldyn'] = list(set(celldyn_cols_to_keep))\n",
    "    else:\n",
    "        if univariate_inclusive:\n",
    "            col_dict['ecg'] = list(set(ecg_inc_anova+ecg_inc_mi+ecg_inc_ks+ecg_inc_wass+ecg_inc_wass2))\n",
    "            col_dict['celldyn'] = list(set(celldyn_inc_anova+celldyn_inc_mi+celldyn_inc_ks+celldyn_inc_wass+celldyn_inc_wass2))\n",
    "        else:\n",
    "            col_dict['ecg'] = list(set(ecg_inc_anova).intersection(set(ecg_inc_mi), set(ecg_inc_ks), set(ecg_inc_wass), set(ecg_inc_wass2)))\n",
    "            col_dict['celldyn'] = list(set(celldyn_inc_anova).intersection(set(celldyn_inc_mi), set(celldyn_inc_ks), set(celldyn_inc_wass), set(celldyn_inc_wass2)))            \n",
    "        \n",
    "    print(\"POST: ECG cols {}, CELLDYN cols {}\".format(len(col_dict['ecg']), len(col_dict['celldyn'])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-collinearity\n",
    "\n",
    "I.e. **the problem of removing all correlated pairs as efficiently as possible, meaning by removing a minimum number of nodes at the lowest computational cost.**\n",
    "\n",
    "The efficient removal of multicollinearity can be cast in graph optimisation problem.\n",
    "\n",
    "* Maximal Clique Enumeration (MCE): from all the maximal non-overlapping cliques, keep only the node with the lowest mean similarity. This is iterative; \n",
    "    * start with the largest cliques, exclude, redetermine cliques, exclude, etc. Until only pairs are left\n",
    "    * Remove all nodes with occurrence > 1, then randomly remove nodes   \n",
    "* Recursively remove 50% of the pairs until no pairs are left\n",
    "* Instead of removing the collinear pairs multiply them. \n",
    "\n",
    "Clique finders:\n",
    "* Walktrap community\n",
    "* Infomap\n",
    "* Sparse Affinity propagation\n",
    "* igraph.leading.eigenvector.community\n",
    "* igraph.edge.betweenness.community\n",
    "* label propagation: networkx.algorithms.community.label_propagation_communities\n",
    "* Louvain method (fast community unfolding), multilevel community\n",
    "* link clustering\n",
    "* Markov clustering\n",
    "* clique percolation method\n",
    "* networkx.algorithms.clique.find_cliques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cosim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosim\n",
    "import networkx as nx\n",
    "\n",
    "def coll_cols(x, threshold=0.99, how='corr_aff', ignore_nan=True):\n",
    "    '''\n",
    "     x : df\n",
    "     threshold : maximum correlation or maximum VIF\n",
    "     how : correlation (corr_aff, corr_pair) of VIF-based (vif)\n",
    "    '''\n",
    "    cols = x.columns.tolist()\n",
    "    cols_to_remove = None\n",
    "    coll_cliques = []\n",
    "    if ignore_nan:\n",
    "        x=x.dropna() # x.fillna(x.median())   \n",
    "    if how=='vif':\n",
    "        cols_to_remove = []\n",
    "        x = add_constant(x)\n",
    "        x[cols] = StandardScaler().fit_transform(x[cols]) \n",
    "        vifs = []\n",
    "        for idx, col in tqdm(enumerate(cols)):\n",
    "            _vif = vif(x.values, idx)\n",
    "            if _vif != np.inf:                \n",
    "                vifs.append(_vif)\n",
    "            if vif(x.values, idx)>threshold:\n",
    "                cols_to_remove.append(col)\n",
    "        vifs = np.array(vifs)\n",
    "        print(\"VIF:\\t max {}, \\t min {}, \\t mean {}, \\t median {}\".format(vifs.max(), vifs.min(), vifs.mean(), np.median(vifs)))\n",
    "        plt.hist(np.log10(vifs), bins=50, density=True)\n",
    "    elif how=='corr_aff':\n",
    "        corrcoefs = np.abs(cosim(x.T)) # x.corr().values  # np.corrcoef(tmp[var_cols]) , tmp[var_cols].corr().values       \n",
    "        #corrcoefs[np.abs(corrcoefs)>threshold] = 0\n",
    "        #corrcoefs[np.abs(corrcoefs)<=threshold] = 1\n",
    "        AF = AffinityPropagation(damping=0.75, max_iter=400, convergence_iter=50, preference=None, affinity='precomputed')\n",
    "        AF.fit_predict(-corrcoefs)\n",
    "        _coll_cliques = AF.labels_\n",
    "        coll_cliques = defaultdict(list)\n",
    "        for idx, _lab in enumerate(_coll_cliques):\n",
    "            coll_cliques[_lab] = coll_cliques[_lab]+[idx]\n",
    "        new_list = []\n",
    "        for k,v in coll_cliques.items():\n",
    "            new_list.append(list(set([k]+v)))\n",
    "        coll_cliques = new_list\n",
    "        exemplars = AF.cluster_centers_indices_\n",
    "        cols_to_remove = [cols[i] for i in exemplars]\n",
    "        # should be cols_to_keep!!\n",
    "    elif how =='corr_clique':\n",
    "        corrcoefs = np.abs(cosim(x.T)) #x.corr().values  # np.corrcoef(tmp[var_cols]) , tmp[var_cols].corr().values         \n",
    "        corrcoefs[np.abs(corrcoefs)>threshold] = 1\n",
    "        corrcoefs[np.abs(corrcoefs)<=threshold] = 0\n",
    "        corrcoefs = corrcoefs.astype(np.int32)\n",
    "        G = nx.Graph(corrcoefs)\n",
    "        coll_cliques = list(nx.algorithms.clique.find_cliques(G))\n",
    "        coll_cliques = [_clique for _clique in coll_cliques if len(_clique)>1]\n",
    "        cols_to_remove = list(set([_node for _clique in coll_cliques for _node in _clique if len(_clique)>1]))\n",
    "        cols_to_remove = [cols[i] for i in cols_to_remove]\n",
    "    elif how=='corr_pair':\n",
    "        # simplistic \n",
    "        corrcoefs = np.abs(cosim(x.T)) #x.corr().values # cosim(x) # x.corr().values # np.abs(np.corrcoef(x, rowvar=False)), cosim(x)\n",
    "        print(corrcoefs.shape, x.shape)\n",
    "        conn_count = { k: 0 for k in cols}\n",
    "        _coll_cliques = defaultdict(list)\n",
    "        for idx, _colx in enumerate(cols):\n",
    "            for jdx, _coly in enumerate(cols):\n",
    "                if jdx<idx:\n",
    "                    if corrcoefs[idx, jdx]>threshold:\n",
    "                        conn_count[_colx]  += 1\n",
    "                        conn_count[_coly]  += 1\n",
    "                        _coll_cliques[idx]  += [jdx]\n",
    "                        _coll_cliques[jdx]  += [idx]         \n",
    "        cols_to_remove = [cols[_v[0]] for _v in _coll_cliques.values()]\n",
    "        for k,v in _coll_cliques.items():\n",
    "            coll_cliques.append([k]+v)            \n",
    "        coll_cliques=[sorted(_clique) for _clique in coll_cliques]\n",
    "        coll_cliques.sort()\n",
    "        coll_cliques = list(coll_cliques for coll_cliques,_ in itertools.groupby(coll_cliques))\n",
    "    #print(\"{} columns should be removed due to collinearity.\".format(len(cols_to_remove)))  \n",
    "    return cols_to_remove, coll_cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_prior = dict()\n",
    "col_dict['inter_celldyn'] = col_dict['history_celldyn'] + col_dict['pheno_celldyn']\n",
    "col_dict['inter_ecg'] = col_dict['history_ecg_cols_agg'] + col_dict['pheno_ecg_cols_agg']\n",
    "\n",
    "cols_prior['ecg'] = col_dict['ecg']\n",
    "cols_prior['celldyn'] = col_dict['celldyn']\n",
    "cols_prior['inter_celldyn'] = col_dict['inter_celldyn']\n",
    "cols_prior['inter_ecg'] = col_dict['inter_ecg']\n",
    "\n",
    "dat_tmp_old = dat_tmp\n",
    "dat_old = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "col_dict['ecg'], col_dict['celldyn'], cols_prior['inter_celldyn'], cols_prior['inter_ecg'] = cols_prior['ecg'], cols_prior['celldyn'], cols_prior['inter_celldyn'], cols_prior['inter_ecg']\n",
    "dat_tmp = dat_tmp_old\n",
    "dat = dat_old\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRE: Condition numbers for RAW set:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t inter CELLDYN {} and \\t inter ECG {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['inter_celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['inter_ecg']])))\n",
    "print(\"PRE: Condition numbers for scaled set:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t inter CELLDYN {} and \\t inter ECG {}\".format(_condition_number(dat_tmp[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat_tmp[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat_tmp[col_dict['inter_celldyn']]),\n",
    "                                                                            _condition_number(dat_tmp[col_dict['inter_ecg']])))\n",
    "\n",
    "if remove_multicoll:\n",
    "    print(\"We start with {} ECG, {} CELLDYN, {} inter ECG and {} inter CELLDYN columns\".format(len(col_dict['ecg']), len(col_dict['celldyn']), len(col_dict['inter_ecg']), len(col_dict['inter_celldyn'])))\n",
    "    gc.collect()\n",
    "    drop_cols_ecg, cliques_ecg = coll_cols(dat_tmp[col_dict['ecg']], how='corr_clique', threshold=0.99, ignore_nan=False)\n",
    "    drop_cols_celldyn, cliques_celldyn = coll_cols(dat_tmp[col_dict['celldyn']], how='corr_clique', threshold=0.99, ignore_nan=False)\n",
    "    drop_cols_intercell, cliques_intercell = coll_cols(dat_tmp[col_dict['inter_celldyn']], how='corr_clique', threshold=0.99, ignore_nan=False)\n",
    "    drop_cols_interecg, cliques_interecg = coll_cols(dat_tmp[col_dict['inter_ecg']], how='corr_clique', threshold=0.99, ignore_nan=False)\n",
    "    \n",
    "    print(\"There are {} ECG cliques, {} CELLDYN cliques, {} intercelldyn cliques, {} interecg cliques\".format(len(cliques_ecg), len(cliques_celldyn), len(cliques_intercell), len(cliques_interecg)))\n",
    "    print(\"We remove {} ECG columns, {} CELLDYN columns, {} intercelldyn columns, {} interecg cliques\".format(len(set(drop_cols_ecg)), len(set(drop_cols_celldyn)), len(cliques_intercell), len(cliques_interecg)))\n",
    "    \n",
    "    # replace the multicoll-columns\n",
    "    dat_ecg_mcoll = process_cliques(dat, agg_function='amean', cliques=cliques_ecg, cols=col_dict['ecg'], prefix='mcoll_ecg_')\n",
    "    dat_celldyn_mcoll = process_cliques(dat, agg_function='amean', cliques=cliques_celldyn, cols=col_dict['celldyn'], prefix='mcoll_celldyn')\n",
    "    dat_interecg_mcoll = process_cliques(dat, agg_function='amean', cliques=cliques_interecg, cols=col_dict['inter_ecg'], prefix='mcoll_interecg_')\n",
    "    dat_intercelldyn_mcoll = process_cliques(dat, agg_function='amean', cliques=cliques_intercell, cols=col_dict['inter_celldyn'], prefix='mcoll_intercelldyn')\n",
    "    \n",
    "    dat_ecg_mcoll_tmp = process_cliques(dat_tmp, agg_function='amean', cliques=cliques_ecg, cols=col_dict['ecg'], prefix='mcoll_ecg_')\n",
    "    dat_celldyn_mcoll_tmp = process_cliques(dat_tmp, agg_function='amean', cliques=cliques_celldyn, cols=col_dict['celldyn'], prefix='mcoll_celldyn')\n",
    "    dat_interecg_mcoll_tmp = process_cliques(dat_tmp, agg_function='amean', cliques=cliques_interecg, cols=col_dict['inter_ecg'], prefix='mcoll_interecg_')\n",
    "    dat_intercelldyn_mcoll_tmp = process_cliques(dat_tmp, agg_function='amean', cliques=cliques_intercell, cols=col_dict['inter_celldyn'], prefix='mcoll_intercelldyn')\n",
    "    \n",
    "    # remove the original columns\n",
    "    col_dict['ecg'] = list(set(col_dict['ecg']) - set(drop_cols_ecg)) + dat_ecg_mcoll.columns.tolist()\n",
    "    col_dict['celldyn'] = list(set(col_dict['celldyn']) - set(drop_cols_celldyn)) + dat_celldyn_mcoll.columns.tolist()\n",
    "    col_dict['inter_ecg'] = list(set(col_dict['inter_ecg']) - set(drop_cols_interecg)) + dat_interecg_mcoll_tmp.columns.tolist()\n",
    "    col_dict['inter_celldyn'] = list(set(col_dict['inter_celldyn']) - set(drop_cols_intercell)) + dat_intercelldyn_mcoll_tmp.columns.tolist()\n",
    "    \n",
    "    print(\"We now have {} ECG, {} CELLDYN, {} inter ECG and {} inter CELLDYN columns\".format(len(col_dict['ecg']), len(col_dict['celldyn']), len(col_dict['inter_ecg']), len(col_dict['inter_celldyn'])))\n",
    "    \n",
    "    dat=pd.concat([dat, dat_ecg_mcoll, dat_celldyn_mcoll, dat_interecg_mcoll, dat_intercelldyn_mcoll], axis=1)\n",
    "    dat_tmp=pd.concat([dat_tmp, dat_ecg_mcoll_tmp, dat_celldyn_mcoll_tmp, dat_interecg_mcoll_tmp, dat_intercelldyn_mcoll_tmp], axis=1)\n",
    "    \n",
    "    print(\"POST: Condition numbers for RAW set:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t inter CELLDYN {} and \\t inter ECG {}\".format(_condition_number(dat[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['inter_celldyn']]),\n",
    "                                                                            _condition_number(dat[col_dict['inter_ecg']])))\n",
    "    print(\"POST: Condition numbers for scaled set:\\n\\t\\t ECG {}, \\t CELLDYN {}, \\t inter CELLDYN {} and \\t inter ECG {}\".format(_condition_number(dat_tmp[col_dict['ecg']]), \n",
    "                                                                            _condition_number(dat_tmp[col_dict['celldyn']]),\n",
    "                                                                            _condition_number(dat_tmp[col_dict['inter_celldyn']]),\n",
    "                                                                            _condition_number(dat_tmp[col_dict['inter_ecg']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling, a posteriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \n",
    "dat_unscaled = dat.copy()\n",
    "\n",
    "if scaling_aposteriori:\n",
    "    if scaler is not None:\n",
    "        if isinstance(scaler, dict):        \n",
    "            for _imp_key, _scaler in scaler.items():\n",
    "                if _scaler is not None:\n",
    "                    dat[col_dict[_imp_key]] = _scaler.fit_transform(dat[col_dict[_imp_key]]) \n",
    "        else:   \n",
    "            _cols = col_dict['ecg']+col_dict['celldyn']\n",
    "            dat[_cols] = pd.DataFrame(data=scaler.fit_transform(dat[_cols]), index=HS.index, columns=_cols)\n",
    "    dat.apply(lambda x: sum(np.isinf(x))).sum(), dat.isna().sum().sum()\n",
    "    \n",
    "fig, ax = plt.subplots(ncols=2, figsize=(14,7))\n",
    "dat_unscaled.min().plot.hist(bins=20, ax=ax[0])\n",
    "dat_unscaled.max().plot.hist(bins=20, ax=ax[0])\n",
    "                      \n",
    "dat.min().plot.hist(bins=20, ax=ax[1])\n",
    "dat.max().plot.hist(bins=20, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "We have already treated the univariate outliers, which revolves around the normality of the feature values \n",
    "from the perspective of the specific feature alone. Univariate outlier treatment was important to regularize \n",
    "any data transformation and feature augmentation. We will now treat the multivariate case where samples \n",
    "can have an outlying combination of feature values. Python libraries to treat outliers are:\n",
    "* [sklearn](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "* [pyod](https://pyod.readthedocs.io/en/latest/install.html)\n",
    "\n",
    "We have three ways of applying multivariate outlier detection:\n",
    "1. a priori removal or replacement of all outlying samples\n",
    "2. detection and tagging of outlying test samples prior to prediction for a posteriori segmentation \n",
    "3. treat the outlier/inlier label as a feature\n",
    "Of course, all can be applied together. Typical methods are:\n",
    "* LOF\n",
    "* isoForest\n",
    "* density-based clustering \n",
    "* AutoEncoder, or any encoder-decoder method, could also be a more conventional matrix factorisation method.\n",
    "\n",
    "Interesting would be to invert the idea of Affinity Propagation (as is done by Janssens' Stochastic Outlier Selection).\n",
    "\n",
    "When we apply an outlier detector on **unseen** data we refer to it as **novelty detection**, the latter also assumes that **no** outliers \n",
    "are present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahalanobis distance for outliers\n",
    "def mahalanobis(x=None, cov=None):\n",
    "    # source: https://www.machinelearningplus.com/statistics/mahalanobis-distance/\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(x)\n",
    "    if not cov:\n",
    "        cov = np.cov(x.values.T)\n",
    "    inv_covmat = sc.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "if remove_outlying_samples_from_train:\n",
    "    var_cols = col_dict['ecg']+col_dict['celldyn']+col_dict['inter_celldyn']+col_dict['inter_ecg']\n",
    "\n",
    "    out_idx_dict= {}\n",
    "\n",
    "    iso = IsolationForest(n_estimators=600, n_jobs=4)\n",
    "    out_in = iso.fit_predict(dat[var_cols])\n",
    "    out_idx_dict['iso'] = np.where(out_in==-1)\n",
    "    in_idx = np.where(out_in==1)\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(14,7))\n",
    "    pd.DataFrame(iso.score_samples(dat[var_cols])).plot.kde(ax=ax[0])\n",
    "\n",
    "    ocs = OneClassSVM(kernel='rbf', gamma='scale', max_iter=1000)\n",
    "    out_in = ocs.fit_predict(dat[var_cols])\n",
    "    out_idx_dict['svm'] = np.where(out_in==-1)\n",
    "\n",
    "    try:\n",
    "        mah = mahalanobis(dat[var_cols])\n",
    "        dat['_maha'] = mah/(np.max(mah)-np.min(mah))\n",
    "        # isolation forest\n",
    "        dat._maha.plot.hist(bins=50, ax=ax[1])\n",
    "    except:\n",
    "        print(\"matrix is singular..you did not remove the collinearity did you ;)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced dimensionality visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_features = ['RF_FamHist', 'RF_Smok', 'RF_HyperTens','RF_obese30', 'RF_HyperChol','RF_Diab', 'RF_CVDHist']\n",
    "col_dict['tn_admission'] = ['tn_admission']\n",
    "feature_set = defaultdict(dict)\n",
    "feature_set['0']['_withoutCelldyn'] = ['pheno'] \n",
    "feature_set['0']['_withCelldyn'] = ['pheno', 'celldyn']\n",
    "feature_set['1']['_withoutCelldyn'] =  ['pheno', 'ecg']\n",
    "feature_set['1']['_withCelldyn'] =  ['pheno', 'ecg', 'celldyn']\n",
    "feature_set['2']['_withoutCelldyn'] =  ['pheno', 'ecg', 'tn_admission']\n",
    "feature_set['2']['_withCelldyn'] =  ['pheno', 'ecg', 'tn_admission', 'celldyn']\n",
    "feature_set['3']['_withoutCelldyn'] =  ['pheno', 'ecg', 'history', 'tn_admission']\n",
    "feature_set['3']['_withCelldyn'] = ['pheno', 'ecg', 'history', 'tn_admission', 'celldyn']\n",
    "feature_set['4']['_withoutCelldyn'] =  ['pheno', 'ecg', 'history', 'tn_admission', 'inter_ecg'] \n",
    "feature_set['4']['_withCelldyn'] = ['pheno', 'ecg', 'history', 'tn_admission', 'celldyn', 'inter_ecg', 'inter_celldyn']\n",
    "feature_set['5']['_withoutCelldyn'] =  ['pheno', 'ecg', 'history', 'tn_admission', 'hs', 'inter_ecg']\n",
    "feature_set['5']['_withCelldyn'] = ['pheno', 'ecg', 'history', 'tn_admission', 'hs', 'celldyn', 'inter_ecg', 'inter_celldyn']\n",
    "feature_set['6']['_withoutCelldyn'] =  ['pheno', 'ecg', 'history', 'tn_admission', 'hs', 'slope', 'inter_ecg']\n",
    "feature_set['6']['_withCelldyn'] = ['pheno', 'ecg', 'history', 'tn_admission', 'hs', 'slope', 'celldyn', 'inter_ecg', 'inter_celldyn'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ECG: {} cols, \\t CELLDYN: {} cols, \\t OTHER:{} cols\".format(len(col_dict['ecg']), len(col_dict['celldyn']), len(col_dict['other'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_viz={}\n",
    "nc = 20\n",
    "red_cols = ['pc_'+str(i) for i in range(0, nc)]\n",
    "pc = {}\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20,7))\n",
    "\n",
    "pc['ecg'] = PCA(n_components=nc, svd_solver='full')\n",
    "tred = pc['ecg'].fit_transform(dat[col_dict['ecg']])\n",
    "ax[0].plot(pc['ecg'].explained_variance_ratio_)\n",
    "ax[0].set_title('Explained variance ECG')\n",
    "dat_viz['pca_celldyn']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "pc['celldyn'] = PCA(n_components=nc, svd_solver='full')\n",
    "tred = pc['celldyn'].fit_transform(dat[col_dict['celldyn']])\n",
    "ax[1].plot(pc['celldyn'].explained_variance_ratio_)\n",
    "ax[1].set_title('Explained variance CELLDYN')\n",
    "dat_viz['pca_ecg']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance of the first component is huge, this suggests that there is a leaking feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18,10))\n",
    "ax[0,0].hist(pc['ecg'].components_[0], bins=100); ax[0,0].set_title('ecg PC 1')\n",
    "ax[0,1].hist(pc['ecg'].components_[1], bins=100); ax[0,1].set_title('ecg PC 2')\n",
    "ax[1,0].hist(pc['celldyn'].components_[0], bins=100); ax[1,0].set_title('celldyn PC 1')\n",
    "ax[1,1].hist(pc['celldyn'].components_[1], bins=100); ax[1,1].set_title('celldyn PC 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(18,7))\n",
    "sns.scatterplot(data=dat_viz['pca_ecg'], x='pc_0', y='pc_1', hue=y, ax=ax[0])\n",
    "sns.scatterplot(data=dat_viz['pca_celldyn'], x='pc_0', y='pc_1', hue=y, ax=ax[1])\n",
    "ax[0].set_title('PCA ECG')\n",
    "ax[1].set_title('PCA CELLDYN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 20\n",
    "red_cols = ['pc_'+str(i) for i in range(0, nc)]\n",
    "\n",
    "nnmf = {}\n",
    "nnmf['ecg'] = NMF(n_components=nc)\n",
    "tred = nnmf['ecg'].fit_transform(dat[col_dict['ecg']]+dat[col_dict['ecg']].min().abs())\n",
    "dat_viz['nmnf_ecg']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "nnmf['celldyn'] = NMF(n_components=nc)\n",
    "tred = nnmf['celldyn'].fit_transform(dat[col_dict['celldyn']]+dat[col_dict['celldyn']].min().abs())\n",
    "dat_viz['nmnf_celldyn']  = pd.DataFrame(data=tred, index=dat.index, columns=red_cols)\n",
    "\n",
    "print(\"Reconstruction errors: ECG {}, \\t CELLDYN {}\".format(nnmf['ecg'].reconstruction_err_, \n",
    "                                                                         nnmf['celldyn'].reconstruction_err_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(18,7))\n",
    "sns.scatterplot(data=dat_viz['nmnf_ecg'], x='pc_0', y='pc_1', hue=y, ax=ax[0])\n",
    "sns.scatterplot(data=dat_viz['nmnf_celldyn'], x='pc_0', y='pc_1', hue=y, ax=ax[1])\n",
    "ax[0].set_title('NMF ECG')\n",
    "ax[1].set_title('NMF CELLDYN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple inspection of the embedded data we expect that the meta features and the HS features are well able to seperate the NSTEMI's from the control group.\n",
    "\n",
    "This suggests that CELLDYN and ECG data (without the aggregation labels provided by the manufacturer, see Mark) alone are not enough to separate the NSTEMI's from the control group.\n",
    "\n",
    "Starting with CELLDYN we add, (age, gender, BMI), (history features) and the initial troponine level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, including HS to the features hardly increases the visual separability of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_red = UMAP(n_components=2)\n",
    "tsne_red = TSNE(n_components=2)\n",
    "\n",
    "set0 = col_dict['pheno']+col_dict['celldyn']\n",
    "set1 = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']\n",
    "set2 = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']\n",
    "set3 = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+col_dict['troponine']\n",
    "set4 = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+col_dict['troponine']+inter_ecg_cols+inter_celldyn_cols\n",
    "set5 = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+col_dict['troponine']+col_dict['hs']+inter_ecg_cols+inter_celldyn_cols\n",
    "set6 = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+col_dict['troponine']+col_dict['hs']+col_dict['slope']+inter_ecg_cols+inter_celldyn_cols\n",
    "\n",
    "dat_viz['umap_fs0'] = pd.DataFrame(data=um_red.fit_transform(dat[set0]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_fs1'] = pd.DataFrame(data=um_red.fit_transform(dat[set1]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_fs2'] =  pd.DataFrame(data=um_red.fit_transform(dat[set2]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_fs3'] = pd.DataFrame(data=um_red.fit_transform(dat[set3]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_fs4'] = pd.DataFrame(data=um_red.fit_transform(dat[set4]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_fs5'] = pd.DataFrame(data=um_red.fit_transform(dat[set5]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['umap_fs6'] = pd.DataFrame(data=um_red.fit_transform(dat[set6]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "\n",
    "dat_viz['tsne_fs0'] = pd.DataFrame(data=tsne_red.fit_transform(dat[set0]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_fs1'] = pd.DataFrame(data=tsne_red.fit_transform(dat[set1]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_fs2'] =  pd.DataFrame(data=tsne_red.fit_transform(dat[set2]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_fs3'] = pd.DataFrame(data=tsne_red.fit_transform(dat[set3]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_fs4'] = pd.DataFrame(data=tsne_red.fit_transform(dat[set4]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_fs5'] = pd.DataFrame(data=tsne_red.fit_transform(dat[set5]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "dat_viz['tsne_fs6'] = pd.DataFrame(data=tsne_red.fit_transform(dat[set6]), index=dat.index, columns=['pc_1', 'pc_2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=7, figsize=(20,40))\n",
    "sns.scatterplot(data=dat_viz['umap_fs0'], x='pc_1', y='pc_2', hue=y, ax=ax[0,0])\n",
    "sns.scatterplot(data=dat_viz['umap_fs1'], x='pc_1', y='pc_2', hue=y, ax=ax[1,0])\n",
    "sns.scatterplot(data=dat_viz['umap_fs2'], x='pc_1', y='pc_2', hue=y, ax=ax[2,0])\n",
    "sns.scatterplot(data=dat_viz['umap_fs3'], x='pc_1', y='pc_2', hue=y, ax=ax[3,0])\n",
    "sns.scatterplot(data=dat_viz['umap_fs4'], x='pc_1', y='pc_2', hue=y, ax=ax[4,0])\n",
    "sns.scatterplot(data=dat_viz['umap_fs5'], x='pc_1', y='pc_2', hue=y, ax=ax[5,0])\n",
    "sns.scatterplot(data=dat_viz['umap_fs6'], x='pc_1', y='pc_2', hue=y, ax=ax[6,0])\n",
    "\n",
    "ax[0,0].set_title('UMAP featureset 0')\n",
    "ax[1,0].set_title('UMAP featureset 1')\n",
    "ax[2,0].set_title('UMAP featureset 2')\n",
    "ax[3,0].set_title('UMAP featureset 3')\n",
    "ax[4,0].set_title('UMAP featureset 4')\n",
    "ax[5,0].set_title('UMAP featureset 5')\n",
    "ax[6,0].set_title('UMAP featureset 6')\n",
    "\n",
    "sns.scatterplot(data=dat_viz['tsne_fs0'], x='pc_1', y='pc_2', hue=y, ax=ax[0,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_fs1'], x='pc_1', y='pc_2', hue=y, ax=ax[1,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_fs2'], x='pc_1', y='pc_2', hue=y, ax=ax[2,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_fs3'], x='pc_1', y='pc_2', hue=y, ax=ax[3,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_fs4'], x='pc_1', y='pc_2', hue=y, ax=ax[4,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_fs5'], x='pc_1', y='pc_2', hue=y, ax=ax[5,1])\n",
    "sns.scatterplot(data=dat_viz['tsne_fs6'], x='pc_1', y='pc_2', hue=y, ax=ax[6,1])\n",
    "ax[0,1].set_title('TSNE featureset 0')\n",
    "ax[1,1].set_title('TSNE featureset 1')\n",
    "ax[2,1].set_title('TSNE featureset 2')\n",
    "ax[3,1].set_title('TSNE featureset 3')\n",
    "ax[4,1].set_title('TSNE featureset 4')\n",
    "ax[5,1].set_title('TSNE featureset 5')\n",
    "ax[6,1].set_title('TSNE featureset 6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = get_var_cols(col_dict)\n",
    "\n",
    "if run_supervised_umap:\n",
    "    um_all_sup = UMAP(n_components=3, n_neighbors=10, min_dist=0.1)\n",
    "    dat_viz['umap_all_sup'] = pd.DataFrame(data=um_all_sup.fit_transform(dat[all_cols], \n",
    "                                                                 y=y), \n",
    "                                       index=dat.index, columns=['pcu_1', 'pcu_2', 'pcu_3'])\n",
    "\n",
    "    um_ecg_sup = UMAP(n_components=3, n_neighbors=10, min_dist=0.1)\n",
    "    dat_viz['umap_ecg_sup'] = pd.DataFrame(data=um_ecg_sup.fit_transform(dat[col_dict['ecg']], \n",
    "                                                                 y=y), \n",
    "                                       index=dat.index, columns=['pcu_1', 'pcu_2', 'pcu_3'])\n",
    "\n",
    "    um_celldyn_sup = UMAP(n_components=3, n_neighbors=10, min_dist=0.1)\n",
    "    dat_viz['umap_celldyn_sup'] = pd.DataFrame(data=um_celldyn_sup.fit_transform(dat[col_dict['celldyn']], \n",
    "                                                                 y=y), \n",
    "                                       index=dat.index, columns=['pcu_1', 'pcu_2', 'pcu_3'])\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(26,18))\n",
    "    sns.scatterplot(data=dat_viz['umap_all_sup'], x='pcu_1', y='pcu_2', hue=y, ax=ax[0,0])\n",
    "    sns.scatterplot(data=dat_viz['umap_all_sup'], x='pcu_1', y='pcu_3', hue=y, ax=ax[0,1])\n",
    "    sns.scatterplot(data=dat_viz['umap_all_sup'], x='pcu_2', y='pcu_3', hue=y, ax=ax[0,2])\n",
    "\n",
    "    sns.scatterplot(data=dat_viz['umap_ecg_sup'], x='pcu_1', y='pcu_2', hue=y, ax=ax[1,0])\n",
    "    sns.scatterplot(data=dat_viz['umap_ecg_sup'], x='pcu_1', y='pcu_3', hue=y, ax=ax[1,1])\n",
    "    sns.scatterplot(data=dat_viz['umap_ecg_sup'], x='pcu_2', y='pcu_3', hue=y, ax=ax[1,2])\n",
    "\n",
    "    sns.scatterplot(data=dat_viz['umap_celldyn_sup'], x='pcu_1', y='pcu_2', hue=y, ax=ax[2,0])\n",
    "    sns.scatterplot(data=dat_viz['umap_celldyn_sup'], x='pcu_1', y='pcu_3', hue=y, ax=ax[2,1])\n",
    "    sns.scatterplot(data=dat_viz['umap_celldyn_sup'], x='pcu_2', y='pcu_3', hue=y, ax=ax[2,2])\n",
    "\n",
    "    ax[0,0].set_title('All features')\n",
    "    ax[0,1].set_title('All features')\n",
    "    ax[0,2].set_title('All features')\n",
    "\n",
    "    ax[1,0].set_title('ECG features')\n",
    "    ax[1,1].set_title('ECG features')\n",
    "    ax[1,2].set_title('ECG features')\n",
    "\n",
    "    ax[2,0].set_title('Celldyn features')\n",
    "    ax[2,1].set_title('Celldyn features')\n",
    "    ax[2,2].set_title('Celldyn features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised UMAP clustering seems to be well able to separate the targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear separability using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_ecg = LDA(n_components=2)\n",
    "lin_sep = pd.DataFrame(data=LDA_ecg.fit_transform(dat[col_dict['ecg']], y=y), index=dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_ecg.coef_[0], columns=['coeff'], index=col_dict['ecg'])\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='0')\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=True, ax=ax[1], label='1')\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('ECG LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators_lda = dict()\n",
    "strong_separators_lda['ecg'] = pd.concat([coeff.loc[coeff.coeff>q95], coeff.loc[coeff.coeff<q05]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0\n",
    "\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear ECG. \\t negative accuracy:{}, positive accuracy:{}'.format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celldyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_celldyn = LDA(n_components=2)\n",
    "cols = col_dict['celldyn']+inter_ecg_cols+inter_celldyn_cols\n",
    "_dat = dat\n",
    "lin_sep = pd.DataFrame(data=LDA_celldyn.fit_transform(_dat[cols], y=y), index=_dat.index, columns=['lda'])\n",
    "coeff = pd.DataFrame(data=LDA_celldyn.coef_[0], columns=['coeff'], index=cols)\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep.loc[y==0].plot.hist(bins=50, histtype='step', density=False, label='0', ax=ax[1])\n",
    "lin_sep.loc[y==1].plot.hist(bins=50, histtype='step', density=False, label='1', ax=ax[1])\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation')\n",
    "plt.suptitle('CELLDYN LDA')\n",
    "\n",
    "# strong linear separators\n",
    "strong_separators_lda['celldyn'] = pd.concat([coeff.loc[coeff.coeff>q95], coeff.loc[coeff.coeff<q05]])\n",
    "\n",
    "t = pd.DataFrame(lin_sep.lda)\n",
    "t['target'] = y\n",
    "thres=0.\n",
    "neg, pos = 1-t.loc[t.lda<thres].target.mean(), t.loc[t.lda>thres].target.mean()\n",
    "print('Linear CELLDYN. \\t negative accuracy:{}, positive accuracy:{}'.format( neg , pos ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very clear separation between the two classes for the celldyn data which suggests that this data can be used to predict NSTEMI accurately. \n",
    "\n",
    "To verify this statement we have to split the data in a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  col_dict['celldyn']+inter_celldyn_cols #get_var_cols(col_dict)\n",
    "LDA_celldyn = LDA(n_components=2, priors=[0.5, 0.5])\n",
    "_dat = dat_unscaled\n",
    "X_train, X_test, y_train, y_test = train_test_split(_dat[cols], y, test_size=0.1, random_state=322, stratify=y)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=321, k_neighbors=50)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "LDA_celldyn.fit(X_train, y=y_train)\n",
    "\n",
    "lin_sep_train = pd.DataFrame(data=LDA_celldyn.transform(X_train), columns=['lda'])\n",
    "lin_sep_test = pd.DataFrame(data=LDA_celldyn.transform(X_test), columns=['lda'])\n",
    "\n",
    "coeff = pd.DataFrame(data=LDA_celldyn.coef_[0], columns=['coeff'], index=cols)\n",
    "\n",
    "q95 = coeff.quantile(0.95)[0]\n",
    "q05 = coeff.quantile(0.05)[0]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(22,8))\n",
    "coeff.plot.hist(bins=100, ax=ax[0])\n",
    "ax[0].axvline(q95, color='black')\n",
    "ax[0].axvline(q05, color='black')\n",
    "lin_sep_train.loc[y_train==0].plot.hist(bins=50, histtype='step', density=False, label='0', ax=ax[1])\n",
    "lin_sep_train.loc[y_train==1].plot.hist(bins=50, histtype='step', density=False, label='1', ax=ax[1])\n",
    "lin_sep_test.loc[y_test==0].plot.hist(bins=50, histtype='step', density=False, label='0', ax=ax[1])\n",
    "lin_sep_test.loc[y_test==1].plot.hist(bins=50, histtype='step', density=False, label='1', ax=ax[1])\n",
    "#ax[1].set_xlim(-20,20)\n",
    "ax[1].legend()\n",
    "ax[0].set_title('Feature coefficients')\n",
    "ax[1].set_title('Class separation of training set')\n",
    "plt.suptitle('CELLDYN LDA train/test')\n",
    "\n",
    "y_pred = LDA_celldyn.predict_proba(X_test)[:,1]\n",
    "\n",
    "_metrics = get_accuracy_plots(y_test, y_pred.copy());\n",
    "print(\"F1:{}, NPV:{}, ACC:{}, REC:{}, AUC:{}\".format(fb_score(y_test, y_pred), \n",
    "                                             npv(y_test, y_pred), \n",
    "                                             balanced_accuracy(y_test, y_pred), \n",
    "                                             recall(y_test, y_pred),\n",
    "                                             metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other aspects are:\n",
    "* sensitivity of data to outliers (should be taken care of with quantile transformations)\n",
    "* co-variance shift\n",
    "* non-linearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient clustering\n",
    "\n",
    "Using weighted correlation between the patient, where the weights are obtained from prior determined feature importances, say from LDA or PCA.\n",
    "\n",
    "We can apply a pair-wise distance metric:\n",
    "* KL-divergence\n",
    "* cosine, euclidean etc.\n",
    "\n",
    "Where it makes sense to include factor weights based on the separability.\n",
    "\n",
    "Clustering options \n",
    "* Spectral clustering\n",
    "* Hierchical clustering\n",
    "* Louvain method (Fast community unfolding)\n",
    "* Label propagation\n",
    "* Walktrap community\n",
    "* Edge betweenness community\n",
    "* Leading eigenvector community\n",
    "* Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cols = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+col_dict['hs']+['tn_admission'] + inter_celldyn_cols + inter_ecg_cols\n",
    "_dat = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral clustering\n",
    "if patient_clustering:\n",
    "    n_comps = 8\n",
    "    nclust= 4\n",
    "    um_all = UMAP(n_components=n_comps, n_neighbors=15, min_dist=0.02)\n",
    "    red_data = pd.DataFrame(data=um_all.fit_transform(_dat[var_cols]), columns=['red_'+str(k) for k in range(0, n_comps)], index=dat.index)\n",
    "\n",
    "    cluster_spectral = SpectralClustering(n_clusters=nclust, eigen_solver=None, n_components=nclust)\n",
    "    cluster_spectral.fit(red_data)\n",
    "    labs = cluster_spectral.labels_\n",
    "\n",
    "    cluster_spectral = SpectralClustering(n_clusters=nclust, eigen_solver=None, n_components=nclust)\n",
    "    cluster_spectral.fit(_dat[var_cols])\n",
    "    labs_full = cluster_spectral.labels_\n",
    "\n",
    "    red_data['spectral_cluster_from_umap'] = labs\n",
    "    red_data['spectral_cluster_from_full'] = labs_full\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(26,14))\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs, s=50, ax=ax[0,0])\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs_full, s=50, ax=ax[0,1])\n",
    "\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=y, s=50, ax=ax[1,0])\n",
    "    try:\n",
    "        sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=mah, s=50, ax=ax[1,1])\n",
    "    except Exception as e:\n",
    "        print(\"Mahalanobis values are not available\")\n",
    "\n",
    "    ax[0, 0].set_title('Clustering on UMAP data')\n",
    "    ax[0, 1].set_title('Clustering on Full data')\n",
    "\n",
    "    red_data = red_data.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTICS clustering\n",
    "if patient_clustering:\n",
    "    cluster_optics = OPTICS(min_samples=10, metric='minkowski', p=1)\n",
    "    cluster_optics.fit(red_data)\n",
    "    labs = cluster_optics.labels_\n",
    "\n",
    "    cluster_optics.fit(_dat[var_cols])\n",
    "    labs_full = cluster_optics.labels_\n",
    "\n",
    "    red_data['optics_cluster_from_umap'] = labs\n",
    "    red_data['optics_cluster_from_full'] = labs_full\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(26,14))\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs, s=50, ax=ax[0,0])\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs_full, s=50, ax=ax[0,1])\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=y, s=50, ax=ax[1,0])\n",
    "    try:\n",
    "        sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=mah, s=50, ax=ax[1,1])\n",
    "    except Exception as e:\n",
    "        print(\"Mahalanobis values are not available\")\n",
    "\n",
    "    ax[0, 0].set_title('Clustering on UMAP data')\n",
    "    ax[0, 1].set_title('Clustering on Full data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative clustering\n",
    "if patient_clustering:\n",
    "    cluster_agglo = AgglomerativeClustering(n_clusters=3, distance_threshold=None, affinity='euclidean')\n",
    "    cluster_agglo.fit(red_data)\n",
    "    labs = cluster_agglo.labels_\n",
    "\n",
    "    cluster_agglo.fit(_dat[var_cols])\n",
    "    labs_full = cluster_agglo.labels_\n",
    "\n",
    "    red_data['agg_cluster_from_umap'] = labs\n",
    "    red_data['agg_cluster_from_full'] = labs_full\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(26,14))\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs, s=50, ax=ax[0,0])\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs_full, s=50, ax=ax[0,1])\n",
    "\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=y, s=50, ax=ax[1,0])\n",
    "    try:\n",
    "        sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=mah, s=50, ax=ax[1,1])\n",
    "    except Exception as e:\n",
    "        print(\"Mahalanobis values are not available\")\n",
    "    ax[0, 0].set_title('Clustering on UMAP data')\n",
    "    ax[0, 1].set_title('Clustering on Full data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if patient_clustering:\n",
    "    cluster_aff = AffinityPropagation(damping=0.5, max_iter=200, convergence_iter=20)\n",
    "    cluster_aff.fit(red_data)\n",
    "    labs = cluster_aff.labels_\n",
    "\n",
    "    cluster_aff.fit(_dat[var_cols])\n",
    "    labs_full = cluster_aff.labels_\n",
    "\n",
    "    red_data['aff_cluster_from_umap'] = labs\n",
    "    red_data['aff_cluster_from_full'] = labs_full\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(26,14))\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs, s=50, ax=ax[0,0])\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs_full, s=50, ax=ax[0,1])\n",
    "\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=y, s=50, ax=ax[1,0])\n",
    "    try:\n",
    "        sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=mah, s=50, ax=ax[1,1])\n",
    "    except Exception as e:\n",
    "        print(\"Mahalanobis values are not available\")\n",
    "    ax[0, 0].set_title('Clustering on UMAP data')\n",
    "    ax[0, 1].set_title('Clustering on Full data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if patient_clustering:\n",
    "    cluster_hdbscan = HDBSCAN()\n",
    "    cluster_hdbscan.fit(red_data)\n",
    "    labs = cluster_aff.labels_\n",
    "\n",
    "    cluster_aff.fit(_dat[var_cols])\n",
    "    labs_full = cluster_aff.labels_\n",
    "\n",
    "    red_data['hdb_cluster_from_umap'] = labs\n",
    "    red_data['hdb_cluster_from_full'] = labs_full\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(26,14))\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs, s=50, ax=ax[0,0])\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=labs_full, s=50, ax=ax[0,1])\n",
    "\n",
    "    sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=y, s=50, ax=ax[1,0])\n",
    "    try:\n",
    "        sns.scatterplot(data=red_data, x='red_0', y='red_1', hue=mah, s=50, ax=ax[1,1])\n",
    "    except Exception as e:\n",
    "        print(\"Mahalanobis values are not available\")\n",
    "    ax[0, 0].set_title('Clustering on UMAP data')\n",
    "    ax[0, 1].set_title('Clustering on Full data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical test for significant difference between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spectral, Agglomerative, Affinity, OPTICS\n",
    "if patient_clustering:\n",
    "    try:\n",
    "        aff_prop = red_data.groupby('aff_cluster_from_full')['target'].agg(['mean', 'count'])\n",
    "        aff_prop['mean'].plot.kde(label='AP', figsize=(14,10))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    spec_prop = red_data.groupby('spectral_cluster_from_full')['target'].agg(['mean', 'count'])\n",
    "    spec_prop['mean'].plot.kde(label='spectral')\n",
    "\n",
    "    opt_prop = red_data.groupby('optics_cluster_from_umap')['target'].agg(['mean', 'count'])\n",
    "    opt_prop['mean'].plot.kde(label='OPTICS')\n",
    "\n",
    "    agg_prop = red_data.groupby('agg_cluster_from_umap')['target'].agg(['mean', 'count'])\n",
    "    agg_prop['mean'].plot.kde(label='Agglomerative')\n",
    "    \n",
    "    hdb_prop = red_data.groupby('hdb_cluster_from_umap')['target'].agg(['mean', 'count'])\n",
    "    hdb_prop['mean'].plot.kde(label='HDBSCAN')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Mean  target values for clusters\")\n",
    "    \n",
    "    print(\"Patients with 95% certainty for N-NSTEMI for AP/OPTICS/Spectral/HDB = {}/{}/{}/{}\".format(\n",
    "            aff_prop.loc[aff_prop['mean']<0.05]['count'].sum(), \n",
    "            opt_prop.loc[opt_prop['mean']<0.05]['count'].sum(),\n",
    "            spec_prop.loc[spec_prop['mean']<0.05]['count'].sum(),\n",
    "            hdb_prop.loc[hdb_prop['mean']<0.05]['count'].sum()))\n",
    "\n",
    "    print(\"Patients with 75% certainty for NSTEMI for AP/OPTICS/Spectral/HDB = {}/{}/{}{}\".format(\n",
    "                aff_prop.loc[aff_prop['mean']>0.75]['count'].sum(), \n",
    "                opt_prop.loc[opt_prop['mean']>0.75]['count'].sum(),\n",
    "                spec_prop.loc[spec_prop['mean']>0.75]['count'].sum(),\n",
    "                hdb_prop.loc[hdb_prop['mean']>.75]['count'].sum()))\n",
    "    \n",
    "    print(\"OPTICS could not cluster {} patients\".format(opt_prop.loc[-1]['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised performance of mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_dict['history']), len(col_dict['pheno']), len(col_dict['hs']), len(col_dict['slope']), len(col_dict['ecg']), len(col_dict['celldyn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+col_dict['troponine']+col_dict['inter_celldyn']+col_dict['inter_ecg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_supervised_umap = True\n",
    "remove_outlying_samples_from_test = False\n",
    "n_comps_viz = 2\n",
    "n_comps_class = 2\n",
    "if run_supervised_umap:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat[cols], y, test_size=0.1, random_state=89)\n",
    "    if remove_outlying_samples_from_test:\n",
    "        iso = IsolationForest(n_estimators=400, n_jobs=4)\n",
    "        iso.fit(X_train)\n",
    "        out_in = iso.predict(X_test)\n",
    "        X_test = X_test.values[np.where(out_in==1)[0].tolist(),:]\n",
    "        y_test = y_test[np.where(out_in==1)[0].tolist()]\n",
    "\n",
    "\n",
    "    umap = UMAP(n_components=n_comps_viz, n_neighbors=7, min_dist=0.05, random_state=323)\n",
    "    umap.fit(X_train, y_train)\n",
    "\n",
    "    red_cols = ['pcu_'+str(k) for k in range(0,n_comps_viz)]\n",
    "    train_transform = pd.DataFrame(data=umap.transform(X_train), columns=red_cols)\n",
    "    test_transform = pd.DataFrame(data=umap.transform(X_test), columns=red_cols)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(26,7))\n",
    "    sns.scatterplot(data=train_transform, x='pcu_0', y='pcu_1', hue=y_train.values, ax=ax[0])\n",
    "    sns.scatterplot(data=test_transform, x='pcu_0', y='pcu_1', hue=y_test.values, ax=ax[1])\n",
    "    ax[0].set_title('CELLDYN, age/gender/BMI, training set')\n",
    "    ax[1].set_title('CELLDYN, age/gender/BMI, test set')\n",
    "    \n",
    "     # get_var_cols(col_dict)\n",
    "    #cols = col_dict['celldyn']+['AGE', 'gender', 'BMI']+['RF_Smok', 'RF_HyperTens', 'RF_Diab', 'RF_FamHist', 'RF_CVDHist', 'RF_HyperChol', 'tn_admission']#+hs_cols+col_dict['ecg'];\n",
    "    clf = RandomForestClassifier() # KNeighborsClassifier(n_neighbors=20, metric='minkowski', p=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat[cols], y, test_size=0.1, random_state=None)\n",
    "    umap = UMAP(n_components=n_comps_class, n_neighbors=7, min_dist=0.07)\n",
    "    umap.fit(X_train, y_train)\n",
    "\n",
    "    train_transform = umap.transform(X_train)\n",
    "    test_transform = umap.transform(X_test)\n",
    "\n",
    "    clf.fit(train_transform, y_train)\n",
    "    y_pred = clf.predict(test_transform)\n",
    "\n",
    "    print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CELLDYN data plus meta information and clinically available information we see that the supervised embedding generalises rather poorly. We expect that with more data this generalisation improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM\n",
    "\n",
    "SuSi: [paper](https://arxiv.org/abs/1903.11114), [code](https://github.com/felixriese/susi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experimental_supervisors:\n",
    "    import susi\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dat[cols], y, test_size=0.1, random_state=89)\n",
    "\n",
    "    cclf = susi.SOMClustering(n_rows=31, n_columns=31)\n",
    "    cclf.fit(dat[cols].values)\n",
    "    print(\"-- SOM fitted --\")\n",
    "\n",
    "    u_matrix = cclf.get_u_matrix()\n",
    "    plt.imshow(np.squeeze(u_matrix), cmap=\"inferno_r\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    clf = susi.SOMClassifier(n_rows=61,\n",
    "                             n_columns=61,\n",
    "                             n_iter_unsupervised=5000,\n",
    "                             n_iter_supervised=5000,\n",
    "                             random_state=0)\n",
    "    clf.fit(X_train, y_train.astype(np.int32))\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experimental_supervisors:\n",
    "    from neupy import algorithms\n",
    "\n",
    "    # reduce\n",
    "    red = UMAP(n_components=40, n_neighbors=30, min_dist=0.01)\n",
    "    #red = PCA(n_components=30)\n",
    "    red.fit(X_train)\n",
    "    X_train = red.transform(X_train)\n",
    "    X_test = red.transform(X_test)\n",
    "\n",
    "\n",
    "    clf = algorithms.LVQ(n_inputs=X_train.shape[1], n_classes=2, n_subclasses=11, shuffle_data=False)\n",
    "    clf.train(X_train, y_train, epochs=500)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    clf = algorithms.LVQ21(n_inputs=X_train.shape[1], n_classes=2, n_subclasses=11, shuffle_data=False)\n",
    "    clf.train(X_train, y_train, epochs=500)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    clf = algorithms.LVQ3(n_inputs=X_train.shape[1], n_classes=2, n_subclasses=11, shuffle_data=False)\n",
    "    clf.train(X_train, y_train, epochs=500)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred), metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning vector quantisation is clearly not able to separate the classses, we can possibly improve this model using parameter tuning but that is beyond the scope of this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "model = dict()\n",
    "cols = col_dict['pheno']+col_dict['celldyn']+col_dict['ecg']+col_dict['history']+['tn_admission']+inter_celldyn_cols+inter_ecg_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_comparison:    \n",
    "    model['HGBM'] = HistGradientBoostingClassifier(max_iter=500)\n",
    "    scores['HGBM'] = pd.DataFrame(cross_validate(model['HGBM'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "\n",
    "    model['NGB'] = NGBClassifier(n_estimators=300, verbose=0, Dist=Bernoulli) # k_categorical(2)\n",
    "    scores['NGB'] = pd.DataFrame(cross_validate(model['NGB'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "    \n",
    "    #model['LDA'] = LDA()\n",
    "    #scores['LDA'] = pd.DataFrame(cross_validate(model['LDA'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "    \n",
    "    model['LR'] = LogisticRegression(max_iter=500, penalty='elasticnet', solver='saga')\n",
    "    scores['LR'] = pd.DataFrame(cross_validate(model['LR'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "    \n",
    "    model['RF'] = RandomForestClassifier(n_estimators=500)\n",
    "    scores['RF'] = pd.DataFrame(cross_validate(model['RF'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "    \n",
    "    #model['SVC'] = NuSVC(nu=0.5)\n",
    "    #scores['SVC'] = pd.DataFrame(cross_validate(model['SVC'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "    \n",
    "    #model['MLP'] = MLPClassifier(hidden_layer_sizes=(100,50,25), max_iter=200, learning_rate_init=0.005, learning_rate='adaptive')\n",
    "    #scores['MLP'] = pd.DataFrame(cross_validate(model['MLP'], dat[cols].values, y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "    \n",
    "    clfs = [('RF', RandomForestClassifier(n_estimators=200, n_jobs=4)),\n",
    "        ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "        ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "        ('MLP', MLPClassifier(hidden_layer_sizes=(100,50,50), \n",
    "                              max_iter=200, learning_rate_init=0.005, learning_rate='adaptive'))]\n",
    "    model['ensemble_vote'] = VotingClassifier(clfs, voting='soft', weights=[10, 5, 7, 3])\n",
    "    scores['ensemble_vote'] = pd.DataFrame(cross_validate(model['ensemble_vote'], dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    model['ensemble_bag'] = BaggingClassifier(base_estimator=LogisticRegression(max_iter=500, solver='liblinear'), n_estimators=5)\n",
    "    scores['ensemble_bag'] = pd.DataFrame(cross_validate(model['ensemble_bag'], dat[cols].values, y.values, \n",
    "                                                        scoring=scoring, \n",
    "                                                         cv=10, return_train_score=True))\n",
    "    \n",
    "    clfs = [('RF', RandomForestClassifier(n_estimators=50, n_jobs=4)),\n",
    "        ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "        ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "        ('MLP', MLPClassifier(hidden_layer_sizes=(40,30,20), \n",
    "                              max_iter=200, learning_rate_init=0.002, learning_rate='adaptive'))]\n",
    "    model['ensemble_vote_hs'] = VotingClassifier(clfs, voting='soft', weights=[10, 5, 7, 3])\n",
    "    scores['ensemble_vote_hs'] = pd.DataFrame(cross_validate(model['ensemble_vote_hs'], dat[hs_cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    # tn_slope_cols\n",
    "    clfs = [('RF', RandomForestClassifier(n_estimators=25, n_jobs=4)),\n",
    "            ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "            ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "            ('MLP', MLPClassifier(hidden_layer_sizes=(30,20,10), \n",
    "                                  max_iter=500, learning_rate_init=0.002, learning_rate='adaptive'))]\n",
    "    model['ensemble_vote_slope'] = VotingClassifier(clfs, voting='soft', weights=[10, 5, 7, 3])\n",
    "    scores['ensemble_vote_slope'] = pd.DataFrame(cross_validate(model['ensemble_vote_slope'], dat[tn_slope_cols+hs_cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    # tn_slope_cols\n",
    "    clfs = [('RF', RandomForestClassifier(n_estimators=200, n_jobs=4)),\n",
    "            ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "            ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "            ('MLP', MLPClassifier(hidden_layer_sizes=(100,50,50), \n",
    "                                  max_iter=500, learning_rate_init=0.002, learning_rate='adaptive'))]\n",
    "\n",
    "\n",
    "    model['ensemble_vote_slope_ALL'] = VotingClassifier(clfs, voting='soft', weights=[10, 7, 5, 3])\n",
    "    scores['ensemble_vote_slope_ALL'] = pd.DataFrame(cross_validate(model['ensemble_vote_slope_ALL'], dat[tn_slope_cols+hs_cols+cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    # celldyn+age+sex+BMI+troponine\n",
    "    clfs = [('RF', RandomForestClassifier(n_estimators=200, n_jobs=4)),\n",
    "            ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "            ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "            ('MLP', MLPClassifier(hidden_layer_sizes=(100,50,50), \n",
    "                                  max_iter=500, learning_rate_init=0.005, learning_rate='adaptive'))]\n",
    "    model['ensemble_vote'] = VotingClassifier(clfs, voting='soft', weights=[10, 7, 5, 3])\n",
    "    scores['ensemble_vote'] = pd.DataFrame(cross_validate(model['ensemble_vote'], dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    # celldyn+age+sex+BMI+troponine\n",
    "    clfs = [('RF', RandomForestClassifier(n_estimators=200, n_jobs=4)),\n",
    "            ('nuSVC', NuSVC(nu=0.5, probability=True)),\n",
    "            ('LR', LogisticRegression(max_iter=500, solver='liblinear')),\n",
    "            ('MLP', MLPClassifier(hidden_layer_sizes=(100,50,50), \n",
    "                                  max_iter=500, learning_rate_init=0.005, learning_rate='adaptive'))]\n",
    "\n",
    "\n",
    "    model['ensemble_vote'] = VotingClassifier(clfs, voting='soft', weights=[10, 7, 5, 3])\n",
    "\n",
    "    scores['ensemble_vote'] = pd.DataFrame(cross_validate(model['ensemble_vote'], dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    \n",
    "    model['EBM'] = EBM()\n",
    "    #red_umap = UMAP(n_components=32)\n",
    "    X = dat[cols].values\n",
    "    #X = red_umap.fit_transform(X)\n",
    "    scores['EBM'] = pd.DataFrame(cross_validate(model['EBM'], X, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    model['XGB'] = XGB(n_jobs=8)\n",
    "    scores['XGB'] = pd.DataFrame(cross_validate(model['XGB'], dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    model['XGB_includingHS'] = XGB(n_jobs=8)\n",
    "    scores['XGB_includingHS'] = pd.DataFrame(cross_validate(model['XGB_includingHS'], dat[cols+hs_cols].values, \n",
    "                                                            y.values, scoring=scoring, cv=10, return_train_score=True))\n",
    "\n",
    "    model['LGBM'] = LGBM(n_jobs=8)\n",
    "    scores['LGBM'] = pd.DataFrame(cross_validate(model['LGBM'], dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "\n",
    "    model['LGBM_includingHS']  = LGBM(n_jobs=8)\n",
    "    scores['LGBM_includingHS'] = pd.DataFrame(cross_validate(model['LGBM_includingHS'] , dat[cols+hs_cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True))\n",
    "    \n",
    "    ######################################\n",
    "    ######################################\n",
    "    \n",
    "    for idx, _mod in enumerate(scores.keys()):\n",
    "        if idx==0:\n",
    "            df = scores[_mod].mean().transpose()\n",
    "        else:\n",
    "            df = pd.concat([df, scores[_mod].mean()], axis=1)\n",
    "    df.columns = list(scores.keys())\n",
    "    df.drop(['fit_time', 'score_time'], axis=0, inplace=True)\n",
    "    df = df.transpose()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.sort_values(by='test_npv', inplace=True)\n",
    "\n",
    "    col_map = {'LGBM_includingHS': 'set 3 plus HS and interfeats',\n",
    "               'XGB_includingHS': 'set 3 plus HS and interfeats',\n",
    "               'ensemble_vote_hs': 'HS',\n",
    "               'ensemble_vote_slope': 'TN slope',\n",
    "               'ensemble_vote_slope_ALL': 'TN slope + set 3/HS/interfeats',\n",
    "               np.nan : 'set 3'\n",
    "               }\n",
    "    df['ds'] = df['index'].map(col_map).fillna('set 3')\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=4, figsize=(28, 7))\n",
    "\n",
    "    # specificity\n",
    "    # precision\n",
    "\n",
    "    sns.barplot(data=df, x='index', y='test_auc', hue='ds', dodge=False, ax=ax[0])\n",
    "    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
    "    ax[0].axhline(df.loc[df['index']=='ensemble_vote_hs', 'test_auc'].values[0], color='green')\n",
    "\n",
    "    sns.barplot(data=df, x='index', y='test_balanced_accuracy', ax=ax[1], hue='ds', dodge=False)\n",
    "    ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "    ax[1].axhline(df.loc[df['index']=='ensemble_vote_hs', 'test_balanced_accuracy'].values[0], color='green')\n",
    "\n",
    "    sns.barplot(data=df, x='index', y='test_npv', ax=ax[2], hue='ds', dodge=False)\n",
    "    ax[2].set_xticklabels(ax[2].get_xticklabels(), rotation=90)\n",
    "    ax[2].axhline(df.loc[df['index']=='ensemble_vote_hs', 'test_npv'].values[0], color='green')\n",
    "\n",
    "    sns.barplot(data=df, x='index', y='test_rec_macro', ax=ax[3], hue='ds', dodge=False)\n",
    "    ax[3].set_xticklabels(ax[3].get_xticklabels(), rotation=90)\n",
    "    ax[3].axhline(df.loc[df['index']=='ensemble_vote_hs', 'test_rec_macro'].values[0], color='green')\n",
    "\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[1].set_xlabel('')\n",
    "    ax[2].set_xlabel('')\n",
    "    ax[3].set_xlabel('')\n",
    "    fig.suptitle(\"Quick model comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1., 1: 2.}\n",
    "X = dat[cols].values\n",
    "X = _transform(X, trans='quantile')[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.1, stratify=y.values)\n",
    "clf = fit_nn(X_train, y_train, network='dnn', verbose=0, batch_size=250, epochs=100, class_weight=class_weights)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Same transformer..Balanced accuracy: {}, F1 score: {}, ROC AUC: {}\".format(balanced_accuracy(y_test, np.round(y_pred[:,0])), \n",
    "                                                            fb_score(y_test, np.round(y_pred[:,0])),\n",
    "                                                            metrics.roc_auc_score(y_test, y_pred[:,0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for train transformer applied to test.\n",
    "class_weights = {0: 1., 1: 3.}\n",
    "X = dat[cols].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.1, stratify=y.values)\n",
    "X_train, _transformers = _transform(X_train, trans='quantile')\n",
    "X_test = X_test if _transformers is None else _transformers[1].transform(_transformers[0].transform(X_test))\n",
    "\n",
    "clf = fit_nn(X_train, y_train, network='dnn', verbose=0, batch_size=250, epochs=150, class_weight=class_weights)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"<<DNN>> Train transformer..Balanced accuracy: {}, F1 score: {}, ROC AUC: {}\".format(balanced_accuracy(y_test, np.round(y_pred[:,0])), \n",
    "                                                            fb_score(y_test, np.round(y_pred[:,0])),\n",
    "                                                            metrics.roc_auc_score(y_test, y_pred[:,0])))\n",
    "pd.DataFrame(y_pred[:, 0]).plot.hist(bins=10)\n",
    "plt.title('Pseudo-probas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for train transformer applied to test.\n",
    "cols= col_dict['ecg']+col_dict['celldyn']+inter_celldyn_cols\n",
    "class_weights = {0: 1., 1: 3.}\n",
    "X = dat[cols].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.1, stratify=y.values)\n",
    "X_train, _transformers = _transform(X_train, trans='quantile')\n",
    "X_test = X_test if _transformers is None else _transformers[1].transform(_transformers[0].transform(X_test))\n",
    "\n",
    "\n",
    "clf = fit_nn(X_train, y_train, network='cnn', verbose=0, batch_size=132, epochs=100, class_weight=class_weights, convs_cnn=[(128,3,2,1), \n",
    "                                                                                                                       (64,3,2,1), \n",
    "                                                                                                                       (32,3,1,1)])\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"<<CNN>> Train transformer..Balanced accuracy: {}, F1 score: {}, ROC AUC: {}\".format(balanced_accuracy(y_test, np.round(y_pred[:,0])), \n",
    "                                                            fb_score(y_test, np.round(y_pred[:,0])),\n",
    "                                                            metrics.roc_auc_score(y_test, y_pred[:,0])))\n",
    "pd.DataFrame(y_pred[:, 0]).plot.hist(bins=10)\n",
    "plt.title('Pseudo-probas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that both the dense and the convolutional networks perform reasonably well on this problem,\n",
    "again this can possibly be improved with parameter tuning but this is beyond the scope of this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extensive_feature_importances:\n",
    "    cols = col_dict['ecg']+col_dict['celldyn']+col_dict['pheno']+col_dict['history']+['tn_admission']+col_dict['hs']\n",
    "    xgb_mod = XGB(n_estimators=300, max_depth=2)\n",
    "    n_splits =10\n",
    "    XGB_RES = pd.DataFrame(cross_validate(xgb_mod, dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=n_splits, return_train_score=True, return_estimator=True))\n",
    "    importances = pd.DataFrame(data=XGB_RES.estimator[0].feature_importances_, index=cols, columns=['xgb_0'])\n",
    "\n",
    "    for i in range(1, n_splits):\n",
    "        importances['xgb_'+str(i)] = XGB_RES.estimator[i].feature_importances_\n",
    "\n",
    "    #########################\n",
    "    lgbm_mod = LGBM(n_estimators=300, max_depth=2)\n",
    "    LGBM_RES = pd.DataFrame(cross_validate(lgbm_mod, dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True, return_estimator=True))\n",
    "    for i in range(0, n_splits):\n",
    "        importances['lgbm_'+str(i)] = LGBM_RES.estimator[i].feature_importances_\n",
    "\n",
    "    #########################\n",
    "\n",
    "    et_mod = ExtraTreesClassifier(n_estimators=600, max_depth=2)\n",
    "    ET_RES = pd.DataFrame(cross_validate(et_mod, dat[cols].values, y.values, \n",
    "                                                          scoring=scoring, \n",
    "                                                          cv=10, return_train_score=True, return_estimator=True))\n",
    "    for i in range(0, n_splits):\n",
    "        importances['et_'+str(i)] = ET_RES.estimator[i].feature_importances_\n",
    "\n",
    "    #########################    \n",
    "    importances_scaled = pd.DataFrame(data=MinMaxScaler().fit_transform(importances), columns=importances.columns, index=importances.index)\n",
    "\n",
    "\n",
    "    xgb_cols = [_col for _col in importances.columns if 'xgb' in _col]\n",
    "    lgbm_cols = [_col for _col in importances.columns if 'lgbm' in _col]\n",
    "    et_cols = [_col for _col in importances.columns if 'et' in _col]\n",
    "\n",
    "    importances_scaled.reset_index(inplace=True)\n",
    "    importances_scaled['ds'] = importances_scaled['index'].map(group_dict)\n",
    "    importances_scaled.index = importances_scaled['index']\n",
    "\n",
    "    imp_agg = pd.DataFrame(data=importances_scaled[xgb_cols+['ds']].groupby('ds').sum().mean(axis=1), columns=['xgb'])\n",
    "    imp_agg['lgbm'] = importances_scaled[lgbm_cols+['ds']].groupby('ds').sum().mean(axis=1)\n",
    "    imp_agg['et'] = importances_scaled[et_cols+['ds']].groupby('ds').sum().mean(axis=1)\n",
    "\n",
    "    top_ = dict()\n",
    "    top_['lgbm'] = importances_scaled[lgbm_cols].mean(axis=1).sort_values(ascending=False)\n",
    "    top_['xgb']  = importances_scaled[xgb_cols].mean(axis=1).sort_values(ascending=False)\n",
    "    top_['et'] = importances_scaled[et_cols].mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    tp = ((pd.DataFrame(top_['lgbm'], columns=['lgbm'])).join(pd.DataFrame(top_['xgb'], columns=['xgb']), how='inner'))\\\n",
    "                                .join(pd.DataFrame(top_['et'], columns=['et']), how='inner')\n",
    "    tp['mean'] = tp.apply(np.mean, axis=1)\n",
    "    tp['group'] = tp.index.map(group_dict)\n",
    "    tp.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "\n",
    "    tp[:100].groupby('group')['mean'].agg(['sum', 'count']), tp[:20].groupby('group')['mean'].agg(['sum', 'count'])\n",
    "\n",
    "    topf=dict()\n",
    "    topf['lgbm'] = importances_scaled[lgbm_cols].mean(axis=1).sort_values(ascending=False)\n",
    "    topf['xgb']  = importances_scaled[xgb_cols].mean(axis=1).sort_values(ascending=False)\n",
    "    topf['et'] = importances_scaled[et_cols].mean(axis=1).sort_values(ascending=False)\n",
    "    tp = ((pd.DataFrame(topf['lgbm'], columns=['lgbm'])).join(pd.DataFrame(topf['xgb'], columns=['xgb']), how='inner')).join(pd.DataFrame(topf['et'], columns=['et']), how='inner')\n",
    "    tp['mean'] = tp.apply(np.mean, axis=1)\n",
    "    tp['group'] = tp.index.map(group_dict)\n",
    "    tp.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "\n",
    "    features = pd.DataFrame(set(get_var_cols(col_dict)), columns=['feature_name'])\n",
    "    features['group']=features['feature_name'].map(group_dict, na_action='ignore')\n",
    "    features.set_index('feature_name', inplace=True)\n",
    "    features = features.join(tp['mean'], how='inner')\n",
    "    features.sort_values(by='mean', ascending=False, inplace=True)\n",
    "    features.to_csv(\"/laupodteam/AIOS/Bram/data/HeartScore/features.csv\", index=False)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final scoring \n",
    "\n",
    "We have the following setup's\n",
    "* Celldyn \n",
    "* Celldyn, age, sex, BMI\n",
    "* Celldyn, age, sex, BMI, ECG\n",
    "* (Celldyn, age, sex, BMI, ECG, troponine) - (age, sex, BMI, ECG, troponine)\n",
    "* reference: Celldyn, age, sex, BMI, ECG, troponine, history\n",
    "* reference: Celldyn, age, sex, BMI, ECG, troponine, history + HS\n",
    "\n",
    "**Deliverable**: comparison between datasets in terms of statistical distance between classes/importance of features.\n",
    "\n",
    "For each setup we obtain the \n",
    "\n",
    "1. ROC curves\n",
    "2. threshold v. recall/precision/F1/true recall\n",
    "3. threshold v. FN/FP/recall/true recall\n",
    "\n",
    "Models: XGB, LGBM, RF, ET, nuSVC, GAM, DNN, CNN, LR, LDA, we combine the models using a soft voting classifier.\n",
    "\n",
    "Pre-processing: \n",
    "* feature combination within CELLDYN and the aggregated ECG features \n",
    "* feature selection using Anova, [mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) and Kolgomorov-Smirnov and the first Wasserstein distance.\n",
    "\n",
    "Follow-up:\n",
    "* identifying promising samples for a celldyn-based predictor, using an auxiliary model\n",
    "* iteratively add features using Hausdorff distance or some other set distance (perhaps more a mean distance based on the center of the Voronoi diagrams e.g. the K-means clusters)\n",
    "* true multicollinearity remover using non-overlapping cliques and a similarity threshold, replacing multi-collinear variables with a combined feature\n",
    "* identify cross-dataset features using ICA/PLS/CCA, e.g. meta-celldyn, meta-ecg features, ecg-celldyn features\n",
    "* try other dimension reduction techniques, e.g. [BPDR](https://towardsdatascience.com/bpdr-a-new-dimensionality-reduction-technique-f570eea3fc65)\n",
    "* try AutoML frameworks suchs as TPOT, AutoKeras, H2O and AutoSklearn.\n",
    "* apply HDBSCAN/DictionaryLearning for clustering, see .e.g. this [blog](https://towardsdatascience.com/how-to-cluster-in-high-dimensions-4ef693bacc6)\n",
    "* find optimal number of clusters, as done [here](https://towardsdatascience.com/spectral-graph-clustering-and-optimal-number-of-clusters-estimation-32704189afbe)\n",
    "* try a combination of dataset-specific models\n",
    "* create three target groups, NSTEMI, NSTEMI with no CV history, NSTEMI with CV history\n",
    "* performance confidence calibration \n",
    "* add CNN/DNN to ensemble\n",
    "* investigate effect of up-sampling positive targets/down-sampling negative targets\n",
    "* get more data to identify and categorize the clusters\n",
    "* use cross_validate and differential evolution (or some other blackbox optimiser) for hyperparameter optimisation\n",
    "* make NN wrapper more generic so you can play with the architecture\n",
    "* try supervised PCA for dimension reduction, and MDS/LLE\n",
    "* identify skewed features and normalise using percentile normalisation:\n",
    "```size = len(df.Helpful_Votes)-1\n",
    "helpful_percentile_linearization = df.Helpful_Votes.rank(method=’min’).apply(lambda x: (x-1)/size)\n",
    "helpful_percentile_linearization.describe()\n",
    "```\n",
    "* try autoencoder for dimension reduction\n",
    "* Apply Deep Feature Synthesis: https://docs.featuretools.com/en/stable/automated_feature_engineering/afe.html\n",
    "* dimension reduction per feature set\n",
    "* Use different CalibratedClassifier ```import ml_insights as mli; clfc = mli.SplineCalibratedClassifierCV(clf, cv=3)```\n",
    "* Perform performance optimisation per dataset, i.e. do not use one model parameterset for different datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_simple_featureset_comparison:\n",
    "    # use XGB with different datasets for a similar plot\n",
    "    gc.collect()\n",
    "    xgb_mod = XGB(n_estimators=250, n_jobs=8, max_depth=2)\n",
    "    ds_scores = []\n",
    "    ds_scores_tot = dict()\n",
    "    num_folds = 10\n",
    "    \n",
    "    for _dat in [dat, dat_unscaled]:\n",
    "        for dkey, fval in feature_set.items():\n",
    "            for ckey, cols in fval.items():\n",
    "                print(\"--- {} ---\".format(cols))\n",
    "                _cols = [__column for _key in cols for __column in col_dict[_key]]\n",
    "                print(\"Processing dataset: {}\".format(dkey+\"_\"+ckey))\n",
    "                _ds_scores = pd.DataFrame(cross_validate(xgb_mod, _dat[_cols].values, y.values, \n",
    "                                                                      scoring=scoring, \n",
    "                                                                      cv=num_folds, return_train_score=True, \n",
    "                                                      return_estimator=False))\n",
    "                _ds_scores['with_celldyn'] = 1 if ckey=='_withCelldyn' else 0\n",
    "                _ds_scores['featureset'] = dkey\n",
    "                ds_scores.append(_ds_scores)\n",
    "                \n",
    "        ds_scores_df = pd.concat(ds_scores)\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(30, 10))\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_auc', hue='with_celldyn', ax=ax[0,0])\n",
    "        ax[0,0].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_auc'].values.mean(), color='green')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_balanced_accuracy', ax=ax[0,1], hue='with_celldyn')\n",
    "        ax[0,1].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_balanced_accuracy'].values.mean(), color='green')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_npv', ax=ax[0,2], hue='with_celldyn')\n",
    "        ax[0,2].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_npv'].values.mean(), color='green')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_rec_macro', ax=ax[0,3], hue='with_celldyn')\n",
    "        ax[0,3].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_rec_macro'].values.mean(), color='green')\n",
    "\n",
    "        ax[0,0].set_xlabel('')\n",
    "        ax[0,1].set_xlabel('')\n",
    "        ax[0,2].set_xlabel('')\n",
    "        ax[0,3].set_xlabel('')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_prec_macro', hue='with_celldyn', ax=ax[1,0])\n",
    "        ax[1,0].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_prec_macro'].values.mean(), color='green')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_spec', ax=ax[1,1], hue='with_celldyn')\n",
    "        ax[1,1].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_spec'].values.mean(), color='green')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_mcc', ax=ax[1,2], hue='with_celldyn')\n",
    "        ax[1,2].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_mcc'].values.mean(), color='green')\n",
    "\n",
    "        sns.barplot(data=ds_scores_df, x='featureset', y='test_f1_macro', ax=ax[1,3], hue='with_celldyn')\n",
    "        ax[1,3].axhline(ds_scores_df.loc[ds_scores_df['featureset']=='4', 'test_f1_macro'].values.mean(), color='green')\n",
    "\n",
    "        ax[1,0].set_xlabel('')\n",
    "        ax[1,1].set_xlabel('')\n",
    "        ax[1,2].set_xlabel('')\n",
    "        ax[1,3].set_xlabel('')\n",
    "        dat_name = retrieve_name(_dat)[0]\n",
    "        fig.suptitle(\"Dataset {} comparison using XGB\".format(dat_name))\n",
    "        \n",
    "        ds_scores_tot[dat_name] = ds_scores_df\n",
    "        \n",
    "        ds_scores_df.to_csv(\"/laupodteam/AIOS/Bram/data/HeartScore/xgb_compare_scores_data_\"+dat_name+\".csv\", sep=\";\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_simple_featureset_comparison:\n",
    "    scores = ds_scores_tot['dat_unscaled']\n",
    "    d1 = scores[(scores.featureset.isin(['0', '1'])) & (scores.with_celldyn==0)].test_npv\n",
    "    d2 = scores[(scores.featureset.isin(['0', '1'])) & (scores.with_celldyn==1)].test_npv\n",
    "    d1.plot.kde(label=\"WO\")\n",
    "    d2.plot.kde(label=\"W\")\n",
    "    print(\"Sets 0 and 1\", ks2(d1,d2))\n",
    "\n",
    "    d1 = scores[(scores.featureset.isin(['2', '3', '4', '5', '6'])) & (scores.with_celldyn==0)].test_npv\n",
    "    d2 = scores[(scores.featureset.isin(['2', '3', '4', '5', '6'])) & (scores.with_celldyn==1)].test_npv\n",
    "    d1.plot.kde(label=\"WO\")\n",
    "    d2.plot.kde(label=\"W\")\n",
    "    plt.legend()\n",
    "    plt.title(\"NPV\")\n",
    "    print(\"Sets 2,3,4,5,6,7\", ks2(d1,d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "scaler= None # {'ecg': QuantileTransformer(n_quantiles=3 3, output_distribution='normal'), 'celldyn': QuantileTransformer(n_quantiles=33, output_distribution='normal')} # StandardScaler(), MinMaxScaler(), RobustScaler() or None\n",
    "dim_red = None # {'celldyn': UMAP(n_components=32), 'ecg': UMAP(n_components=64)} # {'ecg': PCA(n_components=32), 'celldyn': PCA(n_components=64)} # dict with dimension reduction per data group, or one dim red for all, or None, methods: PCA, NMF, UMAP\n",
    "\n",
    "calibrated = False\n",
    "pre_scaled = False\n",
    "make_plots = False\n",
    "upsample  = SMOTE # SMOTE, SVMSMOTE, ADASYN, BorderlineSMOTE\n",
    "n_splits=10\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=323)\n",
    "clfs = [('XGB1', XGB(n_estimators=300, max_depth=2, tree_method='approx', n_jobs=8)), # base_score=0.5, scale_pos_weight=1, tree_method(exact/approx/hist)\n",
    "        ('XGB2', XGB(n_estimators=300, max_depth=3, tree_method='hist', n_jobs=8)),\n",
    "        ('LGBM1', LGBM(n_estimators=300, max_depth=2, boosting='dart', n_jobs=8)),  # boosting(dart/gbdt/goss)\n",
    "        ('LGBM2', LGBM(n_estimators=300, max_depth=3, boosting='gbdt', n_jobs=8)),\n",
    "        ('CATB', CATB(n_estimators=300, max_depth=3, boosting_type='Ordered', thread_count=8, verbose=0)),\n",
    "        ('NGB', NGBClassifier(n_estimators=300, verbose=0, Dist=Bernoulli)),\n",
    "        ('HGB', HistGradientBoostingClassifier(max_iter=500, max_bins=200, max_depth=4))]\n",
    "'''\n",
    "        ('RF', RandomForestClassifier(n_estimators=400, max_depth=2, class_weight=class_weights, n_jobs=4)), \n",
    "        ('ET', ExtraTreesClassifier(n_estimators=500, max_depth=2, class_weight=class_weights, n_jobs=4)),\n",
    "''' \n",
    "\n",
    "model_string =\"_\".join([_clf[0] for _clf in clfs])\n",
    "model_weights  = [6, 4, 6, 4, 4, 4, 5]\n",
    "\n",
    "\n",
    "'''\n",
    "loop to build X_train, X_test, y_train, y_test in case we have dimension reduction and/or scaling\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "tdx = 0 \n",
    "np.seterr(all='warn')\n",
    "for dkey, fval in feature_set.items():\n",
    "    print(\"+\"*20, \"Processing feature set:{}\".format(dkey))\n",
    "    for ckey, cols in fval.items():\n",
    "        print(\"-\"*40, \"Processing {}\".format(ckey))        \n",
    "        _cols = [__column for _key in cols for __column in col_dict[_key]]\n",
    "        if pre_scaled:\n",
    "            X = dat[_cols].values\n",
    "        else:\n",
    "            X = dat_unscaled[_cols].values\n",
    "        \n",
    "        results = []\n",
    "        # we loop through the split generator, get predictions, place the scores in the dictionary and add the y_pred.\n",
    "        for idx, ind_tuple in enumerate(sss.split(X, y)):\n",
    "            #print(70*\"-\",\"Processing fold {} from {} folds\".format(idx+1, n_splits))\n",
    "            train_ind, test_ind = ind_tuple\n",
    "\n",
    "            X_train, X_test = X[train_ind], X[test_ind]\n",
    "            y_train, y_test = y[train_ind], y[test_ind]\n",
    "            \n",
    "            \n",
    "            if scaler is not None:\n",
    "                #print(\"Applying the scaler...\")\n",
    "                if isinstance(scaler, dict):        \n",
    "                    for _imp_key, _scaler in scaler.items():\n",
    "                        if (_scaler is not None) & (_imp_key in cols):\n",
    "                            __cols = col_dict[_imp_key]\n",
    "                            col_idcs = [dat_unscaled[_cols].columns.get_loc(c) for c in __cols if c in dat_unscaled[_cols].columns.tolist()]\n",
    "                            X_train[:, col_idcs] = _scaler.fit_transform(X_train[:, col_idcs])\n",
    "                            X_test[:, col_idcs] = _scaler.transform(X_test[:, col_idcs])\n",
    "                else:   \n",
    "                    if ('ecg' in cols) & ('celldyn' in cols):\n",
    "                        __cols = col_dict['ecg']+col_dict['celldyn']\n",
    "                        col_idcs = [dat_unscaled[_cols].columns.get_loc(c) for c in __cols if c in dat_unscaled[_cols].columns.tolist()]\n",
    "                        X_train[:, col_idcs ] = _scaler.fit_transform(X_train[:, col_idcs])\n",
    "                        X_test[:, col_idcs] = _scaler.transform(X_test[:, col_idcs])\n",
    "            \n",
    "            if upsample is not None:\n",
    "                #print(\"Upsampling the minority class...\")\n",
    "                if 'SMOTE' in upsample.__name__:\n",
    "                    sm = upsample(sampling_strategy='minority', random_state=321, k_neighbors=5)\n",
    "                elif upsample.__name__ == 'ADASYN':\n",
    "                    sm = upsample(sampling_strategy='minority', random_state=321)\n",
    "                X_train, y_train= sm.fit_sample(X_train, y_train)\n",
    "            \n",
    "            if dim_red is not None:\n",
    "                if isinstance(dim_red, str):\n",
    "                    if dim_red == 'PCA':\n",
    "                        reducer = PCA(n_components=dims)\n",
    "                        reducer.fit(X_train)\n",
    "                        X_train = reducer.transform(X_train)\n",
    "                        X_test = reducer.transform(X_test)\n",
    "                    elif dim_red == 'UMAP':\n",
    "                        reducer = UMAP(n_components=dims)\n",
    "                        reducer.fit(X_train, y_train)\n",
    "                        X_train = reducer.transform(X_train)\n",
    "                        X_test = reducer.transform(X_test)\n",
    "                    elif dim_red == 'KPCA':\n",
    "                        reducer = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "                        reducer.fit(X_train, y_train)\n",
    "                        X_train = reducer.transform(X_train)\n",
    "                        X_test = reducer.transform(X_test)\n",
    "                elif isinstance(dim_red, dict):\n",
    "                    # each key in the dict should be in the col_dict\n",
    "                    sup_idcs = []\n",
    "                    for k, reducer in dim_red.items():\n",
    "                        if k in cols:\n",
    "                            #print(\"Reducing dimensionality of {} data\".format(k))\n",
    "                            __cols = col_dict[k]\n",
    "                            col_idcs = [dat_unscaled[_cols].columns.get_loc(c) for c in __cols if c in dat_unscaled[_cols].columns.tolist()]\n",
    "                            sup_idcs.append(col_idcs)\n",
    "                            reducer.fit(X_train[:, col_idcs], y_train)                            \n",
    "                            X_train_red = reducer.transform(X_train[:, col_idcs])\n",
    "                            X_test_red = reducer.transform(X_test[:, col_idcs])   \n",
    "                            X_train = np.hstack((X_train, X_train_red))\n",
    "                            X_test = np.hstack((X_test, X_test_red))\n",
    "                    if len(sup_idcs)>0:\n",
    "                        sup_idcs = np.hstack(sup_idcs)\n",
    "                        X_train = np.delete(X_train, sup_idcs, axis=1)\n",
    "                        X_test = np.delete(X_test, sup_idcs, axis=1)\n",
    "                               \n",
    "                else:\n",
    "                    raise ValueError(\"dim_red should be a string or a dict..\")\n",
    "                \n",
    "            # train model\n",
    "            clf = VotingClassifier(clfs, voting='soft', weights=model_weights)\n",
    "            if calibrated:\n",
    "                clfc = CalibratedClassifierCV(clf, cv=3)\n",
    "                clfc.fit(X_train, y_train)\n",
    "                y_pred = clfc.predict_proba(X_test)\n",
    "            else:\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict_proba(X_test)\n",
    "            results.append({'ypred': y_pred[:,1], 'ytest': y_test, 'dkey': dkey, 'ckey': ckey, 'models': model_string, 'split': idx})\n",
    "            #print(70*\"+\",\"threshold 0.5\\t=\\tAUC: {}, NPV: {}, ACC: {}\".format(metrics.roc_auc_score(y_test, y_pred[:, 1]), npv(y_test, y_pred[:, 1]), balanced_accuracy(y_test, y_pred[:, 1])))\n",
    "            #print(70*\"+\",\"threshold 0.75\\t=\\tAUC: {}, NPV: {}, ACC: {}\".format(metrics.roc_auc_score(y_test, y_pred[:, 1]), npv(y_test, y_pred[:, 1], thresh=0.75), balanced_accuracy(y_test, y_pred[:, 1], thresh=0.75)))\n",
    "            \n",
    "        all_results += results\n",
    "        \n",
    "        \n",
    "        figax = plt.subplots(ncols=4, figsize=(30, 7)) if make_plots else None\n",
    "            \n",
    "        for idx, _res in enumerate(results):            \n",
    "            try:\n",
    "                metr, figax = get_accuracy_plots(_res['ytest'], _res['ypred'], figax=figax, make_plot=make_plots)\n",
    "                metr['IDX'] = idx\n",
    "                metr['model'] = model_string\n",
    "                metr['featureset'] = dkey\n",
    "                metr['with_celldyn'] = 1 if ckey=='_withCelldyn' else 0\n",
    "            except Exception as e:\n",
    "                print(\"The results must be pretty badddd :))): {}\".format(e))\n",
    "            accuracies = pd.concat([accuracies, metr]) if tdx>0 else metr\n",
    "            tdx += 1\n",
    "        if make_plots:\n",
    "            figax[0].suptitle(\"Featureset:\"+dkey+ckey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkey = '6'\n",
    "ytestW = []\n",
    "ypredW = []\n",
    "\n",
    "ytestWO = []\n",
    "ypredWO = [] \n",
    "for _res in all_results:\n",
    "    if (_res['dkey'] == dkey) & (_res['ckey']=='_withCelldyn'):\n",
    "        ytestW.append(_res['ytest'])\n",
    "        ypredW.append(_res['ypred'])\n",
    "    elif (_res['dkey'] == dkey) & (_res['ckey']=='_withoutCelldyn'):\n",
    "        ytestWO.append(_res['ytest'])\n",
    "        ypredWO.append(_res['ypred']) \n",
    "\n",
    "TestW = pd.concat(ytestW)\n",
    "PredW = np.hstack(ypredW)\n",
    "TestWO = pd.concat(ytestWO)\n",
    "PredWO = np.hstack(ypredWO)\n",
    "\n",
    "print(\"AUC W: {}, WO: {}\".format(metrics.roc_auc_score(TestW,PredW), metrics.roc_auc_score(TestWO, PredWO)))\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "roc_curveW = pd.DataFrame(metrics.roc_curve(TestW, PredW)[:2]).transpose()\n",
    "roc_curveW.columns = ['FPR', 'TPR']\n",
    "\n",
    "\n",
    "roc_curveWO = pd.DataFrame(metrics.roc_curve(TestWO, PredWO)[:2]).transpose()\n",
    "roc_curveWO.columns = ['FPR', 'TPR']\n",
    "\n",
    "sns.lineplot(data=roc_curveW, x='FPR', y='TPR', ax=ax, label='with celldyn', color=(0.8666666666666667, 0.5176470588235295, 0.3215686274509804), ci=None)\n",
    "sns.lineplot(data=roc_curveWO, x='FPR', y='TPR', ax=ax, label='without celldyn', color=(0.2980392156862745, 0.4470588235294118, 0.6901960784313725), ci=None)\n",
    "ax.plot(np.array([0,1]), np.array([0,1]), ls=\"--\", c=\"black\")\n",
    "ax.set_title(\"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"3\"\n",
    "withCelldyn = \"_withoutCelldyn\"\n",
    "plots = True\n",
    "cont_acc=False\n",
    "#del accuracies_tmp\n",
    "k = 0\n",
    "if plots:\n",
    "    figax = plt.subplots(ncols=4, figsize=(30, 7))\n",
    "for idx, rd in enumerate(all_results):\n",
    "    if (rd['dkey']==ds) & (rd['ckey']==withCelldyn):\n",
    "        try:            \n",
    "            metr, figax = get_accuracy_plots(rd['ytest'], rd['ypred'], figax=figax, make_plot=plots)\n",
    "            metr['IDX'] = idx\n",
    "            metr['model'] = rd['models']\n",
    "            metr['featureset'] = rd['dkey']\n",
    "            metr['with_celldyn'] = 1 if rd['ckey']=='_withCelldyn' else 0            \n",
    "        except Exception as e:\n",
    "            print(\"The results must be pretty badddd :))): {}\".format(e))\n",
    "        accuracies_tmp = pd.concat([accuracies_tmp, metr]) if (k>0) or (cont_acc==True) else metr\n",
    "        k += 1\n",
    "figax[0].suptitle(\"Featureset:\"+ds+\", with Celldyn\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(30, 7))\n",
    "sns.lineplot(data=accuracies_tmp, x='THRESHOLD', y=\"NPV\", ax=ax[0])\n",
    "sns.lineplot(data=accuracies_tmp, x='THRESHOLD', y=\"TRUE_REC\", ax=ax[1])\n",
    "sns.lineplot(data=accuracies_tmp, x='THRESHOLD', y=\"FPR\", ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies['SPEC'] = 1-accuracies['FPR']\n",
    "#accuracies['NPV'].replace(to_replace=0, value=np.nan, inplace=True)\n",
    "#accuracies['FPR'].replace(to_replace=0, value=np.nan, inplace=True)\n",
    "#accuracies['REC'].replace(to_replace=0, value=np.nan, inplace=True)\n",
    "#accuracies['TRUE_REC'].replace(to_replace=0, value=np.nan, inplace=True)\n",
    "\n",
    "out_df=prep_accs(accuracies)\n",
    "out_df.sort_values(by=[\"featureset\", \"threshold_minimum\"], inplace=True)\n",
    "out_df.drop(['THRESHOLD', 'IDX'], axis=1, inplace=True)\n",
    "out_df.to_csv(\"/laupodteam/AIOS/Bram/data/HeartScore/acc_ensemble1.csv\", sep=\";\", index=False)\n",
    "accuracies.to_csv(\"/laupodteam/AIOS/Bram/data/HeartScore/raw_accuracies_ensemble1.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(24,7))\n",
    "sns.barplot(data=out_df, x='threshold_minimum', y='NPV', hue='with_celldyn', ax=ax[0])\n",
    "sns.barplot(data=out_df, x='threshold_minimum', y='SPEC', hue='with_celldyn', ax=ax[1])\n",
    "ax[0].set_ylim(0.80,1.0)\n",
    "ax[1].set_ylim(0.80,1.0)\n",
    "ax[0].set_title(\"NPV for different thresholds\")\n",
    "ax[1].set_title(\"SPEC for different thresholds\")\n",
    "fig.suptitle(\"Over all featuresets, per threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(24,7))\n",
    "sns.barplot(data=out_df, x='featureset', y='NPV', hue='with_celldyn', ax=ax[0])\n",
    "sns.barplot(data=out_df, x='featureset', y='SPEC', hue='with_celldyn', ax=ax[1])\n",
    "ax[0].set_title(\"NPV for different sets\")\n",
    "ax[1].set_title(\"SPEC for different sets\")\n",
    "ax[0].set_ylim(0.7,1.0)\n",
    "ax[1].set_ylim(0.7,1.0)\n",
    "fig.suptitle(\"Over all thresholds, per feature set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setString='0'\n",
    "order_dict = {'NPV': 1, 'BAL_ACC': 2 ,'F1_SCORE' : 3, 'REC' : 4, 'TRUE_REC' : 5, 'FPR' : 6, 'AUC' : 7}# just to change the order so that NPV is first\n",
    "accuracies_melted = pd.melt(accuracies[(accuracies.THRESHOLD>0.5) & (accuracies.featureset.isin(setString.split(\",\")))],\n",
    "                            value_vars=['BAL_ACC' ,'F1_SCORE', 'NPV', 'REC', 'TRUE_REC', 'SPEC', 'AUC'], \n",
    "                            id_vars=['with_celldyn', 'IDX'], \n",
    "                            var_name='metric', \n",
    "                            value_name='score')\n",
    "accuracies_melted['order'] =accuracies_melted['metric'].map(order_dict)\n",
    "accuracies_melted.sort_values(by='order', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "sns.violinplot(data=accuracies_melted, x='metric', y='score', hue='with_celldyn', k_depth=\"tukey\", ax=ax) \n",
    "sns.stripplot(data=accuracies_melted, x='metric', y='score', hue='with_celldyn', alpha=0.5, ax=ax)\n",
    "#ax.set_xticklabels(order_dict.keys())\n",
    "\n",
    "plt.legend(title='CELLDYN', loc=4)\n",
    "plt.title(\"Performance metrics with/without CELLDYN, set: \"+setString)\n",
    "#ax.axhline(0.95, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, figsize=(36, 8))\n",
    "setString='1,2,3,4'\n",
    "accuracies[(accuracies.with_celldyn==0) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].NPV.plot.kde(label='without celldyn', ax=ax[0])\n",
    "accuracies[(accuracies.with_celldyn==1) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].NPV.plot.kde(label='with celldyn', ax=ax[0])\n",
    "accuracies[(accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].NPV.plot.hist(color='grey', label='', density=True, ax=ax[0])\n",
    "ax[0].legend()\n",
    "ax[0].set_title('NPV')\n",
    "\n",
    "accuracies[(accuracies.with_celldyn==0) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].REC.plot.kde(label='without celldyn', ax=ax[1])\n",
    "accuracies[(accuracies.with_celldyn==1) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].REC.plot.kde(label='with celldyn', ax=ax[1])\n",
    "accuracies[(accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].REC.plot.hist(color='grey', label='', density=True, ax=ax[1])\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Recall')\n",
    "\n",
    "accuracies[(accuracies.with_celldyn==0) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].F1_SCORE.plot.kde(label='without celldyn', ax=ax[2])\n",
    "accuracies[(accuracies.with_celldyn==1) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].F1_SCORE.plot.kde(label='with celldyn', ax=ax[2])\n",
    "accuracies[(accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].F1_SCORE.plot.hist(color='grey', label='', density=True, ax=ax[2])\n",
    "\n",
    "ax[2].legend()\n",
    "ax[2].set_title('F1')\n",
    "\n",
    "accuracies[(accuracies.with_celldyn==0) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].AUC.plot.kde(label='without celldyn', ax=ax[3])\n",
    "accuracies[(accuracies.with_celldyn==1) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].AUC.plot.kde(label='with celldyn', ax=ax[3])\n",
    "accuracies[(accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].AUC.plot.hist(color='grey', label='', density=True, ax=ax[3])\n",
    "ax[3].legend()\n",
    "ax[3].set_title('ROC-AUC')\n",
    "\n",
    "accuracies[(accuracies.with_celldyn==0) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].BAL_ACC.plot.kde(label='without celldyn', ax=ax[4])\n",
    "accuracies[(accuracies.with_celldyn==1) & (accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].BAL_ACC.plot.kde(label='with celldyn', ax=ax[4])\n",
    "accuracies[(accuracies.THRESHOLD>0.50) & (accuracies.featureset.isin(setString.split(\",\")))].BAL_ACC.plot.hist(color='grey', label='', density=True, ax=ax[4])\n",
    "ax[4].legend()\n",
    "ax[4].set_title('Balanced accuracy')\n",
    "\n",
    "fig.suptitle(\"Comparison of scores between featuresets, with/without celldyn, sets \"+setString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres=0.5\n",
    "csets = [\"1\", \"2\", \"3\", \"4\"]\n",
    "d1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].NPV\n",
    "d2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].NPV\n",
    "r1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "r2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "print(thres, d1.mean(), d2.mean(), r1.mean(), r2.mean(), ks2(d1,d2)[1])\n",
    "\n",
    "thres=0.6\n",
    "d1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].NPV\n",
    "d2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].NPV\n",
    "r1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "r2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "print(thres, d1.mean(), d2.mean(), r1.mean(), r2.mean(), ks2(d1,d2)[1])\n",
    "\n",
    "thres=0.7\n",
    "d1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].NPV\n",
    "d2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].NPV\n",
    "r1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "r2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "print(thres, d1.mean(), d2.mean(), r1.mean(), r2.mean(), ks2(d1,d2)[1])\n",
    "\n",
    "thres=0.8\n",
    "d1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].NPV\n",
    "d2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].NPV\n",
    "r1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "r2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "print(thres, d1.mean(), d2.mean(),  r1.mean(), r2.mean(), ks2(d1,d2)[1])\n",
    "\n",
    "thres=0.9\n",
    "d1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].NPV\n",
    "d2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].NPV\n",
    "r1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "r2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "print(thres, d1.mean(), d2.mean(),  r1.mean(), r2.mean(), ks2(d1,d2)[1])\n",
    "\n",
    "thres=0.95\n",
    "d1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].NPV\n",
    "d2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].NPV\n",
    "r1 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==0) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "r2 = accuracies[(accuracies.featureset.isin(csets)) & (accuracies.with_celldyn==1) & (accuracies.THRESHOLD>thres)].TRUE_REC\n",
    "print(thres, d1.mean(), d2.mean(),  r1.mean(), r2.mean(), ks2(d1,d2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1., 1: 2.}\n",
    "dim_red = None\n",
    "n_splits = 10\n",
    "pre_scaled = False\n",
    "upsample = SMOTE #, SVMSMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=323)\n",
    "clfs = [('dnn1', {'batch_size': 100, 'epochs' : 100, 'layers': [(128,0.05), (64,0.05), (48,0.05), (32,0.1)]}), \n",
    "        ('dnn2', {'batch_size': 100, 'epochs' : 150, 'layers': [(128,0.05), (96,0.05), (64,0.05), (32,0.1)]})]\n",
    "model_weights = [1, 2]\n",
    "transform = 'quantile'\n",
    "n_quantiles=250 \n",
    "distr='uniform'\n",
    "reducer='PCA'\n",
    "n_comp=128\n",
    "min_dim = 10\n",
    "\n",
    "tdx = 0 \n",
    "np.seterr(all='warn')\n",
    "for dkey, fval in feature_set.items():\n",
    "    print(\"+\"*20, \"Processing feature set:{}\".format(dkey))\n",
    "    for ckey, cols in fval.items():\n",
    "        print(\"-\"*40, \"Processing {}\".format(ckey))        \n",
    "        _cols = [__column for _key in cols for __column in col_dict[_key]]\n",
    "        if pre_scaled:\n",
    "            X = dat[_cols].values\n",
    "        else:\n",
    "            X = dat_unscaled[_cols].values\n",
    "        \n",
    "        results = []\n",
    "        # we loop through the split generator, get predictions, place the scores in the dictionary and add the y_pred.\n",
    "        for idx, ind_tuple in enumerate(sss.split(X, y)):\n",
    "            print(30*\"-\",\"Processing fold {} from {} folds\".format(idx+1, n_splits))\n",
    "\n",
    "            train_ind, test_ind = ind_tuple\n",
    "\n",
    "            # get train/test\n",
    "            X_train, X_test = X[train_ind], X[test_ind]\n",
    "            y_train, y_test = y[train_ind], y[test_ind]\n",
    "            _n_comp = n_comp if X_train.shape[1]>n_comp else X_train.shape[1]\n",
    "            X_train, _transformers = _transform(X_train, trans=transform, n_quantiles=n_quantiles, distr=distr, reducer=reducer, n_comp=_n_comp)\n",
    "            X_test = X_test if _transformers is None else _transformers[1].transform(_transformers[0].transform(X_test))\n",
    "\n",
    "            # up-sample positive targets/down-sample negative targets\n",
    "            if upsample is not None:\n",
    "                sm = upsample(sampling_strategy='minority', random_state=321, k_neighbors=5)\n",
    "                X_train, y_train= sm.fit_sample(X_train, y_train)\n",
    "\n",
    "            # train model\n",
    "            for idx, _mtup in enumerate(clfs):\n",
    "                if X_train.shape[1]>min_dim:\n",
    "                    is_nn_classifier = True\n",
    "                    clf = fit_nn(X_train, y_train, \n",
    "                                 network=_mtup[0], \n",
    "                                 verbose=0, \n",
    "                                 batch_size=_mtup[1]['batch_size'],\n",
    "                                 epochs=_mtup[1]['epochs'], \n",
    "                                 class_weight=class_weights,\n",
    "                                 convs_dnn=_mtup[1]['layers'])\n",
    "                else:\n",
    "                    print(\"Less then {} dimensions, we are using a simple logistic regression for this one\".format(min_dim))\n",
    "                    is_nn_classifier = False\n",
    "                    clf = LogisticRegression(max_iter=500) # LogisticRegression(max_iter=500)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                \n",
    "                if is_nn_classifier:                    \n",
    "                    _X_test = np.expand_dims(X_test, 2) if _mtup[0]=='cnn' else X_test\n",
    "                    _y_pred = model_weights[idx]*clf.predict(_X_test)[:,0]/sum(model_weights) if idx==0 else _y_pred+model_weights[idx]*clf.predict(_X_test)[:,0]/sum(model_weights)\n",
    "                else:\n",
    "                    _y_pred = clf.predict_proba(X_test)[:,1] \n",
    "            if is_nn_classifier:\n",
    "                model_string =\"_\".join([_clf[0] for _clf in clfs])\n",
    "            else:\n",
    "                model_string = str(clf.__class__)\n",
    "            \n",
    "            # get/store accuracy and store y_test/y_pred for ROC curves\n",
    "            results.append({'ypred': _y_pred, 'ytest': y_test, 'dkey': dkey, 'ckey': ckey, 'models': model_string, 'split': idx})\n",
    "            #print(70*\"+\",\"threshold 0.5\\t=\\tAUC: {}, NPV: {}, ACC: {}\".format(metrics.roc_auc_score(y_test, y_pred[:, 1]), npv(y_test, y_pred[:, 1]), balanced_accuracy(y_test, y_pred[:, 1])))\n",
    "            #print(70*\"+\",\"threshold 0.75\\t=\\tAUC: {}, NPV: {}, ACC: {}\".format(metrics.roc_auc_score(y_test, y_pred[:, 1]), npv(y_test, y_pred[:, 1], thresh=0.75), balanced_accuracy(y_test, y_pred[:, 1], thresh=0.75)))\n",
    "                      \n",
    "        all_results += results\n",
    "        \n",
    "        figax = plt.subplots(ncols=4, figsize=(30, 7))\n",
    "        for idx, _res in enumerate(results):            \n",
    "            try:\n",
    "                metr, figax = get_accuracy_plots(_res['ytest'], _res['ypred'], make_plots=False, figax=figax)\n",
    "                metr['IDX'] = idx\n",
    "                metr['model'] = \"_\".join([_clf[0] for _clf in clfs])\n",
    "                metr['featureset'] = dkey\n",
    "                metr['with_celldyn'] = 1 if ckey=='_withCelldyn' else 0\n",
    "            except Exception as e:\n",
    "                print(\"The results must be pretty badddd :))): {}\".format(e))\n",
    "            accuracies_nn = pd.concat([accuracies_nn, metr]) if tdx>0 else metr\n",
    "            tdx += 1\n",
    "        figax[0].suptitle(\"Featureset:\"+dkey+ckey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setString='1'\n",
    "accuracies_melted = pd.melt(accuracies_nn[(accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))],\n",
    "                            value_vars=['BAL_ACC' ,'F1_SCORE', 'NPV', 'REC', 'TRUE_REC', 'FPR', 'AUC'], \n",
    "                            id_vars=['with_celldyn', 'IDX'], \n",
    "                            var_name='metric', \n",
    "                            value_name='score')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "sns.violinplot(data=accuracies_melted, x='metric', y='score', hue='with_celldyn', k_depth=\"tukey\", ax=ax)\n",
    "sns.stripplot(data=accuracies_melted, x='metric', y='score', hue='with_celldyn', alpha=0.5, ax=ax)\n",
    "\n",
    "plt.legend(title='CELLDYN', loc=4)\n",
    "plt.title(\"Performance metrics with/without CELLDYN, set: \"+setString)\n",
    "ax.axhline(0.95, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, figsize=(36, 8))\n",
    "setString='2,3,4,5'\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==0) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].NPV.plot.kde(label='without celldyn', ax=ax[0])\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==1) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].NPV.plot.kde(label='with celldyn', ax=ax[0])\n",
    "accuracies_nn[(accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].NPV.plot.hist(color='grey', label='', density=True, ax=ax[0])\n",
    "ax[0].legend()\n",
    "ax[0].set_title('NPV')\n",
    "\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==0) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].REC.plot.kde(label='without celldyn', ax=ax[1])\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==1) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].REC.plot.kde(label='with celldyn', ax=ax[1])\n",
    "accuracies_nn[(accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].REC.plot.hist(color='grey', label='', density=True, ax=ax[1])\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Recall')\n",
    "\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==0) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].F1_SCORE.plot.kde(label='without celldyn', ax=ax[2])\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==1) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].F1_SCORE.plot.kde(label='with celldyn', ax=ax[2])\n",
    "accuracies_nn[(accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].F1_SCORE.plot.hist(color='grey', label='', density=True, ax=ax[2])\n",
    "\n",
    "ax[2].legend()\n",
    "ax[2].set_title('F1')\n",
    "\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==0) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].AUC.plot.kde(label='without celldyn', ax=ax[3])\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==1) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].AUC.plot.kde(label='with celldyn', ax=ax[3])\n",
    "accuracies_nn[(accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].AUC.plot.hist(color='grey', label='', density=True, ax=ax[3])\n",
    "ax[3].legend()\n",
    "ax[3].set_title('ROC-AUC')\n",
    "\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==0) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].BAL_ACC.plot.kde(label='without celldyn', ax=ax[4])\n",
    "accuracies_nn[(accuracies_nn.with_celldyn==1) & (accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].BAL_ACC.plot.kde(label='with celldyn', ax=ax[4])\n",
    "accuracies_nn[(accuracies_nn.THRESHOLD>0.50) & (accuracies_nn.featureset.isin(setString.split(\",\")))].BAL_ACC.plot.hist(color='grey', label='', density=True, ax=ax[4])\n",
    "ax[4].legend()\n",
    "ax[4].set_title('Balanced accuracy')\n",
    "\n",
    "fig.suptitle(\"Comparison of scores between featuresets, with/without celldyn, sets \"+setString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat distance \n",
    "d1 = accuracies_nn[accuracies_nn.with_celldyn==0].NPV\n",
    "d2 = accuracies_nn[accuracies_nn.with_celldyn==1].NPV\n",
    "print(\"Means: with CELLDYN {}, without CELLDYN {}\".format(d1.mean(), d2.mean()))\n",
    "ks2(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to disk\n",
    "allres = pd.DataFrame(all_results)\n",
    "allres_f = \"allres_2020-01-29\"\n",
    "allres.to_hdf(\"/laupodteam/AIOS/Bram/data/HeartScore/Results/\"+allres_f+\".hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross_validate and differential evolution for hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform calibration\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoKeras\n",
    "# \n",
    "\n",
    "# AutoGluon\n",
    "# https://autogluon.mxnet.io/tutorials/tabular_prediction/tabular-quickstart.html\n",
    "\n",
    "# Auto-sklearn\n",
    "\n",
    "# TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimisation using differential evolution\n",
    "Using N-fold cross-validation: minimize **variance**, maximize **average precision**.\n",
    "Use a blackbox optimiser like hyperopt or differential evolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from scipy.optimize import differential_evolution as de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_npv                  0.845995\n",
       "test_balanced_accuracy    0.758683\n",
       "test_auc                  0.854668\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = ['pheno', 'ecg', 'history', 'tn_admission', 'celldyn', 'inter_ecg', 'inter_celldyn']\n",
    "_cols = [__column for _key in col_list for __column in col_dict[_key]]\n",
    "X = dat_unscaled[_cols]\n",
    "\n",
    "\n",
    "clfs = [('XGB1', XGB(n_estimators=300, max_depth=2, subsample=0.5, eta=0.3, tree_method='approx', n_jobs=8)), # base_score=0.5, scale_pos_weight=1, tree_method(exact/approx/hist)\n",
    "        ('XGB2', XGB(n_estimators=300, max_bin=100, subsample=0.5, eta=0.3, max_depth=3, tree_method='hist', n_jobs=8)),\n",
    "        ('LGBM1', LGBM(n_estimators=300, max_depth=2, subsample=0.5, boosting='dart', n_jobs=8)),  # boosting(dart/gbdt/goss)\n",
    "        ('LGBM2', LGBM(n_estimators=300, max_depth=3, subsample=0.5, boosting='gbdt', n_jobs=8)),\n",
    "        ('CATB', CATB(n_estimators=300, max_depth=3, boosting_type='Ordered', thread_count=8, verbose=0)),\n",
    "        ('NGB', NGBClassifier(n_estimators=300, verbose=0, Dist=Bernoulli)),\n",
    "        ('HGB', HistGradientBoostingClassifier(max_iter=500, max_bins=100, max_depth=4))]\n",
    "\n",
    "clf_weights = [1,1,1,1,1,1,1]\n",
    "\n",
    "# Test\n",
    "anova_filter = SelectKBest(f_regression, k=128)\n",
    "reducer = PCA(n_components=32)\n",
    "clf = VotingClassifier(clfs, voting='soft', weights=clf_weights)\n",
    "pipe = Pipeline(steps=[('anova', anova_filter), ('reducer', reducer), ('svc', clf)])\n",
    "met = pd.DataFrame(cross_validate(pipe, X.values, y.values, scoring=scoring, cv=5, return_train_score=True))\n",
    "met\n",
    "met[['test_npv', 'test_balanced_accuracy', 'test_auc']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.847154\n",
      "test_balanced_accuracy    0.760522\n",
      "test_auc                  0.837867\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.807098\n",
      "test_balanced_accuracy    0.688889\n",
      "test_auc                  0.816383\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.817297\n",
      "test_balanced_accuracy    0.702087\n",
      "test_auc                  0.797754\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.829278\n",
      "test_balanced_accuracy    0.733208\n",
      "test_auc                  0.828856\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.826715\n",
      "test_balanced_accuracy    0.726890\n",
      "test_auc                  0.836330\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.813708\n",
      "test_balanced_accuracy    0.700324\n",
      "test_auc                  0.804868\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.795985\n",
      "test_balanced_accuracy    0.666250\n",
      "test_auc                  0.779277\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.819312\n",
      "test_balanced_accuracy    0.710774\n",
      "test_auc                  0.820142\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.786585\n",
      "test_balanced_accuracy    0.646822\n",
      "test_auc                  0.778567\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.809680\n",
      "test_balanced_accuracy    0.692624\n",
      "test_auc                  0.806536\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.790816\n",
      "test_balanced_accuracy    0.657326\n",
      "test_auc                  0.775949\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.828644\n",
      "test_balanced_accuracy    0.732095\n",
      "test_auc                  0.843756\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.806971\n",
      "test_balanced_accuracy    0.691556\n",
      "test_auc                  0.814794\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.821294\n",
      "test_balanced_accuracy    0.717036\n",
      "test_auc                  0.802909\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.816214\n",
      "test_balanced_accuracy    0.703711\n",
      "test_auc                  0.819308\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.808799\n",
      "test_balanced_accuracy    0.693801\n",
      "test_auc                  0.817133\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.809120\n",
      "test_balanced_accuracy    0.691737\n",
      "test_auc                  0.812488\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.790320\n",
      "test_balanced_accuracy    0.654544\n",
      "test_auc                  0.786748\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.834238\n",
      "test_balanced_accuracy    0.738138\n",
      "test_auc                  0.832818\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.789573\n",
      "test_balanced_accuracy    0.656542\n",
      "test_auc                  0.779834\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.815376\n",
      "test_balanced_accuracy    0.707475\n",
      "test_auc                  0.825868\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.803936\n",
      "test_balanced_accuracy    0.686605\n",
      "test_auc                  0.817492\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.823218\n",
      "test_balanced_accuracy    0.720805\n",
      "test_auc                  0.825247\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.794195\n",
      "test_balanced_accuracy    0.661939\n",
      "test_auc                  0.782328\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.787806\n",
      "test_balanced_accuracy    0.652063\n",
      "test_auc                  0.775542\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.793995\n",
      "test_balanced_accuracy    0.662587\n",
      "test_auc                  0.805273\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.809940\n",
      "test_balanced_accuracy    0.695065\n",
      "test_auc                  0.810302\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.841216\n",
      "test_balanced_accuracy    0.748357\n",
      "test_auc                  0.826631\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.810473\n",
      "test_balanced_accuracy    0.695399\n",
      "test_auc                  0.799579\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.789424\n",
      "test_balanced_accuracy    0.654908\n",
      "test_auc                  0.782532\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.791142\n",
      "test_balanced_accuracy    0.657322\n",
      "test_auc                  0.768929\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.821147\n",
      "test_balanced_accuracy    0.713114\n",
      "test_auc                  0.815976\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.810756\n",
      "test_balanced_accuracy    0.696152\n",
      "test_auc                  0.813614\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.795657\n",
      "test_balanced_accuracy    0.664133\n",
      "test_auc                  0.772509\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.789427\n",
      "test_balanced_accuracy    0.653123\n",
      "test_auc                  0.777836\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.845893\n",
      "test_balanced_accuracy    0.755072\n",
      "test_auc                  0.828576\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.809362\n",
      "test_balanced_accuracy    0.693301\n",
      "test_auc                  0.826159\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.807030\n",
      "test_balanced_accuracy    0.686991\n",
      "test_auc                  0.781014\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.811199\n",
      "test_balanced_accuracy    0.697468\n",
      "test_auc                  0.795447\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.819726\n",
      "test_balanced_accuracy    0.714464\n",
      "test_auc                  0.827032\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.829256\n",
      "test_balanced_accuracy    0.731228\n",
      "test_auc                  0.814239\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.814035\n",
      "test_balanced_accuracy    0.700524\n",
      "test_auc                  0.828487\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.808670\n",
      "test_balanced_accuracy    0.693613\n",
      "test_auc                  0.794626\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.806945\n",
      "test_balanced_accuracy    0.690883\n",
      "test_auc                  0.815898\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.797588\n",
      "test_balanced_accuracy    0.671085\n",
      "test_auc                  0.787446\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.809989\n",
      "test_balanced_accuracy    0.691670\n",
      "test_auc                  0.789922\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.809771\n",
      "test_balanced_accuracy    0.695430\n",
      "test_auc                  0.811146\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.817708\n",
      "test_balanced_accuracy    0.707938\n",
      "test_auc                  0.808306\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.793362\n",
      "test_balanced_accuracy    0.662246\n",
      "test_auc                  0.781139\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.805002\n",
      "test_balanced_accuracy    0.686377\n",
      "test_auc                  0.816091\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.793825\n",
      "test_balanced_accuracy    0.666722\n",
      "test_auc                  0.796132\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.784381\n",
      "test_balanced_accuracy    0.645939\n",
      "test_auc                  0.770686\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.792555\n",
      "test_balanced_accuracy    0.659463\n",
      "test_auc                  0.782676\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.823621\n",
      "test_balanced_accuracy    0.715809\n",
      "test_auc                  0.825057\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.818834\n",
      "test_balanced_accuracy    0.710377\n",
      "test_auc                  0.825078\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.842559\n",
      "test_balanced_accuracy    0.754480\n",
      "test_auc                  0.842927\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.830429\n",
      "test_balanced_accuracy    0.730332\n",
      "test_auc                  0.828940\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++ test_npv                  0.811343\n",
      "test_balanced_accuracy    0.693353\n",
      "test_auc                  0.800821\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n",
      "H:\\PythonVENV\\Envs\\base\\lib\\site-packages\\ipykernel_launcher.py:295: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-f817e01a9bdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m optres = de(classfun, bounds=[n_est_range, depth_range, iter_range, bins_range, \n\u001b[0;32m     51\u001b[0m                               subsample_range, eta_range, kbest_range, component_range],\n\u001b[1;32m---> 52\u001b[1;33m                maxiter=10, popsize=250, mutation=(0.3,1.5), recombination=0.3,disp=True)\n\u001b[0m",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py\u001b[0m in \u001b[0;36mdifferential_evolution\u001b[1;34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, seed, callback, disp, polish, init, atol, updating, workers, constraints)\u001b[0m\n\u001b[0;32m    304\u001b[0m                                      \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                                      constraints=constraints) as solver:\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py\u001b[0m in \u001b[0;36msolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m             self.population_energies[self.feasible] = (\n\u001b[0;32m    744\u001b[0m                 self._calculate_population_energies(\n\u001b[1;32m--> 745\u001b[1;33m                     self.population[self.feasible]))\n\u001b[0m\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_promote_lowest_energy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py\u001b[0m in \u001b[0;36m_calculate_population_energies\u001b[1;34m(self, population)\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             calc_energies = list(self._mapwrapper(self.func,\n\u001b[1;32m--> 878\u001b[1;33m                                                   parameters_pop[0:nfevs]))\n\u001b[0m\u001b[0;32m    879\u001b[0m             \u001b[0menergies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnfevs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_energies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\scipy\\optimize\\_differentialevolution.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-f817e01a9bdb>\u001b[0m in \u001b[0;36mclassfun\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'anova'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manova_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'reducer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"+\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_npv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_balanced_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 236\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n\u001b[0;32m     67\u001b[0m                                                  sample_weight=sample_weight)\n\u001b[1;32m---> 68\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'drop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             )\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_parallel_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    317\u001b[0m                     \u001b[0ml2_regularization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_regularization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     shrinkage=self.learning_rate)\n\u001b[1;32m--> 319\u001b[1;33m                 \u001b[0mgrower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[0macc_apply_split_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_apply_split_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36mgrow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;34m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplittable_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_intilialize_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians_are_constant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36msplit_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_split_left\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_best_split_and_push\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_child_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_split_right\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_best_split_and_push\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_child_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\PythonVENV\\Envs\\base\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36m_compute_best_split_and_push\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    294\u001b[0m         node.split_info = self.splitter.find_node_split(\n\u001b[0;32m    295\u001b[0m             \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistograms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             node.sum_hessians)\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgain\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# no valid split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wrapping it in a function\n",
    "# params (in order)\n",
    "n_est_range = (10,600)  \n",
    "depth_range = (2,5)\n",
    "iter_range = (100,500)\n",
    "bins_range = (10,400)\n",
    "subsample_range = (0.1,1)\n",
    "eta_range = (0.1,1)\n",
    "kbest_range = (128, len(_cols))\n",
    "component_range = (16, 128)\n",
    "\n",
    "gc.collect()\n",
    "def classfun(params):\n",
    "    '''\n",
    "    params[0] = n_estimators\n",
    "    params[1] = max_depth\n",
    "    params[2] = max_iter\n",
    "    params[3] = max_bins \n",
    "    params[4] = subsample\n",
    "    params[5] = eta\n",
    "    params[6] = kbest\n",
    "    params[7] = n_components\n",
    "    '''\n",
    "    \n",
    "    clfs = [('XGB1', XGB(n_estimators=int(params[0]), max_depth=int(params[1]), subsample=params[4], eta=params[5], tree_method='approx', n_jobs=8)), # base_score=0.5, scale_pos_weight=1, tree_method(exact/approx/hist)\n",
    "            ('XGB2', XGB(n_estimators=int(params[0]), max_bin=int(params[3]), subsample=params[4], eta=params[5], max_depth=int(params[1]), tree_method='hist', n_jobs=8)),\n",
    "            ('LGBM1', LGBM(n_estimators=int(params[0]), max_depth=int(params[1]), subsample=params[4], boosting='dart', n_jobs=8)),  # boosting(dart/gbdt/goss)\n",
    "            ('LGBM2', LGBM(n_estimators=int(params[0]), max_depth=int(params[1]), subsample=params[4], boosting='gbdt', n_jobs=8)),\n",
    "            ('CATB', CATB(n_estimators=int(params[0]), max_depth=int(params[1]), boosting_type='Ordered', thread_count=8, verbose=0)),\n",
    "            ('NGB', NGBClassifier(n_estimators=int(params[0]), verbose=0, Dist=Bernoulli)),\n",
    "            ('HGB', HistGradientBoostingClassifier(max_iter=int(params[2]), max_bins=np.min([255, int(params[3])]),\n",
    "                                                   max_depth=int(params[1])))]\n",
    "\n",
    "    clf_weights = [1,1,1,1,1,1,1]\n",
    "\n",
    "    # Test\n",
    "    anova_filter = SelectKBest(f_regression, k=int(params[6]))\n",
    "    reducer = PCA(n_components=int(params[7]))\n",
    "    clf = VotingClassifier(clfs, voting='soft', weights=clf_weights)\n",
    "    pipe = Pipeline(steps=[('anova', anova_filter), ('reducer', reducer), ('svc', clf)])\n",
    "        \n",
    "    met = pd.DataFrame(cross_validate(pipe, X.values, y.values, scoring=scoring, cv=5, return_train_score=True))        \n",
    "    print(\"+\"*30, met[['test_npv', 'test_balanced_accuracy', 'test_auc']].mean())\n",
    "     \n",
    "    ba = met.test_balanced_accuracy.mean()\n",
    "    npv = met.test_npv.mean()\n",
    "    vnpv = met.test_npv.std()\n",
    "    return (1-npv)\n",
    "    \n",
    "optres = de(classfun, bounds=[n_est_range, depth_range, iter_range, bins_range, \n",
    "                              subsample_range, eta_range, kbest_range, component_range],\n",
    "               maxiter=10, popsize=50, mutation=(0.3,1.5), recombination=0.3,disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of features using permutations/SHAP & LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "# see https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature combinations using RuleFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = col_dict['pheno']+col_dict['ecg']+col_dict['celldyn']+col_dict['troponine']+col_dict['hs']+col_dict['history']+inter_celldyn_cols+inter_ecg_cols\n",
    "X = dat[cols].values\n",
    "gb = GradientBoostingClassifier(n_estimators=300)  #XGBRegressor(n_estimators=10, objective ='reg:squarederror', max_depth=3) # XGBRegressor\n",
    "clf = RuleFit(tree_generator=gb, rfmode='class')\n",
    "clf.fit(X, y.values, feature_names=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and threshold-recall/precision dependencies\n",
    "\n",
    "Unnecessary denied NSTEMIs **v.** unnecessary signaled NSTEMIs **v.** total recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy clustering: identifying promising samples for celldyn predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_unscaled.tn_slope2.plot.kde(figsize=(14,10))\n",
    "dat_unscaled[['tn_peak', 'tn_peak24', 'tn_admission']].apply(lambda x: x[1]-x[0], axis=1).plot.kde() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_unscaled[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_unscaled[dat_unscaled.tn_slope2>10][['tn_slope2', 'tn_admission', 'tn_peak', 'tn_peak24']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised clustering assisted supervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised model division for a model per cluster approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also see this: https://towardsdatascience.com/cluster-then-predict-for-classification-tasks-142fdfdc87d6\n",
    "# also relates in some way to : https://arxiv.org/abs/2002.05709\n",
    "\n",
    "# use k-means\n",
    "# use WSS/Silhouette and the cross-entropy per cluster as metrics to find the optimum number of clusters\n",
    "\n",
    "\n",
    "# use density-based clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised clustering and inclusion of cluster labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
