{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "from scipy.stats import ks_2samp as ks2\n",
    "from scipy.stats import mannwhitneyu as mwu\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import wasserstein_distance as w1_dist\n",
    "from scipy.stats import energy_distance as w2_dist\n",
    "from scipy.stats import epps_singleton_2samp as epps\n",
    "from category_encoders import *\n",
    "\n",
    "import SimpSOM as sps\n",
    "import minisom as msom\n",
    "#import sompy\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM, NuSVC\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, BayesianRidge, LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cross_decomposition import PLSRegression as PLS\n",
    "from ngboost import NGBRegressor\n",
    "\n",
    "from sklearn.cluster import SpectralClustering, AffinityPropagation, OPTICS, AgglomerativeClustering\n",
    "#from hdbscan import HDBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "def get_var_cols(d):\n",
    "    assert isinstance(d, dict), 'input variables is not a dictionary'\n",
    "    return list(itertools.chain.from_iterable(d.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploration\n",
    "->\n",
    "dataprep\n",
    "->\n",
    "clustering\n",
    "->\n",
    "classification\n",
    "\n",
    "\n",
    "* feature combination\n",
    "* data imputance\n",
    "* dimension reduction and outlier removal\n",
    "* re-balancing using SMOTE\n",
    "* add F1-score minimizer, see [this](https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d) blog\n",
    "\n",
    "Model\n",
    "* dummy classifiers\n",
    "* ensemble of classifiers\n",
    "* hyperoptimisation\n",
    "\n",
    "Models:\n",
    "* ensemble trees: XGB, LGBM\n",
    "* probabilistic: NB, GPC, SGD\n",
    "* linear: GAM, LR, Huber, Ridge, Lasso, LDA\n",
    "* kernel: nuSVM\n",
    "* neural: MLP\n",
    "* other: quadratic classifier, [learning vector quantization 1](https://machinelearningmastery.com/implement-learning-vector-quantization-scratch-python/), [learning vector quantization 2](http://neupy.com/docs/tutorials.html)\n",
    "\n",
    "Compare removing outliers from test to increasing threshold.\n",
    "\n",
    "# Deliverable:\n",
    "* ROC, F1, precision/recall curves\n",
    "* relative feature importances\n",
    "* methodology\n",
    "\n",
    "# Plan de campagne:\n",
    "* produce parameterized model notebook\n",
    "* write into compact functions in .py and search through parameter space for best model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "scaler= {'ecg': MinMaxScaler(), 'celldyn': MinMaxScaler(), 'other': MinMaxScaler()} # StandardScaler(), MinMaxScaler(), RobustScaler() or None\n",
    "dim_reduction = None # {'ecg': PCA(n_components=6), 'celldyn': PCA(n_components=20)} # dict with dimension reduction per data group, or one dim red for all, or None, methods: PCA, NMF, UMAP\n",
    "# dict with column name and impute type: median, mean, remove, regressor, (nmf?), or None, or knnimputer, or iterative which uses a round-robin approach using BayesianRidge as the regressor\n",
    "feature_expansion = ['celldyn']\n",
    "\n",
    "feature_weights = 'glm' # glm, tree, gam\n",
    "clustering = 'hdbscan' # hdbscan, SOM, spectral\n",
    "remove_nan_patients = False\n",
    "# NGBRegressor(), BayesianRidge(), MLPRegressor(hidden_layer_sizes=(70,70,30))\n",
    "imputance = {'BMI': BayesianRidge(), \n",
    "             'P_RInterval_ECG': BayesianRidge(), \n",
    "             'POnset_ECG': BayesianRidge(), \n",
    "             'PAxis_ECG': BayesianRidge(), \n",
    "             'POffset_ECG': BayesianRidge()}\n",
    "missing_dummy = True\n",
    "variance_remove = True\n",
    "remove_multicoll = False\n",
    "remove_outlying_samples_from_train = True\n",
    "remove_outlying_samples_from_test = False\n",
    "remove_weak_univariates = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"T:\\laupodteam\\AIOS\\Bram\")\n",
    "HS = pd.read_csv(\"data/HeartScore/Data/MATRIX_FULL_23jul2019_ECG.csv\", sep=\";\")\n",
    "index_cols = ['pathos_key', 'upod_id'] \n",
    "date_cols = ['AcquisitionDateTime_ECG'] \n",
    "meta_cols = ['setsrc', 'Analyzer']\n",
    "pheno_cols = ['AGE', 'gender', 'BMI', 'RF_Diab', 'RF_Smok', 'RF_HyperTens', 'RF_HyperChol', 'RF_CVDHist', 'RF_FamHist', 'RF_obese30']\n",
    "ign_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_RiskFacts', 'HS_new2', 'tn_slope2', 'HN_TN'] # remove\n",
    "tn_cols = ['tn_admission'] # moreve tn_slope2 and HN_TN\n",
    "\n",
    "rem_cols = ['Door']+['delay_Celldyn', 'HS_new']+date_cols+meta_cols\n",
    "HS.drop(rem_cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# convert bool in int64\n",
    "for _col in HS.columns.tolist():\n",
    "    if str(HS[_col].dtype)=='bool':\n",
    "        HS[_col] = HS[_col].astype(int)\n",
    "        \n",
    "target = 'casecontrol'\n",
    "HS.rename(index=str, columns={target: 'target'}, inplace=True)\n",
    "tmap = {'Control': 0, 'NSTEMI': 1}\n",
    "HS['target'] = HS.target.map(tmap)\n",
    "#HS.drop(target, axis=1, inplace=True)\n",
    "\n",
    "gmap = {'M': 0, 'F': 1}\n",
    "HS['gender'] = HS.gender.map(gmap)\n",
    "\n",
    "HS.set_index(index_cols, inplace=True)\n",
    "\n",
    "cols = HS.columns.tolist()\n",
    "var_cols = list(set(cols) - set(meta_cols) - set(index_cols) -set(date_cols) - set(['target']) - set(ign_cols))\n",
    "\n",
    "cell_dyn_cols = [\"c_b_wbc\",\"c_b_wvf\",\"c_b_neu\",\"c_b_seg\",\"c_b_bnd\",\"c_b_ig\",\"c_b_lym\",\"c_b_lyme\",\"c_b_vlym\",\"c_b_mon\",\"c_b_mone\",\"c_b_blst\",\n",
    "                 \"c_b_eos\",\"c_b_bas\",\"c_b_pneu\",\"c_b_pseg\",\"c_b_pbnd\",\"c_b_pig\",\"c_b_plym\",\"c_b_plyme\",\"c_b_pvlym\",\"c_b_pmon\",\"c_b_pmone\",\n",
    "                 \"c_b_pblst\",\"c_b_peos\",\"c_b_pbas\",\"c_b_namn\",\"c_b_nacv\",\"c_b_nimn\",\"c_b_nicv\",\"c_b_npmn\",\"c_b_npcv\",\"c_b_ndmn\",\n",
    "                 \"c_b_ndcv\",\"c_b_nfmn\",\"c_b_nfcv\",\"c_b_Lamn\",\"c_b_Lacv\",\"c_b_Limn\",\"c_b_Licv\"] # delay_Celldyn: remove\n",
    "ecg_cols_agg =  [\"VentricularRate_ECG\",\"AtrialRate_ECG\",\"P_RInterval_ECG\",\"QRS_Duration_ECG\",\"Q_TInterval_ECG\",\n",
    "                 \"QTCCalculation_ECG\",\"PAxis_ECG\",\"RAxis_ECG\",\"TAxis_ECG\",\"QRSCount_ECG\",\"QOnset_ECG\",\n",
    "                 \"QOffset_ECG\",\"POnset_ECG\",\"POffset_ECG\",\"T_Onset_ECG\",\"T_Offset_ECG\",\"QRS_Onset_ECG\",\n",
    "                 \"QRS_Offset_ECG\",\"AvgRRInterval_ECG\",\"QTcFredericia_ECG\",\"QTcFramingham_ECG\",\"QTc_Bazett_ECG\"]\n",
    "\n",
    "ecg_leads = ['Lead_I_', 'Lead_II_', 'Lead_III_', 'Lead_V1_', 'Lead_V2_', 'Lead_V3_', 'Lead_V4_', 'Lead_V5_', 'Lead_V6_', 'Lead_aVF_', 'Lead_aVL_', 'Lead_aVR_']\n",
    "ecg_msrmnt = ['MaxST_ECG',  'Max_R_Ampl_ECG', 'Max_S_Ampl_ECG', 'MinST_ECG', 'PFull_Area_ECG', 'PP_Area_ECG', 'PP_Duration_ECG',\n",
    " 'PP_PeakAmpl_ECG', 'PP_PeakTime_ECG', 'P_Area_ECG', 'P_Duration_ECG', 'P_PeakAmpl_ECG', 'P_PeakTime_ECG', 'QRS_Area_ECG', 'QRS_Balance_ECG',\n",
    " 'QRS_Deflection_ECG', 'QRSint_ECG', 'Q_Area_ECG', 'Q_Duration_ECG', 'Q_PeakAmpl_ECG', 'Q_PeakTime_ECG', 'RP_Area_ECG', 'RP_Duration_ECG', 'RP_PeakAmpl_ECG',\n",
    " 'RP_PeakTime_ECG', 'R_Area_ECG', 'R_Duration_ECG', 'R_PeakAmpl_ECG', 'R_PeakTime_ECG', 'SP_Area_ECG', 'SP_Duration_ECG', 'SP_PeakAmpl_ECG', \n",
    " 'SP_PeakTime_ECG', 'STE_ECG', 'STJ_ECG', 'STM_ECG', 'S_Area_ECG', 'S_Duration_ECG', 'S_PeakAmpl_ECG', 'S_PeakTime_ECG',\n",
    " 'TFull_Area_ECG', 'TP_Area_ECG', 'TP_Duration_ECG', 'TP_PeakAmpl_ECG', 'TP_PeakTime_ECG', 'T_Area_ECG', 'T_Duration_ECG', 'T_End_ECG',\n",
    " 'T_PeakAmpl_ECG', 'T_PeakTime_ECG', 'T_Special_ECG', 'P_OnsetAmpl_ECG']\n",
    "\n",
    "\n",
    "ecg_cols_dyn = [_lead+_msrmnt for _lead in ecg_leads for _msrmnt in ecg_msrmnt]\n",
    "\n",
    "ecg_cols_agg = list(set(ecg_cols_agg) & set(var_cols))\n",
    "ecg_cols_dyn = list(set(ecg_cols_dyn) & set(var_cols))\n",
    "cell_dyn_cols = list(set(cell_dyn_cols) & set(var_cols))\n",
    "\n",
    "ecg_cols = list(set(ecg_cols_agg+ecg_cols_dyn))\n",
    "other_cols = list(set(var_cols)-set(ecg_cols)-set(cell_dyn_cols))\n",
    "\n",
    "col_dict = {'ecg': ecg_cols, 'celldyn': cell_dyn_cols, 'other': other_cols}\n",
    "print(\"ECG: {} cols, \\t CELLDYN: {} cols\".format(len(ecg_cols), len(cell_dyn_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical encoding, only for HS\n",
    "# https://github.com/scikit-learn-contrib/categorical-encoding\n",
    "dummy_cols = ['HS_AGE', 'HS_History', 'HS_ECG', 'HS_new2', 'HS_RiskFacts', 'HN_TN']\n",
    "HS[dummy_cols] = HS[dummy_cols].astype('category')\n",
    "HS = pd.get_dummies(HS, prefix_sep={_dummy: \"_dummy_\" for _dummy in dummy_cols})\n",
    "dummy_cols = [_col for _col in HS.columns if '_dummy_' in _col]\n",
    "\n",
    "hs_cols = ['tn_admission', 'tn_slope2', 'tn_diff_abs', 'tn_diff_rel']+dummy_cols\n",
    "\n",
    "HS.loc[HS['tn_admission']>4000, 'tn_admission'] = 4000\n",
    "HS.loc[HS['tn_slope2']<-100, 'tn_slope2'] = 100\n",
    "\n",
    "HS['tn_diff_abs'] = HS[['tn_admission', 'tn_slope2']].apply(lambda x: np.sign(x[0]*x[1])*np.log10(np.abs(x[0]*x[1])+0.01), axis=1)\n",
    "HS['tn_diff_rel'] = HS[['tn_admission', 'tn_slope2']].apply(lambda x: x[1]/(x[0]+1), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1, echte recall, precisie/specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HS params: tn_slope2 etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
